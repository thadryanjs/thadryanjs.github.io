{
  "hash": "5f69415befb7f2422d7ae42fc6c033d0",
  "result": {
    "markdown": "---\nauthor: Thadryan\ntitle: \"Stats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling\"\ndate: 2022-12-13\ndescription: Transferring the basics of my R modeling knowledge back to my first language.\nbibliography: Part3.bib\ntoc: true\n---\n\n# Statistics\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport numpy as np\nfrom numpy.random import default_rng\n\nfrom scipy import stats\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache = True).data\n```\n:::\n\n\n## Summary Stats\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# get the data as a vector\nv = df.mpg\n\nnp.mean(v)\n# (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html)\nstats.mode(v, keepdims = False)\nnp.median(v)\nnp.var(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n35.188974609374995\n```\n:::\n:::\n\n\n## Simulating Data\n\nAfter trying a few things, it looks like [numpy](https://numpy.org/doc/stable/reference/random/index.html) is king here.\n\n### Create an RNG Object\n\nTo get started, create an RNG object that will make sure everything is initialized correctly.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nrng = default_rng()\n```\n:::\n\n\nWe can now go through a list of common distributions, simulate from them, and visualize to make sure we're on the right track.\n\n### Normal/Guassian\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nn = 1000\n\n# because calling the mean and sd the \"mean\" and \"sd\" would be too obvious\nv_norm = rng.normal(loc = 5, scale = 2.5, size = n)\nsns.kdeplot(v_norm)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<AxesSubplot: ylabel='Density'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=597 height=411}\n:::\n:::\n\n\n### Uniform\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# this makes sense lol\nv_unif = rng.uniform(low = 0, high = 1, size = n)\nsns.kdeplot(v_unif)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<AxesSubplot: ylabel='Density'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=589 height=411}\n:::\n:::\n\n\n### Poisson\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# lamda was too long I guess\nv_poisson = rng.poisson(lam = 1, size = n)\nsns.histplot(v_poisson)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<AxesSubplot: ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=593 height=411}\n:::\n:::\n\n\n### Bernoulli\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# bernoulli \n# https://stackoverflow.com/questions/47012474/bernoulli-random-number-generator\nv_bernoulli = rng.binomial(n = 1, p = 0.5, size = n)\nsns.histplot(v_bernoulli)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<AxesSubplot: ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=593 height=411}\n:::\n:::\n\n\n### Binomial \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# binomial\nv_binomial = rng.binomial(n = 4, p = 0.15, size = n)\nsns.histplot(v_binomial)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<AxesSubplot: ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=593 height=411}\n:::\n:::\n\n\n### Negative Binomial\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# this makes sense\nv_negative_binomial = rng.negative_binomial(n = 1, p = 0.25, size = n)\nsns.histplot(v_negative_binomial)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<AxesSubplot: ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=593 height=411}\n:::\n:::\n\n\n## Modeling\n\n### Linear Regression\n\nAnother routine thing that will come up a lot is linear regression. It's not as obvious as R, but it's pretty straight-forward. \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# you can use formula is you use smf \nlinear_model = smf.ols(formula = 'wt ~ disp + mpg', data = df).fit()\n```\n:::\n\n\nWe can get the results using sume `summary()` method, and it will look pretty familiar to R users. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nlinear_model.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>           <td>wt</td>        <th>  R-squared:         </th> <td>   0.836</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.824</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   73.65</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 27 Jan 2023</td> <th>  Prob (F-statistic):</th> <td>4.31e-12</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>10:21:49</td>     <th>  Log-Likelihood:    </th> <td> -15.323</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   36.65</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    29</td>      <th>  BIC:               </th> <td>   41.04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    3.5627</td> <td>    0.699</td> <td>    5.094</td> <td> 0.000</td> <td>    2.132</td> <td>    4.993</td>\n</tr>\n<tr>\n  <th>disp</th>      <td>    0.0043</td> <td>    0.001</td> <td>    3.818</td> <td> 0.001</td> <td>    0.002</td> <td>    0.007</td>\n</tr>\n<tr>\n  <th>mpg</th>       <td>   -0.0663</td> <td>    0.023</td> <td>   -2.878</td> <td> 0.007</td> <td>   -0.113</td> <td>   -0.019</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.431</td> <th>  Durbin-Watson:     </th> <td>   1.028</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.806</td> <th>  Jarque-Bera (JB):  </th> <td>   0.579</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.187</td> <th>  Prob(JB):          </th> <td>   0.749</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.458</td> <th>  Cond. No.          </th> <td>2.52e+03</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.52e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n### Logistic Regression\n\nThe next logical step is logistic regression. No surprises here.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# fit the model\nlogistic_model = smf.glm(\"am ~ wt + mpg\", data = df,\n  family = sm.families.Binomial()).fit()\n\nlogistic_model.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>am</td>        <th>  No. Observations:  </th>  <td>    32</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>    29</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     2</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -8.5921</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Fri, 27 Jan 2023</td> <th>  Deviance:          </th> <td>  17.184</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>10:21:49</td>     <th>  Pearson chi2:      </th>  <td>  32.7</td> \n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>7</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.5569</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   25.8866</td> <td>   12.194</td> <td>    2.123</td> <td> 0.034</td> <td>    1.988</td> <td>   49.785</td>\n</tr>\n<tr>\n  <th>wt</th>        <td>   -6.4162</td> <td>    2.547</td> <td>   -2.519</td> <td> 0.012</td> <td>  -11.407</td> <td>   -1.425</td>\n</tr>\n<tr>\n  <th>mpg</th>       <td>   -0.3242</td> <td>    0.239</td> <td>   -1.354</td> <td> 0.176</td> <td>   -0.794</td> <td>    0.145</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\n### ANOVA\n\nThe next basic anaylsis I wanted to recreate was ANOVA. This is handled nicely by `statsmodels`, looking more or less like the previous models.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# fit the initial model\nanova_model = smf.ols(\"cyl ~ mpg + disp\", data = df).fit()\n\nanova = sm.stats.anova_lm(anova_model)\nanova\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>df</th>\n      <th>sum_sq</th>\n      <th>mean_sq</th>\n      <th>F</th>\n      <th>PR(&gt;F)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mpg</th>\n      <td>1.0</td>\n      <td>71.801048</td>\n      <td>71.801048</td>\n      <td>132.393794</td>\n      <td>2.496891e-12</td>\n    </tr>\n    <tr>\n      <th>disp</th>\n      <td>1.0</td>\n      <td>11.346399</td>\n      <td>11.346399</td>\n      <td>20.921600</td>\n      <td>8.274021e-05</td>\n    </tr>\n    <tr>\n      <th>Residual</th>\n      <td>29.0</td>\n      <td>15.727553</td>\n      <td>0.542329</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n#### Tukey Test\n\nAfter an ANOVA, some sort of post-hoc test is usually preformed. This isn't as obvious as the ones above, requiring us to specify monovariate vectors instead of using a formla.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# specify the groups without the formual\ntukey_results = sm.stats.multicomp.pairwise_tukeyhsd(endog = df[\"mpg\"],\n  groups = df[\"cyl\"])\n\nprint(tukey_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\n     4      6  -6.9208 0.0003 -10.7693 -3.0722   True\n     4      8 -11.5636    0.0 -14.7708 -8.3565   True\n     6      8  -4.6429 0.0112  -8.3276 -0.9581   True\n-----------------------------------------------------\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}