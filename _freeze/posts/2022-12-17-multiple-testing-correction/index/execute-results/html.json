{
  "hash": "105e2a4b3245fb7c87c087490c309e98",
  "result": {
    "markdown": "---\nauthor: Thadryan\ntitle: \"Multiple testing correction for family-wise error-rate and false discovery rate\"\ndescription: \"Re-implementing for knowledge and profit(?).\"\ndate: 2022-12-17\ntoc: true\nbibliography: MultipleTesting.bib\ncsl: style.csl\n---\n\nIn this post we explain and re-implement two multiple testing corrections, Bonferroni and Benjamini-Hochberg, for the sake of understanding.\n\n## Preliminaries\n\nWe prepare the libraries we'll need.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# data wrangling, numerical computing, visualization\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# statistical models\nimport statsmodels\nimport statsmodels.api as sm\n\n# existing tools to check our implementation against\nfrom statsmodels.stats.multitest import fdrcorrection\nfrom statsmodels.stats.multitest import multipletests\t\n\n# random number generation\nfrom numpy.random import default_rng\n```\n:::\n\n\n## Simulating a motivating example\n\nThe motivation behind multiple testing correction is that if you test enough hypotheses, you will eventually find a result just by chance. The relevance of this concern increases with the number of tests - some experiments test dozens, hundreds of hypotheses.\n\nWe will develop a motivating example with which to work. Initially, we will simulate p values from populations with the same parameters so we know the null hypothesis of a t test is true: they're not different.\n\nWe can set this up nicely in Python using a function utilizing numpy and a list comprehension:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# seed the results for reproducibility \nrng = default_rng(seed = 1)\n\n# write a simulation function\ndef simulation(mu1, sd1, mu2, sd2, draws = 100, n = 1000):\n    # generate different populations \n    pop1 = rng.normal(loc = mu1, scale = sd1, size = draws)\n    pop2 = rng.normal(loc = mu2, scale = sd2, size = draws)\n    # this returns three things, we only need the middle one\n    tstat, pval, degf = sm.stats.ttest_ind(pop1, pop2)\n\n    return pval\n```\n:::\n\n\nThis function generations two populations. We can make them the same by passing the same mean and standard deviation. Now we can test it:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# set parameters for our populations.\nmu1 = 5\nsd1 = 2.5\n\nmu2 = 5\nsd2 = 2.5\n\n# get a single p value\nsimulation(mu1, sd1, mu2, sd2)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n0.9993893617043748\n```\n:::\n:::\n\n\nRecall that we've \"simulated under the null\" - we've specified that the parameters are the same for both distributions. Thus, we assume any positives are false. We can now repeat this a bunch of times to get a distribution of p values. We're using an $\\alpha$ (alpha) of 0.5 (more on this later), so we expect around 5% to be false positives.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# set a seed\nrng = default_rng(seed = 1)\n\n# run a thousand simulations\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1001)]\n\n# get the ones below alpha\nsig_vals = [p for p in pvals if p <= 0.05]\n\n# calculate the percent that are \"significant\"\nsig_percent = (len(sig_vals)/len(pvals))*100\n\n# percent significant hits\nround(sig_percent, 4)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n6.2937\n```\n:::\n:::\n\n\nWe can see that we have a small but non-trivial amount of hits flagged as significant. If we think about the definition of a p value though, this makes sense. \n\n> p value: The probability of seeing evidence against the null that is this extreme or more given the null is true [@@HypothesisTestingHandbook] - given our modeling assumptions are met.\n\nIf we're operating at $\\alpha = 0.05$, we've accepting that we will be wrong about 5% of the time. In experiments where we're testing a lot of hypotheses though, this adds up to a lot of mistakes.\n\nFor example, consider a differential expression experiment involving 20,000 genes:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# 5 percent wrong in this case...\n0.05 * 20000\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n1000.0\n```\n:::\n:::\n\n\n...we're talking about being wrong a thousand times. This is why multiple testing correction exists. We will consider two types with one example each.\n\n## Controlling Family-wise Error Rate (FWER) with the Bonferroni method\n\nBonferroni is perhaps the most common method for controlling the Family-wise error rate, which is the probability of making at least one false finding [@MultipleComparisonsHandbook]. In this method we simply establish a new threshold for significance by dividing $\\alpha$ by the number of tests.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# a function to apply the correction\ndef calc_bonferroni_threshold(vector, alpha = 0.5):\n    # divide alpha by the number of tests, ie, pvalues in the vector\n    return alpha/len(vector)\n\n# compute a new threshold\nbonferroni_threshold = calc_bonferroni_threshold(pvals)\n\n# new threshold\nbonferroni_threshold\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n0.0004995004995004995\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# see which ones are significant now\nsig_adj_pvals = [p for p in pvals if p <= bonferroni_threshold]\n\n# calculate the percent\nsig_percent_adj = len(sig_adj_pvals)/len(pvals)\n\n# inspect \nround(sig_percent_adj, 4)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.0\n```\n:::\n:::\n\n\nLet's see if we also get 0 with Python's version. This doesn't return the threshold, it returns a list of corrected pvalues and a list of True/False values telling us to reject not. If our code agrees with this, we should see a vector with no`True`s in it.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# call the function\ncheck = multipletests(pvals, method=\"bonferroni\", alpha = 0.05)\n\n# the 0th index contains the booleans\ncheck_trues = [i for i in check[0] if i == True]\n\nlen(check_trues)/len(pvals)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n0.0\n```\n:::\n:::\n\n\nWe've taken care of the false-positives. As you may have intuited, this is a pretty strict correction (dividing by 20k, in our other example); it does come at the cost of statistical power (the ability of a test to find something if it's there). There is no free lunch, especially in statistical power. Sometimes a more permissive tradeoff can be made.\n\n## False Discovery Rate (FDR)\n\nWhile Bonferroni is tried and true, it's something of a blunt instrument however, and there are some situations where another common method Benjamini-Hochberg [@benjaminiControllingFalseDiscovery1995], often abbreviated \"BH\", is preferable. The Benjamini-Hochberg controls the False-discovery rate [@benjaminiControllingFalseDiscovery1995], which is the percent of the time we're making a false positive. This is useful in situations where the strictness of Bonferroni might be limiting (more on comparing the two later). The method involves:\n\n  - Ranking the p values from lowest to highest.\n  - Computing a critical value for each one.\n  - Comparing the p value to the critical value.\n  - The highest p value that is lower that the corresponding critical is the new threshold - it and all smaller than it are considered significant.\n\nThe formula for the critical value is:\n\n$$\nc = (\\frac{r}{n})\\alpha\n$$\n\nNote you will often see $\\alpha$ denoted $q$ - I'm just trying to keep things simple by calling it what it is (not exactly a time-honored tradition in statistics).\n\nThis is relatively straightforward to implement in Python (there is a built in function in R as well as a function in `statsmodels` for Python, but we will reimplement it for the sake of learning and transparency). We define the function below:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# function to compute the new BH threshold\ndef calc_bh_threshold(vector, alpha = 0.05):\n\n    # the number of hypotheses\n    m = len(vector)\n    # the is just naming convention\n    q = alpha\n\n    # sort the pvalues\n    sorted_pvals = sorted(vector)\n\n    # collect a critical value for each p value\n    critical_vals = []\n    for i in range(0, len(sorted_pvals)):\n        rank = i+1 # the rank is the index +1 as Python is zero-indexed\n        # the formula for \n        critical_val = (rank/m)*q\n        critical_vals.append(critical_val)\n    \n    # organize our results\n    df_res = pd.DataFrame({\n        \"pvalue\": sorted_pvals,\n        \"critical_value\": critical_vals\n    })\n    # get the values where the pvalue is lower than the critical value\n    df_res_p_less_crit = df_res.query(\"pvalue <= critical_value\").\\\n        sort_values(by = \"pvalue\", ascending = False)\n    \n    # if none meet the criteria return 0 so no comparison to it will be significant\n    if len(df_res_p_less_crit) == 0:\n        return 0\n    else:\n        # the largest of these is the new threshold\n        return df_res_p_less_crit[\"pvalue\"].max()\n\nbh_threshold = calc_bh_threshold(pvals)\nbh_threshold\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n0\n```\n:::\n:::\n\n\nNow we can compare apply this method to correcting for multiple testing on our p values (some of which we know are false positives).\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# organize the p values\ndf_res = pd.DataFrame({\"pval\": pvals})\n\n# apply a test - are they lower than the new threshold?\ndf_res[\"bg_reject\"] = df_res.pval.apply(lambda x: x <= bh_threshold)\n\n# find where we reject the null (false positive)\nsig_adj_bh = df_res.query(\"bg_reject == True\")\n\n# get the new percent\nlen(sig_adj_bh)/len(pvals)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.0\n```\n:::\n:::\n\n\nWe see we no longer get significant p values. Just to make sure we're correct, we can simulate where the mean actually is different and see if our implementation agrees with the official one.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# here, the means are different\nmu1 = 5\nsd1 = 1\n\nmu2 = 5.5\nsd2 = 1\n\n# create a another simulated vector of p values\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1000)]\n\n# organize it\ndf_check = pd.DataFrame({\"pval\": pvals})\n\n# put some ids to identify them by for better comparison\ndf_check[\"sample_id\"] = [\"sample\" + str(i+1) for i in range(0, len(pvals))]\n\n# get a new threshold\nbh_threshold_check = calc_bh_threshold(df_check[\"pval\"])\nbh_threshold_check\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.04645320875671015\n```\n:::\n:::\n\n\nWe've got our new threshold, now we can compare the p values to it.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# compare the values in the pval function to the new criteria and see how many we get\ndf_check[\"bh_reject_custom\"] = df_check.pval.apply(lambda x: x <= bh_threshold_check)\n\n# a count of reject true/false\ndf_check.bh_reject_custom.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nTrue     931\nFalse     69\nName: bh_reject_custom, dtype: int64\n```\n:::\n:::\n\n\nNow we compare that to the statsmodels implementation:\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# apply the statsmodels function to the p values\ndf_check[\"bh_reject_default\"] = fdrcorrection(df_check[\"pval\"])[0]\n\n# a table of reject true/false\ndf_check.bh_reject_default.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nTrue     931\nFalse     69\nName: bh_reject_default, dtype: int64\n```\n:::\n:::\n\n\nLet's asses the agreement by making a column based on their comparison:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# do they agree?\ndf_check[\"agreement\"] = \\\n    df_check[\"bh_reject_custom\"] == df_check[\"bh_reject_default\"]\n\n# see where they disagree\n[i for i in df_check[\"agreement\"] if i == False]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[]\n```\n:::\n:::\n\n\nNo disagreement. Just to be absolutely safe, we check the IDs of the rejected p values.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# subset where the custom rejects\ndf_check_custom_agree = df_check.query(\"bh_reject_custom == True\")\n\n# a subset where the default rejects\ndf_check_default_agree = df_check.query(\"bh_reject_default == True\")\n\n# compare the ids\nagreement = df_check_custom_agree.sample_id == \\\n  df_check_default_agree.sample_id\n\n# observe the concordance\nagreement.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nTrue    931\nName: sample_id, dtype: int64\n```\n:::\n:::\n\n\n## When to use these methods?\n\nThis will depend on the particulars of the experiment in question and must be evaluated on a case-by-case basis. In rough terms however, it depends on if you need to be absolutely sure about a particular result vs if you're willing to get a few false positives. For example, if you're conducting a follow-up study to confirm a result before committing a large amount of time and money to a line of research, Bonferroni makes more sense - making even a single error is costly, and this is what the FWER tells us (the probability of even a single mistake). If you're doing a preliminary experiment to nominate targets for further study, then a few false positives might be fine. You might nominate 10-15 targets to be followed up on in the lab, and if only a few pan out, that's ok, they're the new focus. This is common if differential expression experiments [@HypothesisTestingHandbook].\n\n## References \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}