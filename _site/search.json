[
  {
    "objectID": "wip/wtf/testing.html",
    "href": "wip/wtf/testing.html",
    "title": "Thadryan J. Sweeney, MS",
    "section": "",
    "text": "var = 1\n\n\ndef inc(x):\n    return x+1\n\n\ninc(var)\n\n2"
  },
  {
    "objectID": "wip/multiple-testing-correction/index.html",
    "href": "wip/multiple-testing-correction/index.html",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "",
    "text": "In this post we explain and re-implement two multiple testing corrections, Bonferroni and Benjamini-Hochberg, for the sake of understanding."
  },
  {
    "objectID": "wip/multiple-testing-correction/index.html#preliminaries",
    "href": "wip/multiple-testing-correction/index.html#preliminaries",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe prepare the libraries we’ll need.\n\n# data wrangling, numerical computing, visualization\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# statistical models\nimport statsmodels\nimport statsmodels.api as sm\n\n# existing tools to check our implementation against\nfrom statsmodels.stats.multitest import fdrcorrection\nfrom statsmodels.stats.multitest import multipletests   \n\n# random number generation\nfrom numpy.random import default_rng"
  },
  {
    "objectID": "wip/multiple-testing-correction/index.html#simulating-a-motivating-example",
    "href": "wip/multiple-testing-correction/index.html#simulating-a-motivating-example",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Simulating a motivating example",
    "text": "Simulating a motivating example\nThe motivation behind multiple testing correction is that if you test enough hypotheses, you will eventually find a result just by chance. The relevance of this concern increases with the number of tests - some experiments test dozens, hundreds of hypotheses.\nWe will develop a motivating example with which to work. Initially, we will simulate p values from populations with the same parameters so we know the null hypothesis of a t test is true: they’re not different.\nWe can set this up nicely in Python using a function utilizing numpy and a list comprehension:\n\n# seed the results for reproducibility \nrng = default_rng(seed = 1)\n\n# write a simulation function\ndef simulation(mu1, sd1, mu2, sd2, draws = 100, n = 1000):\n    # generate different populations \n    pop1 = rng.normal(loc = mu1, scale = sd1, size = draws)\n    pop2 = rng.normal(loc = mu2, scale = sd2, size = draws)\n    # this returns three things, we only need the middle one\n    tstat, pval, degf = sm.stats.ttest_ind(pop1, pop2)\n\n    return pval\n\nThis function generations two populations. We can make them the same by passing the same mean and standard deviation. Now we can test it:\n\n# set parameters for our populations.\nmu1 = 5\nsd1 = 2.5\n\nmu2 = 5\nsd2 = 2.5\n\n# get a single p value\nsimulation(mu1, sd1, mu2, sd2)\n\n0.9993893617043748\n\n\nRecall that we’ve “simulated under the null” - we’ve specified that the parameters are the same for both distributions. Thus, we assume any positives are false. We can now repeat this a bunch of times to get a distribution of p values. We’re using an \\(\\alpha\\) (alpha) of 0.5 (more on this later), so we expect around 5% to be false positives.\n\n# set a seed\nrng = default_rng(seed = 1)\n\n# run a thousand simulations\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1001)]\n\n# get the ones below alpha\nsig_vals = [p for p in pvals if p <= 0.05]\n\n# calculate the percent that are \"significant\"\nsig_percent = (len(sig_vals)/len(pvals))*100\n\n# percent significant hits\nround(sig_percent, 4)\n\n6.2937\n\n\nWe can see that we have a small but non-trivial amount of hits flagged as significant. If we think about the definition of a p value though, this makes sense.\n\np value: The probability of seeing evidence against the null that is this extreme or more given the null is true [@ 1] - given our modeling assumptions are met.\n\nIf we’re operating at \\(\\alpha = 0.05\\), we’ve accepting that we will be wrong about 5% of the time. In experiments where we’re testing a lot of hypotheses though, this adds up to a lot of mistakes.\nFor example, consider a differential expression experiment involving 20,000 genes:\n\n# 5 percent wrong in this case...\n0.05 * 20000\n\n1000.0\n\n\n…we’re talking about being wrong a thousand times. This is why multiple testing correction exists. We will consider two types with one example each."
  },
  {
    "objectID": "wip/multiple-testing-correction/index.html#controlling-family-wise-error-rate-fwer-with-the-bonferroni-method",
    "href": "wip/multiple-testing-correction/index.html#controlling-family-wise-error-rate-fwer-with-the-bonferroni-method",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Controlling Family-wise Error Rate (FWER) with the Bonferroni method",
    "text": "Controlling Family-wise Error Rate (FWER) with the Bonferroni method\nBonferroni is perhaps the most common method for controlling the Family-wise error rate, which is the probability of making at least one false finding [2]. In this method we simply establish a new threshold for significance by dividing \\(\\alpha\\) by the number of tests.\n\n# a function to apply the correction\ndef calc_bonferroni_threshold(vector, alpha = 0.5):\n    # divide alpha by the number of tests, ie, pvalues in the vector\n    return alpha/len(vector)\n\n# make a list of Bonferroni-adjusted p values\nbonferroni_threshold = calc_bonferroni_threshold(pvals)\n\n# new threshold\nbonferroni_threshold\n\n0.0004995004995004995\n\n\n\n# see which ones are significant now\nsig_adj_pvals = [p for p in pvals if p <= bonferroni_threshold]\n\n# calculate the percent\nsig_percent_adj = len(sig_adj_pvals)/len(pvals)\n\n# inspect \nround(sig_percent_adj, 4)\n\n0.0\n\n\nLet’s see if we also get 0 with Python’s version. This doesn’t return the threshold, it returns a list of corrected pvalues and a list of True/False values telling us to reject not. If our code agrees with this, we should see a vector with noTrues in it.\n\n# call the function\ncheck = multipletests(pvals, method=\"bonferroni\", alpha = 0.05)\n\n# the 0th index contains the booleans\ncheck_trues = [i for i in check[0] if i == True]\n\nlen(check_trues)/len(pvals)\n\n0.0\n\n\nWe’ve taken care of the false-positives. As you may have intuited, this is a pretty strict correction (dividing by 20k, in our other example); it does come at the cost of statistical power (the ability of a test to find something if it’s there). There is no free lunch, especially in statistical power. Sometimes a more permissive tradeoff can be made."
  },
  {
    "objectID": "wip/multiple-testing-correction/index.html#false-discovery-rate-fdr",
    "href": "wip/multiple-testing-correction/index.html#false-discovery-rate-fdr",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\nWhile Bonferroni is tried and true, it’s something of a blunt instrument however, and there are some situations where another common method Benjamini-Hochberg [3], often abbreviated “BH”, is preferable. The Benjamini-Hochberg controls the False-discovery rate [3], which is the percent of the time we’re making a false positive. This is useful in situations where the strictness of Bonferroni might be limiting (more on comparing the two later). The method involves:\n\nRanking the p values from lowest to highest.\nComputing a critical value for each one.\nComparing the p value to the critical value.\nThe highest p value that is lower that the corresponding critical is the new threshold - it and all smaller than it are considered significant.\n\nThe formula for the critical value is:\n\\[\nc = (\\frac{r}{n})\\alpha\n\\]\nNote you will often see \\(\\alpha\\) denoted \\(q\\) - I’m just trying to keep things simple by calling it what it is (not exactly a time-honored tradition in statistics).\nThis is relatively straightforward to implement in Python (there is a built in function in R as well as a function in statsmodels for Python, but we will reimplement it for the sake of learning and transparency). We define the function below:\n\n# function to compute the new BH threshold\ndef calc_bh_threshold(vector, alpha = 0.05):\n\n    # the number of hypotheses\n    m = len(vector)\n    # the is just naming convention\n    q = alpha\n\n    # sort the pvalues\n    sorted_pvals = sorted(vector)\n\n    # collect a critical value for each p value\n    critical_vals = []\n    for i in range(0, len(sorted_pvals)):\n        rank = i+1 # the rank is the index +1 as Python is zero-indexed\n        # the formula for \n        critical_val = (rank/m)*q\n        critical_vals.append(critical_val)\n    \n    # organize our results\n    df_res = pd.DataFrame({\n        \"pvalue\": sorted_pvals,\n        \"critical_value\": critical_vals\n    })\n    # get the values where the pvalue is lower than the critical value\n    df_res_p_less_crit = df_res.query(\"pvalue <= critical_value\").\\\n        sort_values(by = \"pvalue\", ascending = False)\n    \n    # if none meet the criteria return 0 so no comparison to it will be significant\n    if len(df_res_p_less_crit) == 0:\n        return 0\n    else:\n        # the largest of these is the new threshold\n        return df_res_p_less_crit[\"pvalue\"].max()\n\nbh_threshold = calc_bh_threshold(pvals)\nbh_threshold\n\n0\n\n\nNow we can compare apply this method to correcting for multiple testing on our p values (some of which we know are false positives).\n\n# organize the p values\ndf_res = pd.DataFrame({\"pval\": pvals})\n\n# apply a test - are they lower than the new threshold?\ndf_res[\"bg_reject\"] = df_res.pval.apply(lambda x: x <= bh_threshold)\n\n# find where we reject the null (false positive)\nsig_adj_bh = df_res.query(\"bg_reject == True\")\n\n# get the new percent\nlen(sig_adj_bh)/len(pvals)\n\n0.0\n\n\nWe see we no longer get significant p values. Just to make sure we’re correct, we can simulate where the mean actually is different and see if our implementation agrees with the official one.\n\n# here, the means are different\nmu1 = 5\nsd1 = 1\n\nmu2 = 5.5\nsd2 = 1\n\n# create a another simulated vector of p values\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1000)]\n\n# organize it\ndf_check = pd.DataFrame({\"pval\": pvals})\n\n# put some ids to identify them by for better comparison\ndf_check[\"sample_id\"] = [\"sample\" + str(i+1) for i in range(0, len(pvals))]\n\n# get a new threshold\nbh_threshold_check = calc_bh_threshold(df_check[\"pval\"])\nbh_threshold_check\n\n0.04645320875671015\n\n\nWe’ve got our new threshold, now we can compare the p values to it.\n\n# compare the values in the pval function to the new criteria and see how many we get\ndf_check[\"bh_reject_custom\"] = df_check.pval.apply(lambda x: x <= bh_threshold_check)\n\n# a count of reject true/false\ndf_check.bh_reject_custom.value_counts()\n\nTrue     931\nFalse     69\nName: bh_reject_custom, dtype: int64\n\n\nNow we compare that to the statsmodels implementation:\n\n# apply the statsmodels function to the p values\ndf_check[\"bh_reject_default\"] = fdrcorrection(df_check[\"pval\"])[0]\n\n# a table of reject true/false\ndf_check.bh_reject_default.value_counts()\n\nTrue     931\nFalse     69\nName: bh_reject_default, dtype: int64\n\n\nLet’s asses the agreement by making a column based on their comparison:\n\n# do they agree?\ndf_check[\"agreement\"] = \\\n    df_check[\"bh_reject_custom\"] == df_check[\"bh_reject_default\"]\n\n# see where they disagree\n[i for i in df_check[\"agreement\"] if i == False]\n\n[]\n\n\nNo disagreement. Just to be absolutely safe, we check the IDs of the rejected p values.\n\n# subset where the custom rejects\ndf_check_custom_agree = df_check.query(\"bh_reject_custom == True\")\n\n# a subset where the default rejects\ndf_check_default_agree = df_check.query(\"bh_reject_default == True\")\n\n# compare the ids\nagreement = df_check_custom_agree.sample_id == \\\n  df_check_default_agree.sample_id\n\n# observe the concordance\nagreement.value_counts()\n\nTrue    931\nName: sample_id, dtype: int64"
  },
  {
    "objectID": "wip/multiple-testing-correction/index.html#when-to-use-these-methods",
    "href": "wip/multiple-testing-correction/index.html#when-to-use-these-methods",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "When to use these methods?",
    "text": "When to use these methods?\nThis will depend on the particulars of the experiment in question and must be evaluated on a case-by-case basis. In rough terms however, it depends on if you need to be absolutely sure about a particular result vs if you’re willing to get a few false positives. For example, if you’re conducting a follow-up study to confirm a result before committing a large amount of time and money to a line of research, Bonferroni makes more sense - making even a single error is costly, and this is what the FWER tells us (the probability of even a single mistake). If you’re doing a preliminary experiment to nominate targets for further study, then a few false positives might be fine. You might nominate 10-15 targets to be followed up on in the lab, and if only a few pan out, that’s ok, they’re the new focus. This is common if differential expression experiments [1]."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Drops of Jupyter",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nStats Python in a Hurry Part 3: Basic Analysis & Modeling\n\n\n\n\n\nTransferring the basics of my R modeling knowledge back to my first language.\n\n\n\n\n\n\nDec 13, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStats Python in a Hurry Part 2: Visualization\n\n\n\n\n\nTransferring my R data viz knowledge back to my first language.\n\n\n\n\n\n\nDec 12, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStats Python in a Hurry Part 1: Data Wrangling\n\n\n\n\n\nTransferring my R data-munging knowledge back to my first language.\n\n\n\n\n\n\nDec 11, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummarizing a review of contributors to the opioid epidemic\n\n\n\n\n\n\n\nOpioids\n\n\nResearch\n\n\n\n\nA concise summary of Cerdá, et al with some additional fundamentals.\n\n\n\n\n\n\nDec 10, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Survival Guide for Students Starting an MS in Bioinformatics\n\n\n\n\n\n\n\nAcademia\n\n\nAdvice\n\n\n\n\nSome lessons learned in grad school.\n\n\n\n\n\n\nSep 29, 2021\n\n\nThadyan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRygor: a Retraction Watch database companion\n\n\n\n\n\n\n\nTools\n\n\n\n\nA command line tool to check a list of citations against The Retraction Watch Database\n\n\n\n\n\n\nJan 19, 2021\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nIn Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited\n\n\n\n\n\n\n\nSociopolitical\n\n\n\n\nSome more wordclouds to celebrate the day.\n\n\n\n\n\n\nJan 18, 2021\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn interactive Bayseian app for interpretation of SARS-CoV-2 antibody tests\n\n\n\n\n\n\n\nTools\n\n\nPublications\n\n\nStatistics\n\n\nJavaScript\n\n\n\n\nA webapp to complement our recent paper.\n\n\n\n\n\n\nJan 8, 2021\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFact Check: The Quote Heather Mac Donald presents when discussing Johnson et al comes from a passage of a pre-print that does not appear in the published work.\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nMore bad form.\n\n\n\n\n\n\nJul 11, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvidence Heather Mac Donald Presented A Pre-print Claim That Was Remove In Peer Review As Scientific\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nMore details on a previously observed biff from HMD.\n\n\n\n\n\n\nJul 10, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetter - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nApplying some more skepticism to a recent WSJ Op-ed, because it deserves it.\n\n\n\n\n\n\nJul 9, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nApplying some skepticism to a recent WSJ Op-ed\n\n\n\n\n\n\nJun 27, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Case study in Silent Data Corruption in an RNA-Seq Experiment\n\n\n\n\n\n\n\nR\n\n\nBioinformatics\n\n\nStatistics\n\n\nDifferential Expression\n\n\nTools\n\n\n\n\nHow a subtle bug and misleading error message can transform your RNA-Seq data.\n\n\n\n\n\n\nApr 27, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nHow to install Arch Linux on a VirtualBox VM\n\n\n\n\n\n\n\nLinux\n\n\n\n\nTesting Arch before you wreck your computer installing it.\n\n\n\n\n\n\nMar 30, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nIn Honor of MLK 2020: ‘Letter from Birmingham Jail’ Wordcloud Example in R\n\n\n\n\n\n\n\nSociopolitical\n\n\n\n\nMarking the occasion with word clouds.\n\n\n\n\n\n\nJan 20, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonality in gun deaths\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\n\n\nPracticing data analysis on some CDC gun data.\n\n\n\n\n\n\nAug 30, 2019\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClosures in Python\n\n\n\n\n\n\n\nPython\n\n\nFunctional Programming\n\n\n\n\nMakeshift type checking in Python and when I find it handy.\n\n\n\n\n\n\nMar 28, 2019\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA more rigorous ASH\n\n\n\n\n\n\n\nAntigens\n\n\nPython\n\n\n\n\nBuiling on the ASH tool from a previous post.\n\n\n\n\n\n\nJan 31, 2019\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerl for the impatient\n\n\n\n\n\n\n\nPerl\n\n\n\n\nA dangerously fast introduction to a dangerously concise language.\n\n\n\n\n\n\nOct 4, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython imports: Sample Project + Explanations\n\n\n\n\n\n\n\nPython\n\n\n\n\nAnswering a question on using imports in Python pograms\n\n\n\n\n\n\nSep 30, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObject-oriented Python: an overview\n\n\n\n\n\n\n\nPython\n\n\nObject-Oriented Programming\n\n\n\n\nAn overview of object-oriented programming in Python.\n\n\n\n\n\n\nAug 30, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA handshake with Scala\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome light-hearted oop\n\n\n\n\n\n\n\nPython\n\n\nObject-Oriented Programming\n\n\n\n\n…or: circumstantial evidence that I am still alive.\n\n\n\n\n\n\nMay 10, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind and replace in Python\n\n\n\n\n\n\n\nPython\n\n\nData Processing\n\n\n\n\nLike ctrl-f with for Python.\n\n\n\n\n\n\nMar 12, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nASH: Antigen Selection Heuristic\n\n\n\n\n\n\n\nAntigens\n\n\nPython\n\n\n\n\nA prototype to select antigenic peptides.\n\n\n\n\n\n\nFeb 20, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple date-string conversion\n\n\n\n\n\n\n\nPython\n\n\nData Processing\n\n\n\n\nAnswering a question an dates and strings.\n\n\n\n\n\n\nFeb 6, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nMatrin Luther King Day\n\n\n\n\n\n\n\nSociopolitical\n\n\nR\n\n\nNLP\n\n\n\n\nMarking the occasion with word clouds.\n\n\n\n\n\n\nJan 16, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrump v. Clinton: Twitter metadata classifier\n\n\n\n\n\nClassifiying tweets as Hilary or Trump.\n\n\n\n\n\n\nJan 9, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython for Perl users\n\n\n\n\n\n\n\nPython\n\n\nPerl\n\n\n\n\nAll the kids are using Python now.\n\n\n\n\n\n\nDec 21, 2017\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzip and dict\n\n\n\n\n\n\n\nPerl\n\n\nPython\n\n\n\n\nThe peanutbetter and jelly of paired data.\n\n\n\n\n\n\nDec 20, 2017\n\n\nThadryan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m a Firefighter and Human Services Professional turned Data Scientist interested in statistics, interactive visualization, and biomarker discovery. I do statistically oriented biology research using R, Python, and JavaScript. I’m currently pursing a PhD in Quantitative Biomedical Sciences.\nBefore starting my doctorate, I was at Mass General Hospital where I worked in the  Alzheimer’s Clinical & Translational Research Unit. Before moving to ACTRU, I did statistics and web app development for studies of SARS-Cov-2 and various heart diseases at the  Cardiovascular Research Center.\nI also taught an introductory class in the Bioinformatics MS  program at Northeastern University. If you’re interested in seeing what I’m working on professionally, check out my  CV  or my  LinkedIn.  When I’m not doing science-y stuff, I play RPGs, write music, or make cartoons."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "What I Work On",
    "section": "",
    "text": "My work falls roughly into five (often overlapping) spheres.\n\nGeneral Research Programming\nI work on  apps  that complement  publications  to make their content easier to use and increase engagement.\n\n\nOriginal Software\nI work on creating new software for unmet needs. One example is our  Bayesian COVID Diagnostics app. This serves to complement our  manuscript  on lateral flow assays for SARS-CoV-2 antibodies as well as providing the user with an easy-to-use tool to avoid the pitfalls of black-and-white thinking about diagnostics and counter-intuitive conditional probability. Another is  ContrApption, which is a JavaScript-powered R package that allows to user to create an interactive widget for exploring the results of differential expression experiments or any dataset with similar structure.\n\n\nStatistics for biomarkers\nI work on statistical analysis for biomarkers of potential use in Alzheimer’s Dementia and Neurological symptoms of COVID-19. We’ve got two manuscripts submitted and a few more nearing completion in the sphere, so stay tuned and see my CV for the titles if you’re curious.\n\n\nStatistics and genomics for basic science\nFor an example of this, check out our  manuscript  on stress-dependent signatures of cellular and extracellular tRNA-derived small RNAs (tDRs). We’re interested in this from a basic science perspective as well as a potential avenue of biomarkers for diseases.\n\n\n“Hacktivism”\nI had the privilege of contributing to a  web tool  that allows users to interact with a dataset that ranks the states in the US based on how well they support the financial well-being of survivors of domestic abuse. I did this as a member of  RagTag  in support of  FREEFROM."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "For computational and data science projects, see my portfolio."
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "",
    "text": "update: 12-17-22 - added sorting and table."
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#basics",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#basics",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Basics",
    "text": "Basics\nHow to get basic information about the df.\n\nDimensions of a df\nThis is like dim in R.\n\n# rows by columns \ndf.shape\n\n(32, 11)\n\n\n\n\nNumber of rows\nThis is like nrow in R.\n\n# just the rows\nlen(df)\n\n32\n\n\n\n\nNumber of columns\nThis is like ncol in R.\n\n# just the columns\nlen(df.columns)\n\n11\n\n\nYou could also just index the results of df.shape.\n\n\nData types\n\n# a summary of types by column\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nIndex: 32 entries, Mazda RX4 to Volvo 142E\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   mpg     32 non-null     float64\n 1   cyl     32 non-null     int64  \n 2   disp    32 non-null     float64\n 3   hp      32 non-null     int64  \n 4   drat    32 non-null     float64\n 5   wt      32 non-null     float64\n 6   qsec    32 non-null     float64\n 7   vs      32 non-null     int64  \n 8   am      32 non-null     int64  \n 9   gear    32 non-null     int64  \n 10  carb    32 non-null     int64  \ndtypes: float64(5), int64(6)\nmemory usage: 3.0+ KB\n\n\n\n\nSummary\n\n# summary of distributional information\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      count\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.0000\n    \n    \n      mean\n      20.090625\n      6.187500\n      230.721875\n      146.687500\n      3.596563\n      3.217250\n      17.848750\n      0.437500\n      0.406250\n      3.687500\n      2.8125\n    \n    \n      std\n      6.026948\n      1.785922\n      123.938694\n      68.562868\n      0.534679\n      0.978457\n      1.786943\n      0.504016\n      0.498991\n      0.737804\n      1.6152\n    \n    \n      min\n      10.400000\n      4.000000\n      71.100000\n      52.000000\n      2.760000\n      1.513000\n      14.500000\n      0.000000\n      0.000000\n      3.000000\n      1.0000\n    \n    \n      25%\n      15.425000\n      4.000000\n      120.825000\n      96.500000\n      3.080000\n      2.581250\n      16.892500\n      0.000000\n      0.000000\n      3.000000\n      2.0000\n    \n    \n      50%\n      19.200000\n      6.000000\n      196.300000\n      123.000000\n      3.695000\n      3.325000\n      17.710000\n      0.000000\n      0.000000\n      4.000000\n      2.0000\n    \n    \n      75%\n      22.800000\n      8.000000\n      326.000000\n      180.000000\n      3.920000\n      3.610000\n      18.900000\n      1.000000\n      1.000000\n      4.000000\n      4.0000\n    \n    \n      max\n      33.900000\n      8.000000\n      472.000000\n      335.000000\n      4.930000\n      5.424000\n      22.900000\n      1.000000\n      1.000000\n      5.000000\n      8.0000"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#selections",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#selections",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Selections",
    "text": "Selections\nHow to specify which column you want.\n\nSelect a column\nNote that I’m using head() to get the first few rows to keep the output small. It’s not part of the selection of columns.\n\n# access a single column like an object property\ndf.mpg.head()\n\nMazda RX4            21.0\nMazda RX4 Wag        21.0\nDatsun 710           22.8\nHornet 4 Drive       21.4\nHornet Sportabout    18.7\nName: mpg, dtype: float64\n\n\nOr…\n\n# select a column (as a series)\ndf[\"mpg\"].head()\n\nMazda RX4            21.0\nMazda RX4 Wag        21.0\nDatsun 710           22.8\nHornet 4 Drive       21.4\nHornet Sportabout    18.7\nName: mpg, dtype: float64\n\n\n\n# select column (as a dataframe)\ndf[[\"mpg\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n    \n    \n      Mazda RX4 Wag\n      21.0\n    \n    \n      Datsun 710\n      22.8\n    \n    \n      Hornet 4 Drive\n      21.4\n    \n    \n      Hornet Sportabout\n      18.7\n    \n  \n\n\n\n\n\n\nSelect several columns by string name\n\n# select a column by several names\ndf[[\"mpg\", \"wt\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      wt\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      2.620\n    \n    \n      Mazda RX4 Wag\n      21.0\n      2.875\n    \n    \n      Datsun 710\n      22.8\n      2.320\n    \n    \n      Hornet 4 Drive\n      21.4\n      3.215\n    \n    \n      Hornet Sportabout\n      18.7\n      3.440\n    \n  \n\n\n\n\n\n\nSelect from one column through another with names\n\n# select a range by name of start and end \"mph through hp\"\ndf.loc[:, \"mpg\":\"hp\"].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n    \n    \n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n    \n    \n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n    \n  \n\n\n\n\n\n\nSelect arbitrarily\nHow to get based on a condition of your choosing. We might want to get only columns that start with “d”, for example.\n\n# get a list\nstarts_with_d = [i for i in list(df.columns) if i.startswith(\"d\")]\n\n# pass the list to pandas\ndf[starts_with_d].head()\n\n\n\n\n\n  \n    \n      \n      disp\n      drat\n    \n  \n  \n    \n      Mazda RX4\n      160.0\n      3.90\n    \n    \n      Mazda RX4 Wag\n      160.0\n      3.90\n    \n    \n      Datsun 710\n      108.0\n      3.85\n    \n    \n      Hornet 4 Drive\n      258.0\n      3.08\n    \n    \n      Hornet Sportabout\n      360.0\n      3.15"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#filtering",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#filtering",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Filtering",
    "text": "Filtering\nOperations where we’d used dplyr in R. In Pandas, filtering refers to operations by index, so what I’m thinking of is more like “querying” in Pandas terms.\n\nSingle condition\nPass logic in strings to the query method.\n\ndf.query(\"cyl == 6\")\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      Valiant\n      18.1\n      6\n      225.0\n      105\n      2.76\n      3.460\n      20.22\n      1\n      0\n      3\n      1\n    \n    \n      Merc 280\n      19.2\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.30\n      1\n      0\n      4\n      4\n    \n    \n      Merc 280C\n      17.8\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.90\n      1\n      0\n      4\n      4\n    \n    \n      Ferrari Dino\n      19.7\n      6\n      145.0\n      175\n      3.62\n      2.770\n      15.50\n      0\n      1\n      5\n      6\n    \n  \n\n\n\n\n\n\nMultiple conditions\n\ndf.query(\"cyl == 6 & hp > 105\")\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      Merc 280\n      19.2\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.30\n      1\n      0\n      4\n      4\n    \n    \n      Merc 280C\n      17.8\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.90\n      1\n      0\n      4\n      4\n    \n    \n      Ferrari Dino\n      19.7\n      6\n      145.0\n      175\n      3.62\n      2.770\n      15.50\n      0\n      1\n      5\n      6"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#sorting",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#sorting",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Sorting",
    "text": "Sorting\n\nAscending\n\ndf.mpg.sort_values().head(10)\n\nLincoln Continental    10.4\nCadillac Fleetwood     10.4\nCamaro Z28             13.3\nDuster 360             14.3\nChrysler Imperial      14.7\nMaserati Bora          15.0\nMerc 450SLC            15.2\nAMC Javelin            15.2\nDodge Challenger       15.5\nFord Pantera L         15.8\nName: mpg, dtype: float64\n\n\n\n\nDescending\nThere is no ‘descending’ per se, just set ascending to False.\n\ndf.mpg.sort_values(ascending = False).head(10)\n\nToyota Corolla    33.9\nFiat 128          32.4\nLotus Europa      30.4\nHonda Civic       30.4\nFiat X1-9         27.3\nPorsche 914-2     26.0\nMerc 240D         24.4\nDatsun 710        22.8\nMerc 230          22.8\nToyota Corona     21.5\nName: mpg, dtype: float64"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#applying-functions",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#applying-functions",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Applying functions",
    "text": "Applying functions\n\nMap a function with no arguments to a single column\n\n# some function to apply\ndef increment(x):\n    return x+1\n\n# could also be done as a lambda expression\ndf[\"ApplyResult\"] = df.mpg.map(increment)\n\ndf[[\"mpg\", \"ApplyResult\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      ApplyResult\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      22.0\n    \n    \n      Mazda RX4 Wag\n      21.0\n      22.0\n    \n    \n      Datsun 710\n      22.8\n      23.8\n    \n    \n      Hornet 4 Drive\n      21.4\n      22.4\n    \n    \n      Hornet Sportabout\n      18.7\n      19.7\n    \n  \n\n\n\n\n\n\nApply a function with arguments using kwargs\n\n# some function yadda\ndef incrementBy(x, by):\n    return x + by\n\n# could also be done as a lambda expression\ndf[\"ApplyResult\"] = df.mpg.apply(incrementBy, by = 5)\n\ndf[[\"mpg\", \"ApplyResult\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      ApplyResult\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      26.0\n    \n    \n      Mazda RX4 Wag\n      21.0\n      26.0\n    \n    \n      Datsun 710\n      22.8\n      27.8\n    \n    \n      Hornet 4 Drive\n      21.4\n      26.4\n    \n    \n      Hornet Sportabout\n      18.7\n      23.7"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#missing-values",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#missing-values",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Missing values",
    "text": "Missing values\nExpanding on an example from here. Pandas uses np.nan instead of NA.\n\ndf2 = pd.DataFrame({\n    \"x\": [1,2,3,4,5,np.nan,7,8,np.nan,10,11,12,np.nan],\n    \"y\": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,13]\n})\ndf2\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n    \n    \n      1\n      2.0\n      NaN\n    \n    \n      2\n      3.0\n      3.0\n    \n    \n      3\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n    \n    \n      5\n      NaN\n      6.0\n    \n    \n      6\n      7.0\n      NaN\n    \n    \n      7\n      8.0\n      8.0\n    \n    \n      8\n      NaN\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n    \n    \n      12\n      NaN\n      13.0\n    \n  \n\n\n\n\n\nMissing in the whole df\n\n# no NA the whole thing\ndf2.isnull()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      False\n      False\n    \n    \n      1\n      False\n      True\n    \n    \n      2\n      False\n      False\n    \n    \n      3\n      False\n      False\n    \n    \n      4\n      False\n      False\n    \n    \n      5\n      True\n      False\n    \n    \n      6\n      False\n      True\n    \n    \n      7\n      False\n      False\n    \n    \n      8\n      True\n      False\n    \n    \n      9\n      False\n      False\n    \n    \n      10\n      False\n      False\n    \n    \n      11\n      False\n      False\n    \n    \n      12\n      True\n      False\n    \n  \n\n\n\n\n\n\nMissing in a single column\n\n# just the x column\ndf2.x.isnull()\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5      True\n6     False\n7     False\n8      True\n9     False\n10    False\n11    False\n12     True\nName: x, dtype: bool\n\n\n\n\nReplace NAs in a column\n\n# use the fillna function \ndf2.x = df2.x.fillna(\"MISSING\")\ndf2\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n    \n    \n      1\n      2.0\n      NaN\n    \n    \n      2\n      3.0\n      3.0\n    \n    \n      3\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n    \n    \n      5\n      MISSING\n      6.0\n    \n    \n      6\n      7.0\n      NaN\n    \n    \n      7\n      8.0\n      8.0\n    \n    \n      8\n      MISSING\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n    \n    \n      12\n      MISSING\n      13.0\n    \n  \n\n\n\n\n\n\nReplace the NAs in the full dataset\n\n# files the other even though I didn't specify a column\ndf2.fillna(\"MISSING!!!\")\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n    \n    \n      1\n      2.0\n      MISSING!!!\n    \n    \n      2\n      3.0\n      3.0\n    \n    \n      3\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n    \n    \n      5\n      MISSING\n      6.0\n    \n    \n      6\n      7.0\n      MISSING!!!\n    \n    \n      7\n      8.0\n      8.0\n    \n    \n      8\n      MISSING\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n    \n    \n      12\n      MISSING\n      13.0\n    \n  \n\n\n\n\n\n\nDrop NAs\nThe official docs are great on this.\n\n# create another sample df\ndf3 = pd.DataFrame({\n    \"x\": [1,np.nan,3,4,5,np.nan,np.nan,8,np.nan,10,11,12,np.nan],\n    \"y\": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,np.nan],\n    \"z\": [1,2,np.nan,4,5,6,np.nan,8,9,10,11,12,np.nan]\n})\ndf3\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      NaN\n      NaN\n      2.0\n    \n    \n      2\n      3.0\n      3.0\n      NaN\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      5\n      NaN\n      6.0\n      6.0\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      8\n      NaN\n      9.0\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0\n    \n    \n      12\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nDrop all across the df\n\n# no NAs period \ndf3.dropna()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0\n    \n  \n\n\n\n\n\n\nDrop if they’re all na in that row\n\n# drop if the whole row is NA\ndf3.dropna(how = \"all\")\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      NaN\n      NaN\n      2.0\n    \n    \n      2\n      3.0\n      3.0\n      NaN\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      5\n      NaN\n      6.0\n      6.0\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      8\n      NaN\n      9.0\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0\n    \n  \n\n\n\n\n\n\nDrop all in certain columns\nThis is so clutch! Much simpler than filtering across in R (though there might be a cleaner way for that)\n\ndf3.dropna(subset = [\"x\", \"y\"])\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      2\n      3.0\n      3.0\n      NaN\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#group-summarize",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#group-summarize",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Group & Summarize",
    "text": "Group & Summarize\n\nGroup by a factor level\n\ndf[[\"mpg\", \"disp\", \"cyl\"]].groupby(by = \"cyl\")\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f7146769b40>\n\n\n\n\nGroupby factor level and get the mean\n\ndf[[\"mpg\", \"disp\", \"cyl\"]].groupby(by = \"cyl\").mean()\n\n\n\n\n\n  \n    \n      \n      mpg\n      disp\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      4\n      26.663636\n      105.136364\n    \n    \n      6\n      19.742857\n      183.314286\n    \n    \n      8\n      15.100000\n      353.100000"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#melting-a-dataframe",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#melting-a-dataframe",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Melting a DataFrame",
    "text": "Melting a DataFrame\n\n# use the melt function specifying ids and value vars\ndf_long = pd.melt(df, id_vars = \"cyl\", value_vars = [\"mpg\", \"wt\"])\ndf_long\n\n\n\n\n\n  \n    \n      \n      cyl\n      variable\n      value\n    \n  \n  \n    \n      0\n      6\n      mpg\n      21.000\n    \n    \n      1\n      6\n      mpg\n      21.000\n    \n    \n      2\n      4\n      mpg\n      22.800\n    \n    \n      3\n      6\n      mpg\n      21.400\n    \n    \n      4\n      8\n      mpg\n      18.700\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      59\n      4\n      wt\n      1.513\n    \n    \n      60\n      8\n      wt\n      3.170\n    \n    \n      61\n      6\n      wt\n      2.770\n    \n    \n      62\n      8\n      wt\n      3.570\n    \n    \n      63\n      4\n      wt\n      2.780\n    \n  \n\n64 rows × 3 columns"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#misc",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#misc",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Misc",
    "text": "Misc\n\nCounts of values (like table in R)\n\n# shows how many there are of each factor\ndf.cyl.value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n\nFormatting long chains\n\n# a little ugly but it works\ndf[[\"mpg\", \"disp\", \"cyl\"]].\\\n    query(\"cyl > 4\").\\\n    groupby(by = \"cyl\").\\\n    mean()\n\n\n\n\n\n  \n    \n      \n      mpg\n      disp\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      6\n      19.742857\n      183.314286\n    \n    \n      8\n      15.100000\n      353.100000\n    \n  \n\n\n\n\n\n\nIterating over a df\nFor mutating the dataframe apply/map is recommended, but I’m showing how to do this for completeness.\n\nfor i, j in df.iterrows():\n    print(df[\"mpg\"][i])\n    # j would give you all the columns with just that row\n\n21.0\n21.0\n22.8\n21.4\n18.7\n18.1\n14.3\n24.4\n22.8\n19.2\n17.8\n16.4\n17.3\n15.2\n10.4\n10.4\n14.7\n32.4\n30.4\n33.9\n21.5\n15.5\n15.2\n13.3\n19.2\n27.3\n26.0\n30.4\n15.8\n19.7\n15.0\n21.4\n\n\nhttps://www.statsmodels.org/stable/examples/notebooks/generated/glm.html https://www.geeksforgeeks.org/linear-regression-in-python-using-statsmodels/ https://datagy.io/pandas-iterate-over-rows/ https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html"
  }
]