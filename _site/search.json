[
  {
    "objectID": "wip/wtf/testing.html",
    "href": "wip/wtf/testing.html",
    "title": "Thadryan J. Sweeney, MS",
    "section": "",
    "text": "var = 1\n\n\ndef inc(x):\n    return x+1\n\n\ninc(var)\n\n2"
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "",
    "text": "In this post we explain and re-implement two multiple testing corrections, Bonferroni and Benjamini-Hochberg, for the sake of understanding."
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#preliminaries",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#preliminaries",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe prepare the libraries we’ll need.\n\n# data wrangling, numerical computing, visualization\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# statistical models\nimport statsmodels\nimport statsmodels.api as sm\n\n# existing tools to check our implementation against\nfrom statsmodels.stats.multitest import fdrcorrection\nfrom statsmodels.stats.multitest import multipletests   \n\n# random number generation\nfrom numpy.random import default_rng"
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#simulating-a-motivating-example",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#simulating-a-motivating-example",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Simulating a motivating example",
    "text": "Simulating a motivating example\nThe motivation behind multiple testing correction is that if you test enough hypotheses, you will eventually find a result just by chance. The relevance of this concern increases with the number of tests - some experiments test dozens, hundreds of hypotheses.\nWe will develop a motivating example with which to work. Initially, we will simulate p values from populations with the same parameters so we know the null hypothesis of a t test is true: they’re not different.\nWe can set this up nicely in Python using a function utilizing numpy and a list comprehension:\n\n# seed the results for reproducibility \nrng = default_rng(seed = 1)\n\n# write a simulation function\ndef simulation(mu1, sd1, mu2, sd2, draws = 100, n = 1000):\n    # generate different populations \n    pop1 = rng.normal(loc = mu1, scale = sd1, size = draws)\n    pop2 = rng.normal(loc = mu2, scale = sd2, size = draws)\n    # this returns three things, we only need the middle one\n    tstat, pval, degf = sm.stats.ttest_ind(pop1, pop2)\n\n    return pval\n\nThis function generations two populations. We can make them the same by passing the same mean and standard deviation. Now we can test it:\n\n# set parameters for our populations.\nmu1 = 5\nsd1 = 2.5\n\nmu2 = 5\nsd2 = 2.5\n\n# get a single p value\nsimulation(mu1, sd1, mu2, sd2)\n\n0.9993893617043748\n\n\nRecall that we’ve “simulated under the null” - we’ve specified that the parameters are the same for both distributions. Thus, we assume any positives are false. We can now repeat this a bunch of times to get a distribution of p values. We’re using an \\(\\alpha\\) (alpha) of 0.5 (more on this later), so we expect around 5% to be false positives.\n\n# set a seed\nrng = default_rng(seed = 1)\n\n# run a thousand simulations\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1001)]\n\n# get the ones below alpha\nsig_vals = [p for p in pvals if p &lt;= 0.05]\n\n# calculate the percent that are \"significant\"\nsig_percent = (len(sig_vals)/len(pvals))*100\n\n# percent significant hits\nround(sig_percent, 4)\n\n6.2937\n\n\nWe can see that we have a small but non-trivial amount of hits flagged as significant. If we think about the definition of a p value though, this makes sense.\n\np value: The probability of seeing evidence against the null that is this extreme or more given the null is true [@ 1] - given our modeling assumptions are met.\n\nIf we’re operating at \\(\\alpha = 0.05\\), we’ve accepting that we will be wrong about 5% of the time. In experiments where we’re testing a lot of hypotheses though, this adds up to a lot of mistakes.\nFor example, consider a differential expression experiment involving 20,000 genes:\n\n# 5 percent wrong in this case...\n0.05 * 20000\n\n1000.0\n\n\n…we’re talking about being wrong a thousand times. This is why multiple testing correction exists. We will consider two types with one example each."
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#controlling-family-wise-error-rate-fwer-with-the-bonferroni-method",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#controlling-family-wise-error-rate-fwer-with-the-bonferroni-method",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Controlling Family-wise Error Rate (FWER) with the Bonferroni method",
    "text": "Controlling Family-wise Error Rate (FWER) with the Bonferroni method\nBonferroni is perhaps the most common method for controlling the Family-wise error rate, which is the probability of making at least one false finding [2]. In this method we simply establish a new threshold for significance by dividing \\(\\alpha\\) by the number of tests.\n\n# a function to apply the correction\ndef calc_bonferroni_threshold(vector, alpha = 0.5):\n    # divide alpha by the number of tests, ie, pvalues in the vector\n    return alpha/len(vector)\n\n# compute a new threshold\nbonferroni_threshold = calc_bonferroni_threshold(pvals)\n\n# new threshold\nbonferroni_threshold\n\n0.0004995004995004995\n\n\n\n# see which ones are significant now\nsig_adj_pvals = [p for p in pvals if p &lt;= bonferroni_threshold]\n\n# calculate the percent\nsig_percent_adj = len(sig_adj_pvals)/len(pvals)\n\n# inspect \nround(sig_percent_adj, 4)\n\n0.0\n\n\nLet’s see if we also get 0 with Python’s version. This doesn’t return the threshold, it returns a list of corrected pvalues and a list of True/False values telling us to reject not. If our code agrees with this, we should see a vector with noTrues in it.\n\n# call the function\ncheck = multipletests(pvals, method=\"bonferroni\", alpha = 0.05)\n\n# the 0th index contains the booleans\ncheck_trues = [i for i in check[0] if i == True]\n\nlen(check_trues)/len(pvals)\n\n0.0\n\n\nWe’ve taken care of the false-positives. As you may have intuited, this is a pretty strict correction (dividing by 20k, in our other example); it does come at the cost of statistical power (the ability of a test to find something if it’s there). There is no free lunch, especially in statistical power. Sometimes a more permissive tradeoff can be made."
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#false-discovery-rate-fdr",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#false-discovery-rate-fdr",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\nWhile Bonferroni is tried and true, it’s something of a blunt instrument however, and there are some situations where another common method Benjamini-Hochberg [3], often abbreviated “BH”, is preferable. The Benjamini-Hochberg controls the False-discovery rate [3], which is the percent of the time we’re making a false positive. This is useful in situations where the strictness of Bonferroni might be limiting (more on comparing the two later). The method involves:\n\nRanking the p values from lowest to highest.\nComputing a critical value for each one.\nComparing the p value to the critical value.\nThe highest p value that is lower that the corresponding critical is the new threshold - it and all smaller than it are considered significant.\n\nThe formula for the critical value is:\n\\[\nc = (\\frac{r}{n})\\alpha\n\\]\nNote you will often see \\(\\alpha\\) denoted \\(q\\) - I’m just trying to keep things simple by calling it what it is (not exactly a time-honored tradition in statistics).\nThis is relatively straightforward to implement in Python (there is a built in function in R as well as a function in statsmodels for Python, but we will reimplement it for the sake of learning and transparency). We define the function below:\n\n# function to compute the new BH threshold\ndef calc_bh_threshold(vector, alpha = 0.05):\n\n    # the number of hypotheses\n    m = len(vector)\n    # the is just naming convention\n    q = alpha\n\n    # sort the pvalues\n    sorted_pvals = sorted(vector)\n\n    # collect a critical value for each p value\n    critical_vals = []\n    for i in range(0, len(sorted_pvals)):\n        rank = i+1 # the rank is the index +1 as Python is zero-indexed\n        # the formula for \n        critical_val = (rank/m)*q\n        critical_vals.append(critical_val)\n    \n    # organize our results\n    df_res = pd.DataFrame({\n        \"pvalue\": sorted_pvals,\n        \"critical_value\": critical_vals\n    })\n    # get the values where the pvalue is lower than the critical value\n    df_res_p_less_crit = df_res.query(\"pvalue &lt;= critical_value\").\\\n        sort_values(by = \"pvalue\", ascending = False)\n    \n    # if none meet the criteria return 0 so no comparison to it will be significant\n    if len(df_res_p_less_crit) == 0:\n        return 0\n    else:\n        # the largest of these is the new threshold\n        return df_res_p_less_crit[\"pvalue\"].max()\n\nbh_threshold = calc_bh_threshold(pvals)\nbh_threshold\n\n0\n\n\nNow we can compare apply this method to correcting for multiple testing on our p values (some of which we know are false positives).\n\n# organize the p values\ndf_res = pd.DataFrame({\"pval\": pvals})\n\n# apply a test - are they lower than the new threshold?\ndf_res[\"bg_reject\"] = df_res.pval.apply(lambda x: x &lt;= bh_threshold)\n\n# find where we reject the null (false positive)\nsig_adj_bh = df_res.query(\"bg_reject == True\")\n\n# get the new percent\nlen(sig_adj_bh)/len(pvals)\n\n0.0\n\n\nWe see we no longer get significant p values. Just to make sure we’re correct, we can simulate where the mean actually is different and see if our implementation agrees with the official one.\n\n# here, the means are different\nmu1 = 5\nsd1 = 1\n\nmu2 = 5.5\nsd2 = 1\n\n# create a another simulated vector of p values\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1000)]\n\n# organize it\ndf_check = pd.DataFrame({\"pval\": pvals})\n\n# put some ids to identify them by for better comparison\ndf_check[\"sample_id\"] = [\"sample\" + str(i+1) for i in range(0, len(pvals))]\n\n# get a new threshold\nbh_threshold_check = calc_bh_threshold(df_check[\"pval\"])\nbh_threshold_check\n\n0.04645320875671015\n\n\nWe’ve got our new threshold, now we can compare the p values to it.\n\n# compare the values in the pval function to the new criteria and see how many we get\ndf_check[\"bh_reject_custom\"] = df_check.pval.apply(lambda x: x &lt;= bh_threshold_check)\n\n# a count of reject true/false\ndf_check.bh_reject_custom.value_counts()\n\nTrue     931\nFalse     69\nName: bh_reject_custom, dtype: int64\n\n\nNow we compare that to the statsmodels implementation:\n\n# apply the statsmodels function to the p values\ndf_check[\"bh_reject_default\"] = fdrcorrection(df_check[\"pval\"])[0]\n\n# a table of reject true/false\ndf_check.bh_reject_default.value_counts()\n\nTrue     931\nFalse     69\nName: bh_reject_default, dtype: int64\n\n\nLet’s asses the agreement by making a column based on their comparison:\n\n# do they agree?\ndf_check[\"agreement\"] = \\\n    df_check[\"bh_reject_custom\"] == df_check[\"bh_reject_default\"]\n\n# see where they disagree\n[i for i in df_check[\"agreement\"] if i == False]\n\n[]\n\n\nNo disagreement. Just to be absolutely safe, we check the IDs of the rejected p values.\n\n# subset where the custom rejects\ndf_check_custom_agree = df_check.query(\"bh_reject_custom == True\")\n\n# a subset where the default rejects\ndf_check_default_agree = df_check.query(\"bh_reject_default == True\")\n\n# compare the ids\nagreement = df_check_custom_agree.sample_id == \\\n  df_check_default_agree.sample_id\n\n# observe the concordance\nagreement.value_counts()\n\nTrue    931\nName: sample_id, dtype: int64"
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#when-to-use-these-methods",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#when-to-use-these-methods",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "When to use these methods?",
    "text": "When to use these methods?\nThis will depend on the particulars of the experiment in question and must be evaluated on a case-by-case basis. In rough terms however, it depends on if you need to be absolutely sure about a particular result vs if you’re willing to get a few false positives. For example, if you’re conducting a follow-up study to confirm a result before committing a large amount of time and money to a line of research, Bonferroni makes more sense - making even a single error is costly, and this is what the FWER tells us (the probability of even a single mistake). If you’re doing a preliminary experiment to nominate targets for further study, then a few false positives might be fine. You might nominate 10-15 targets to be followed up on in the lab, and if only a few pan out, that’s ok, they’re the new focus. This is common if differential expression experiments [1]."
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache = True).data\n\ndf.head()\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\nSeaborn seems to be the most efficient way to get decent looking exploratory plots in a hurry.\n\n\n\nsns.lineplot(df, x = \"mpg\", y = \"disp\")\n\n&lt;AxesSubplot: xlabel='mpg', ylabel='disp'&gt;\n\n\n\n\n\n\n\nUse the hue argument to break out factors into separate lines.\n\nsns.lineplot(df, x = \"mpg\", y = \"disp\", hue = \"am\")\n\n&lt;AxesSubplot: xlabel='mpg', ylabel='disp'&gt;\n\n\n\n\n\n\n\n\nMean/CI are automatic if you melt the df.\n\ndf_long = pd.melt(df, id_vars = \"cyl\", value_vars = \"mpg\")\n\ndf_long\n\n\n\n\n\n\n\n\ncyl\nvariable\nvalue\n\n\n\n\n0\n6\nmpg\n21.0\n\n\n1\n6\nmpg\n21.0\n\n\n2\n4\nmpg\n22.8\n\n\n3\n6\nmpg\n21.4\n\n\n4\n8\nmpg\n18.7\n\n\n5\n6\nmpg\n18.1\n\n\n6\n8\nmpg\n14.3\n\n\n7\n4\nmpg\n24.4\n\n\n8\n4\nmpg\n22.8\n\n\n9\n6\nmpg\n19.2\n\n\n10\n6\nmpg\n17.8\n\n\n11\n8\nmpg\n16.4\n\n\n12\n8\nmpg\n17.3\n\n\n13\n8\nmpg\n15.2\n\n\n14\n8\nmpg\n10.4\n\n\n15\n8\nmpg\n10.4\n\n\n16\n8\nmpg\n14.7\n\n\n17\n4\nmpg\n32.4\n\n\n18\n4\nmpg\n30.4\n\n\n19\n4\nmpg\n33.9\n\n\n20\n4\nmpg\n21.5\n\n\n21\n8\nmpg\n15.5\n\n\n22\n8\nmpg\n15.2\n\n\n23\n8\nmpg\n13.3\n\n\n24\n8\nmpg\n19.2\n\n\n25\n4\nmpg\n27.3\n\n\n26\n4\nmpg\n26.0\n\n\n27\n4\nmpg\n30.4\n\n\n28\n8\nmpg\n15.8\n\n\n29\n6\nmpg\n19.7\n\n\n30\n8\nmpg\n15.0\n\n\n31\n4\nmpg\n21.4\n\n\n\n\n\n\n\n\nsns.lineplot(df_long, x = \"cyl\", y = \"value\")\n\n&lt;AxesSubplot: xlabel='cyl', ylabel='value'&gt;\n\n\n\n\n\nOr, use lmplot to git a linear model like you’d get with geom_smooth(method = lm).\n\nsns.lmplot(df, x = \"mpg\", y = \"disp\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f5d567270d0&gt;\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(df, x = \"cyl\", y = \"mpg\")\n\n&lt;AxesSubplot: xlabel='cyl', ylabel='mpg'&gt;\n\n\n\n\n\n\n\n\n\nsns.histplot(df, x = \"wt\")\n\n&lt;AxesSubplot: xlabel='wt', ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df, x = \"disp\")\n\n&lt;AxesSubplot: xlabel='disp', ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\nsns.heatmap(df[[\"disp\", \"hp\"]])\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df.loc[:, [\"mpg\", \"wt\"]])\n\n&lt;AxesSubplot: ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\nsns.lineplot(df.loc[:, [\"mpg\", \"wt\"]])\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\n\n\n\n\n# create three empty spots\ngrid = sns.FacetGrid(data = df, col = \"cyl\", col_wrap=2)\n\n# puts a historgram on each of them\ngrid.map(sns.histplot, \"wt\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f5d55cf2590&gt;\n\n\n\n\n\nThe initial display is automatic. If you want to show the same plot again, access the figure property of the object.\n\n# just typing it out gives the object metadata\ngrid\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f5d55cf2590&gt;\n\n\n\ngrid.figure\n\n\n\n\n\n\n\n\n\nThe plot we made of weight and mpg had mostly unusable x tick labels. Let’s revist it.\n\np_line = sns.lineplot(df.loc[:, [\"mpg\", \"wt\"]])\np_line.figure \n\n\n\n\n\n\n\nThe syntax is a little awkward. Essentially there is a set method, and you use a get method to retrieve the labels to pass into it, specifying a rotation.\n\n# set what you get from the get method v--here\np_line.set_xticklabels(p_line.get_xticklabels(), rotation = 45)\np_line.figure\n\n/tmp/ipykernel_9407/667538073.py:2: UserWarning: FixedFormatter should only be used together with FixedLocator\n  p_line.set_xticklabels(p_line.get_xticklabels(), rotation = 45)\n\n\n\n\n\nThey still conflict a little. We can make them a little smaller overall. The technique is the same, just setting a different property.\n\np_line.set_xticklabels(p_line.get_xticklabels(), size = 5)\np_line.figure\n\n/tmp/ipykernel_9407/3729791072.py:1: UserWarning: FixedFormatter should only be used together with FixedLocator\n  p_line.set_xticklabels(p_line.get_xticklabels(), size = 5)\n\n\n\n\n\n\n\n\n\np_line.set(title = \"0_o\")\n\np_line.figure\n\n\n\n\n\n\n\n\n\nSeaborn lets you preview color palettes by calling them as a function argument to sns.color_palette.\n\nsns.color_palette(\"dark\")\n\n\n\n\nThe plotting functions will then have arguments for color scheming:\n\np_box = sns.boxplot(df, x = \"cyl\", y = \"mpg\", palette = \"dark\")\n\n\n\n\n\n\n\n\nsns.color_palette(\"mako\", as_cmap = True)\n\nmako  underbad over \n\n\n\nsns.heatmap(df[[\"disp\", \"hp\"]], cmap = \"mako\")\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#line-plot",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#line-plot",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "sns.lineplot(df, x = \"mpg\", y = \"disp\")\n\n&lt;AxesSubplot: xlabel='mpg', ylabel='disp'&gt;\n\n\n\n\n\n\n\nUse the hue argument to break out factors into separate lines.\n\nsns.lineplot(df, x = \"mpg\", y = \"disp\", hue = \"am\")\n\n&lt;AxesSubplot: xlabel='mpg', ylabel='disp'&gt;\n\n\n\n\n\n\n\n\nMean/CI are automatic if you melt the df.\n\ndf_long = pd.melt(df, id_vars = \"cyl\", value_vars = \"mpg\")\n\ndf_long\n\n\n\n\n\n\n\n\ncyl\nvariable\nvalue\n\n\n\n\n0\n6\nmpg\n21.0\n\n\n1\n6\nmpg\n21.0\n\n\n2\n4\nmpg\n22.8\n\n\n3\n6\nmpg\n21.4\n\n\n4\n8\nmpg\n18.7\n\n\n5\n6\nmpg\n18.1\n\n\n6\n8\nmpg\n14.3\n\n\n7\n4\nmpg\n24.4\n\n\n8\n4\nmpg\n22.8\n\n\n9\n6\nmpg\n19.2\n\n\n10\n6\nmpg\n17.8\n\n\n11\n8\nmpg\n16.4\n\n\n12\n8\nmpg\n17.3\n\n\n13\n8\nmpg\n15.2\n\n\n14\n8\nmpg\n10.4\n\n\n15\n8\nmpg\n10.4\n\n\n16\n8\nmpg\n14.7\n\n\n17\n4\nmpg\n32.4\n\n\n18\n4\nmpg\n30.4\n\n\n19\n4\nmpg\n33.9\n\n\n20\n4\nmpg\n21.5\n\n\n21\n8\nmpg\n15.5\n\n\n22\n8\nmpg\n15.2\n\n\n23\n8\nmpg\n13.3\n\n\n24\n8\nmpg\n19.2\n\n\n25\n4\nmpg\n27.3\n\n\n26\n4\nmpg\n26.0\n\n\n27\n4\nmpg\n30.4\n\n\n28\n8\nmpg\n15.8\n\n\n29\n6\nmpg\n19.7\n\n\n30\n8\nmpg\n15.0\n\n\n31\n4\nmpg\n21.4\n\n\n\n\n\n\n\n\nsns.lineplot(df_long, x = \"cyl\", y = \"value\")\n\n&lt;AxesSubplot: xlabel='cyl', ylabel='value'&gt;\n\n\n\n\n\nOr, use lmplot to git a linear model like you’d get with geom_smooth(method = lm).\n\nsns.lmplot(df, x = \"mpg\", y = \"disp\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f5d567270d0&gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#box-plot",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#box-plot",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "sns.boxplot(df, x = \"cyl\", y = \"mpg\")\n\n&lt;AxesSubplot: xlabel='cyl', ylabel='mpg'&gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#histogram",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#histogram",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "sns.histplot(df, x = \"wt\")\n\n&lt;AxesSubplot: xlabel='wt', ylabel='Count'&gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#density-plot",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#density-plot",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "sns.kdeplot(df, x = \"disp\")\n\n&lt;AxesSubplot: xlabel='disp', ylabel='Density'&gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#heatmap",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#heatmap",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "sns.heatmap(df[[\"disp\", \"hp\"]])\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#multiple-variable-plots",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#multiple-variable-plots",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "sns.kdeplot(df.loc[:, [\"mpg\", \"wt\"]])\n\n&lt;AxesSubplot: ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\nsns.lineplot(df.loc[:, [\"mpg\", \"wt\"]])\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#faceting",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#faceting",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "# create three empty spots\ngrid = sns.FacetGrid(data = df, col = \"cyl\", col_wrap=2)\n\n# puts a historgram on each of them\ngrid.map(sns.histplot, \"wt\")\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f5d55cf2590&gt;\n\n\n\n\n\nThe initial display is automatic. If you want to show the same plot again, access the figure property of the object.\n\n# just typing it out gives the object metadata\ngrid\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f5d55cf2590&gt;\n\n\n\ngrid.figure"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html#tweaking-plots",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html#tweaking-plots",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "The plot we made of weight and mpg had mostly unusable x tick labels. Let’s revist it.\n\np_line = sns.lineplot(df.loc[:, [\"mpg\", \"wt\"]])\np_line.figure \n\n\n\n\n\n\n\nThe syntax is a little awkward. Essentially there is a set method, and you use a get method to retrieve the labels to pass into it, specifying a rotation.\n\n# set what you get from the get method v--here\np_line.set_xticklabels(p_line.get_xticklabels(), rotation = 45)\np_line.figure\n\n/tmp/ipykernel_9407/667538073.py:2: UserWarning: FixedFormatter should only be used together with FixedLocator\n  p_line.set_xticklabels(p_line.get_xticklabels(), rotation = 45)\n\n\n\n\n\nThey still conflict a little. We can make them a little smaller overall. The technique is the same, just setting a different property.\n\np_line.set_xticklabels(p_line.get_xticklabels(), size = 5)\np_line.figure\n\n/tmp/ipykernel_9407/3729791072.py:1: UserWarning: FixedFormatter should only be used together with FixedLocator\n  p_line.set_xticklabels(p_line.get_xticklabels(), size = 5)\n\n\n\n\n\n\n\n\n\np_line.set(title = \"0_o\")\n\np_line.figure\n\n\n\n\n\n\n\n\n\nSeaborn lets you preview color palettes by calling them as a function argument to sns.color_palette.\n\nsns.color_palette(\"dark\")\n\n\n\n\nThe plotting functions will then have arguments for color scheming:\n\np_box = sns.boxplot(df, x = \"cyl\", y = \"mpg\", palette = \"dark\")\n\n\n\n\n\n\n\n\nsns.color_palette(\"mako\", as_cmap = True)\n\nmako  underbad over \n\n\n\nsns.heatmap(df[[\"disp\", \"hp\"]], cmap = \"mako\")\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html",
    "href": "posts/2022-12-10-opiods/index.html",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "",
    "text": "A professor I’ll be working with in the winter term of 2022 recommended I read a review[1] of factors contributing to the opioid epidemic as part of my orientation to his work. It’s very good, recent, and approachable. It can be found in full here. I’d highly recommend it to anyone interested in learning more about the epidemic. My summary (largely an exercise for my own learning) is below for those pressed for time."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#what-are-opioids-opioids2022",
    "href": "posts/2022-12-10-opiods/index.html#what-are-opioids-opioids2022",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "What are opioids? [2]",
    "text": "What are opioids? [2]\nOpioids are a class of drugs used to treat pain by activating opioid receptors in the central nervous system. They are derived from the poppy plant, or are engineered to mimic substances that are. They are sometimes referred to as “narcotics”, “painkillers”, or “opiates”."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#what-is-opioid-use-disorder-opioidusedisorder2022",
    "href": "posts/2022-12-10-opiods/index.html#what-is-opioid-use-disorder-opioidusedisorder2022",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "What is opioid use disorder? [3]",
    "text": "What is opioid use disorder? [3]\nOpioid use disorder is characterized by compulsive use of opioid drugs that continues after a person wants to stop. It is characterized by cravings for opioids, risky use such as sharing needles, and withdrawal when they are not taken."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#overview-of-the-opioid-epidemic-cerda2021",
    "href": "posts/2022-12-10-opiods/index.html#overview-of-the-opioid-epidemic-cerda2021",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "Overview of the opioid epidemic [1]",
    "text": "Overview of the opioid epidemic [1]\nThe opioid epidemic is a public health phenomenon in which the rates of opioid overdose deaths increased ninefold between 1999 and 2018.\n\nPhase 1: Prescription use in urban white populations [1]\nWhen the epidemic was first observed no particular racial trends were apparent. With time, the trend grew in white populations in urban regions with well developed supply chains. Evidence suggests black patients are less likely to receive opioids for similar conditions and pain levels than white patients, which is considered a possible explanation for this trend. Some increase in use was also noted in US Native populations.\n\n\nPhase 2: Illegal heroin, fentanyl, and demographic generalization. [1]\nIn the 2010s the trend extended to other demographics. State interventions aimed at decreasing prescription of opioids coincide with an increase in the use of illegal heroin, with some theorizing the regulations drove users to the underground market - evidence suggests a significant amount of illegal heroin users used prescription opioids first. The west coast illegal markets were largely based on “black tar” heroin from Mexico, whereas the east coast supply revolved around Columbian heroin. The east coast product was highly pure and less expensive, driving down the cost of the drug in general. In 2013 the illegal market saw the emergence of fentanyl, a drug that is easier to produce and 30-40 times stronger than heroin. This was especially true on the east coast and a surge of fentanyl-related deaths followed.\n\n\nGeographic shifts [1]\nAs the epidemic evolved, the geographic regions changed, moving from urban regions to Appalachia, the Midwest, and the Northeast. Areas most affected by deindustrialization show the worst increase, though the trend is observed in all states."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#theories-of-supply-and-demand",
    "href": "posts/2022-12-10-opiods/index.html#theories-of-supply-and-demand",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "Theories of supply and demand",
    "text": "Theories of supply and demand\nSome of the trends observed in the epidemic are consistent with basic economic theories.\n\nSupply [1]\nIn the 1990s, medical associations urged physicians to prioritize pain control in an effort to reduce chronic pain. Around the same time Purdue Pharmaceuticals released OxyContin, an extended release version of oxycodone. The company funded campaigns promoting the use of opioids, and the industry as a whole spent tens of millions of dollars promoting their use to physicians.\n\n\nDemand [1]\nOpioid use disorder is associated with poor access to education and employment opportunities by a significant body of literature, as is drug use overall. Increase in opioid use disorder trends strongly with deindustrialization. Further understanding recent trends in other demographics is an area of need for research."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#risk-factors-cerda2021",
    "href": "posts/2022-12-10-opiods/index.html#risk-factors-cerda2021",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "Risk factors [1]",
    "text": "Risk factors [1]\nWhite males are particularly vulnerable to risk associated with loss of employment, in particular in manufacturing related fields. Individuals with disabilities are also at increased risk of opioid misuse."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#policy",
    "href": "posts/2022-12-10-opiods/index.html#policy",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "Policy",
    "text": "Policy\nNumerous laws have targeted legal and illegal opioid use [1].\n\nSupply-side [1]\nSome laws target the prescribing of opioids. There is not currently a large body of evidence on these laws. Short-term evidence does not indicate an effect on misuse, and there is some evidence restrictions on prescribing increases demand in the illegal market. There is evidence programs that monitor prescribing closely decrease deaths from prescription drugs but are also associated with an increase in deaths from illegal opioids. As the epidemic evolved, providers became more aware and guidelines may have decreased prescribing overall.\n\nCriminalization [1]\nCriminalization incentivizes the use of illegal drugs which are becoming cheaper and more dangerous. It is also the most widespread intervention. There are decades of evidence that the policies promoted by the War on Drugs have little impact on the use of drugs. Stricter laws theoretically intended to punish serious traffickers are used to charge friends and families of users nearly half the time they’re used. Many incarcerated individuals used opioids prior to arrest, and individuals recently released from prison are at increased risk of overdose.\n\n\nHarm reduction [1]\nHarm reduction measures aim to promote use of drugs in ways that have a lower probability of a bad outcome.\n\n“Good Samaritan” laws [1]\nGood Samaritan Laws aim to increase the willingness of individuals using drugs to contact emergency services by offering them legal protections. In theory, someone using drugs and witnessing another individual overdose could get them medical attention with less fear of reprisal. Evidence of the effectiveness of these laws is limited and mixed. Understanding their impact is an area of research need.\n\n\nNaloxone access [1]\nNaloxone (Narcan) is a drug that counteracts the respiratory effects of an overdose. Evidence from regions that implement laws to facilitate access to it shows a decrease in deaths from overdose. Concern that access to treatment with Naloxone or easing punishments for drug crimes will increase use it cited as a reason for hesitancy in these interventions. Initial evidence showed an increase in use in a before/after study of these interventions, though it was a single study and did not control for increasing use in general. Two more recent studies show no increase in use with access to this tool.\n\n\nSafer use practices [1]\nSafer-use measures may take the form of needle exchanges or safe-use facilities where drugs are taken in a controlled environment. Safe use measures are associated with a decrease in deaths in other countries (non-US). There are currently campaigns to attempt these measures in New York and Philadelphia, though “NIMBYism” (“Not In My Back Yard”-ism)i is a possible barrier. Fentanyl detection strips allow users to asses if they have a product contaminated by stronger drugs. Evidence suggests the strips decrease use somewhat but individuals do not appear willing to dispose of the product overall.\n\n\n\n\nDemand-side [1]\nThese approaches involve treatment of the disorder itself as a opposed to controlling the supply.\n\nTreatment [1]\nMedications that treat opioid use disorder have been studied and shown to reduce opioid use. There are three such medications: methadone, buprenorphine, and naltrexone. The first two are the gold standard of care given the amount of evidence accumulated over time to support their use, including multiple longitudinal studies. Crucially, however, the majority of those treated for opioid use disorder do not receive any such medications (commonly know as “Medications for opioid use disorder”, or MOUD). Most programs do not offer them. Many patients who receive them stop taking them. Studies of barriers to adoption of medications among treatment providers show regulatory and fiscal obstacles as driver of this difficulty. Clinics offering methadone must receive a specific license from the Drug Enforcement Agency (DEA), and patients must visit the clinic daily. Buprenorphine can be given as a take-home treatment, but the DEA requires prescribing physicians to obtain a waiver for this use. Medication-based programs often have rigid compliance standards for additional services like therapy and some demand total abstinence from other substances, increasing the difficulty for some patients."
  },
  {
    "objectID": "posts/2022-12-10-opiods/index.html#future-research-cerda2021",
    "href": "posts/2022-12-10-opiods/index.html#future-research-cerda2021",
    "title": "Summarizing a review of contributors to the opioid epidemic",
    "section": "Future research [1]",
    "text": "Future research [1]\nMore research is needed on the precise differences in usage across demographics so that the way the epidemic impacts different groups can be understood and addressed. The treatment of chronic pain must evolve such that people who need treatment are not left under-treated for fear of prescribing opioids. Research must engage with newer trends in fentanyl and multi-drug use, and other groups in addition to deindustrialized white communities. Further research is also needed in the area of stigma and how it informs policy, as evidence suggests that it is a barrier to adoption of certain treatments.\n\nproof read: @milliebotreads"
  },
  {
    "objectID": "posts/2021-01-26-mlk/index.html",
    "href": "posts/2021-01-26-mlk/index.html",
    "title": "In Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited",
    "section": "",
    "text": "For most of the last few years, I’ve made wordclouds of famous speeches MLK to mark the holiday that bears his name - they’re here and here. This year, I revisted the “I Have A Dream” speech with some new knowledge and tools and created the following as I reflected on the importance of the legacy of the day."
  },
  {
    "objectID": "posts/2021-01-26-mlk/index.html#initials",
    "href": "posts/2021-01-26-mlk/index.html#initials",
    "title": "In Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited",
    "section": "Initials",
    "text": "Initials\n\n\n\nwordcloud"
  },
  {
    "objectID": "posts/2021-01-26-mlk/index.html#face",
    "href": "posts/2021-01-26-mlk/index.html#face",
    "title": "In Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited",
    "section": "Face",
    "text": "Face\n\n\n\nwordcloud"
  },
  {
    "objectID": "posts/2021-01-26-mlk/index.html#circle-cloud",
    "href": "posts/2021-01-26-mlk/index.html#circle-cloud",
    "title": "In Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited",
    "section": "Circle Cloud",
    "text": "Circle Cloud\n\n\n\nwordcloud\n\n\nThe source code leverages the wordcloud and wordcloud2 packages and is published here."
  },
  {
    "objectID": "posts/2021-01-08-preprint-and-app/index.html",
    "href": "posts/2021-01-08-preprint-and-app/index.html",
    "title": "An interactive Bayseian app for interpretation of SARS-CoV-2 antibody tests",
    "section": "",
    "text": "Our lab has been working with a number of collaborators on a study of lateral flow assays for the detection of SARS-CoV-2 antibodies. We’ve designed and implemented an interactive app to complement the study. Interested parties getting an antibody test are advised to speak with their healthcare provider about interpreting their results with that app, which uses local COVID19 prevelance data from usafacts.org (also used by the CDC) and measurments from the study to estimate the probability they do or do not have SARS-CoV-2 antibodies given their result."
  },
  {
    "objectID": "posts/2020-07-10-hmd-removed-quote/index.html",
    "href": "posts/2020-07-10-hmd-removed-quote/index.html",
    "title": "Evidence Heather Mac Donald Presented A Pre-print Claim That Was Remove In Peer Review As Scientific",
    "section": "",
    "text": "Lately I’ve been practicing some statistical skepticism in regards to a recent op-ed that contained a lot of very obvious statistical and journalistic errors.\nOne of the issues I describe is Mac Donald using of a quote that doesn’t appear in the paper she is citing:"
  },
  {
    "objectID": "posts/2020-07-10-hmd-removed-quote/index.html#the-latest-in-a-series-of-studies-undercutting-the-claim-of-systemic-police-bias-was-published-in-august-2019-in-the-proceedings-of-the-national-academy-of-sciences.-the-researchers-found-that-the-more-frequently-officers-encounter-violent-suspects-from-any-given-racial-group-the-greater-the-chance-that-a-member-of-that-group-will-be-fatally-shot-by-a-police-officer.-there-is-no-significant-evidence-of-antiblack-disparity-in-the-likelihood-of-being-fatally-shot-by-police-they-concluded.",
    "href": "posts/2020-07-10-hmd-removed-quote/index.html#the-latest-in-a-series-of-studies-undercutting-the-claim-of-systemic-police-bias-was-published-in-august-2019-in-the-proceedings-of-the-national-academy-of-sciences.-the-researchers-found-that-the-more-frequently-officers-encounter-violent-suspects-from-any-given-racial-group-the-greater-the-chance-that-a-member-of-that-group-will-be-fatally-shot-by-a-police-officer.-there-is-no-significant-evidence-of-antiblack-disparity-in-the-likelihood-of-being-fatally-shot-by-police-they-concluded.",
    "title": "Evidence Heather Mac Donald Presented A Pre-print Claim That Was Remove In Peer Review As Scientific",
    "section": "‘The latest in a series of studies undercutting the claim of systemic police bias was published in August 2019 in the Proceedings of the National Academy of Sciences. The researchers found that the more frequently officers encounter violent suspects from any given racial group, the greater the chance that a member of that group will be fatally shot by a police officer. There is “no significant evidence of antiblack disparity in the likelihood of being fatally shot by police,” they concluded.’",
    "text": "‘The latest in a series of studies undercutting the claim of systemic police bias was published in August 2019 in the Proceedings of the National Academy of Sciences. The researchers found that the more frequently officers encounter violent suspects from any given racial group, the greater the chance that a member of that group will be fatally shot by a police officer. There is “no significant evidence of antiblack disparity in the likelihood of being fatally shot by police,” they concluded.’\nIn the WSJ piece, the link to the Johnson paper is offered as a source despite the fact that it doesn’t contain the quote. It’s hard not to see this as deceitful. Mac Donald refers to peer-reviewed work, links to it, but then pulls a quote from a pre-print that ended up getting removed prior to publication. It’s an intellectual slight-of-hand - invoking rigor and credibility but slipping in something unsavory instead.\nAnd to be clear on the timing of all this, the preprint was published August 16 of 2019. The peer-reviewed reply was published January 21, 2020. The WSJ op-ed was published June 2nd of 2020. Both were available to Mac Donald, indeed the peer-reviewed version is linked to at the top of the page of the Johnson paper she sends us to."
  },
  {
    "objectID": "posts/2020-06-27-the-myth-of-police-racism-debunk1/index.html",
    "href": "posts/2020-06-27-the-myth-of-police-racism-debunk1/index.html",
    "title": "Video - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors",
    "section": "",
    "text": "UPDATE: One of the articles misused in the piece has been retracted, with the authors calling out Mac Donald by name as misusing their work.\n\n\nUPDATE (07/11/2020): The mysterious quote has been found and unleashed a new error. Brief rundown here, details to follow.\n\nA recent op-ed that ran in the Wall Street Journal contains numerous severe statistical and journalistic errors and makes an enormous claim which is fails to defend. I’ve created a YouTube video showing my research, explaining the errors, and placing them in context.\n\nThe piece relies heavily on research about police shootings specifically to make claims about police generally. This ignores literally every other way police can use force.\nThe piece cites a study which prompted two follow-up papers that found errors and contradictory results, and in parts actually contradicts the claim the author is trying to make anyway.\nThe piece also cites well-known research out of context which actually contradicts their claim when taken in context\nThe author appears to fabricate a quote to make the piece sound more definitive (or something)."
  },
  {
    "objectID": "posts/2020-03-30-arch-linux-virtual-box/index.html",
    "href": "posts/2020-03-30-arch-linux-virtual-box/index.html",
    "title": "How to install Arch Linux on a VirtualBox VM",
    "section": "",
    "text": "There have been several walkthroughs on installing Arch Linux on VMs, like this one for example. However, I noticed there have been some changes that call for a few updates, so I thought I’d post a procedure that included them. As of the date of this post, this guide will produce a working system from the ground up. I also added some tweaks to get the most out of the VM, creating and adding users, and installing a desktop environment.\n\n\nTL;DNR: Arch is great is you want a well-documented, minimal system that you can customize, has up-to-date software, and will teach you about Linux. It’s not great if you want something up and running quickly with out-of-the-box features or something that doesn’t update often.\nThe benefits and downsides or Arch have been debated extensively (ad nauseam?) all over the internet. This is my take on summarizing them in as briefly as possible while still giving a good enough overview that people can choose for themselves (this is purely from a practical standpoint and doesn’t get into more philosphical stuff like systemd).\n\n\n\nMinimal - Arch installs a minimal system without even so much as a desktop environment and not a lot of stuff to slow your system down. The user doesn’t have the hassle of tracking down extra software and figuring if it’s bloat or important.\n\n\nCustomizable - This minimal approach means you can install whatever you want. In particular, you get to choose a desktop without depending on that has been made into a flavor/edition/spin.\n\n\nWell-documented - The much-hyped Arch Wiki is, in fact, great.\n\n\nTransparent - It’s often said Arch is good if you want to learn more about GNU/Linux and how it works. Arch is helpful for this in my experience - I learned a lot tinkering under the hood.\n\n\nCurrent - Arch is a rolling release distro know for getting updates quickly.\n\n\n\n\n\n(Relatively) Complex to install - Arch has a more involved installation process that other distros; the minimal system doesn’t come without a trade-off. It’s less geared to getting a system setup quickly and easily than other distros.\n\n\nRequires some maintenance - every now and then (a handful of times a year maybe) a package will requires you to manually run a command or two to update properly. They’ll be noted on the front page of the Arch website, but if you let them build up, things can get weird. In theory, this increases the chances that some update will screw up your machine and require you too boot into a live medium and fix it.\n\n\n\n\n\nHead over to the Arch Linux download page and either torrent yourself an ISO or download one from one of the mirrors.\n\n\nVirtualBox can be downloaded here. For Windows/Mac people, select the installer from the list. When on Linux I prefer to go with my distro-specific package manager.\n\n\nThere are a few steps to the VM setup but they’re simple and the defaults will work in most cases. Once you’ve got VirtualBox running, click “New” to get things started.\n\nGive your machine a snappy name! VirtualBox will detect what system it is if it starts with “Arch”. If it doesn’t, you can select it from the dropdown. Choose “Arch Linux (64-bit)”.\n\nVBox will ask what you want for memory size. I usually double this just to make sure the machine isn’t sluggish, then click “Next”.\n\nWe want to create a disk, not load an existing one. Just click “Create”.\n\nWe can accept the default drive type and click “Next”.\n\nI use dynamically allocated by default, and it will work fine for our purposes.\n\nI put the size up to 24 GB in case we want to build out the system and try it for a daily driver for a while or something. It’s just a VM, so it’s no big deal!\n\nWe will now see the machine on the VBox menu on the left side of the screen.\n\nBefore we go ahead and start it and select our ISO, a few little tweaks make the VBox experience a lot more pleasant and fast. Click “Settings”. You’ll see a menu like this:\n\nNavigate to “System”.\n\nChoose the “Processor” tab, and increase the number of CPUs the VM is allowed to use (assuming you have more than one on the host machine). Then move over to “Display” on the menu on the left side of the screen.\n\nI like to increase the video to max as well. Exit the “Settings” menu by clicking “Ok”. We’re ready to fire up the machine and connect it to our Arch ISO. In the top menu click the green “Start” button.\n\nVBox will now ask you where your bootable media is. Click the folder icon. You’ll see a screen like this:\n\nClick the “Add” symbol in the left of the menu, navigate to wherever you downloaded your ISO, and select it with “Choose”, then click “Start”.\n\nSelect the first option, which boots us into a live Arch system. When you boot in, a series of message will flash by and then you’ll see something like this:\n\nNote: If the terminal seems cluttered, pressing ctl-l between commands will clear the screen. If the amount of black space on your screen looks different than mine it’s probably just that I’ve done this.\nHere goes!\n\n\n\n\n\n\nThis is where we prepare the hard drive for installation. There are many ways to do this, with separate partitions for boot, swap, etc, but for a simple test VM, we’re just going to install it all into one partition. We prepare it using the following command.\ncfdisk\nThis starts a partitioning tool. You’ll be given an option to select “label type” by moving the arrows up and down. We’re looking for dos.\n\nNext you will see a screen like the one below. You can move left and write with arrow keys to make a selection. Move to “New”, and hit “enter” to make a new partition of the virtual hard rive.\n\nHit enter to use all 24G.\n\nSelect “primary” when asked for type (extended partitions allow for sub-dividing space, which isn’t a concern since we’re using the whole virtual disk for one thing).\n\nNow move to the “Bootable” option on the left side of the screen and hit enter.\n\nNavigate to “Write” and hit enter to make these changes actually happen. If we were on a real machine, this would be the point of no return where we had erased the underlying system.\n\nYou will have to type “yes” to consent to the operation. The top of the screen should show a partition of 24G on /dev/sda1 with a * under “Boot”.\n\nNow we can head over to “Quit” and exit the partition editor tool.\n\n\n\n\nNext, we assign the file system type to the newly partitioned drive partition. ext4 is the default file system for Linux.\nmkfs.ext4 /dev/sda1\n\nYou will see some output from the results of the command. Now we mount the drive so we can install the base system:\nmount /dev/sda1 /mnt\n\n\n\nArch uses a utility called pacstrap utility to install the base Arch system. If you’re looking for an update, this is one place where things are different from previous tutorials. In previous versions only base and base-devel were needed here. Failing to include linux and linux-firmware will cause a failure to boot later on because this is the actual Linux kernel and associated content. This step will most likely take several minutes.\npacstrap /mnt base base-devel linux linux-firmware\nInitially, you’ll see:\n\n…and then something like:\n\nThis is the normal display of Arch syncing and installing packages, the equivalent of the apt update, etc procedure in Debian based systems. Hit enter to continue when prompted.\nWhen it’s done, it will look like this:\n\nWe now need to create a file system table. This is a record of our hard drive partitions assigned identifiers for efficient look-up. If you’re curious, you can view the help for the command like this:\ngenfstab -h\n\nTo use it, and redirect the output to the proper spot on our system, use this command:\ngenfstab /mnt &gt;&gt; /mnt/etc/fstab\nNow we use the Arch version of chroot. This creates an isolated environment that doesn’t have access to the main system in case we biff something (the extra arguments are where to put this environment and what shell to use).\narch-chroot /mnt /bin/bash\nYou should notice a change in the terminal prompt:\n\nWe will install nano, and simple text-editor, within chroot so we can do some manual configuration. pacman is the package manager for Arch. It’s fast, simple, and dearly beloved. -S means “sync”, as in “sync these packages with my machine”. This is also a divergence from previous tutorials.\npacman -S nano\n\nHit enter to accept the default and proceed with the installation. This should be quick.\n\n\n\nNow we’re free to edit some files:\nnano /etc/locale.gen\n\nWe’re doing this setup for English language systems, use whatever you’re looking for. Basically we’re just un-commenting the language locale we want. If you’re configuring an English language system, scroll until you find…\n#en_US.UTF-8 UTF-8\n…and un-comment it.\n\nPress ctr-x to exit nano. You’ll be asked if you want to save the modified buffer.\n\nType y, then hit enter when asked for the file name to save it as (it will default to the one we gave it using the nano command). We can now run the locale-gen command to generate the locale information.\nlocale-gen\n\nWe’ve generated to locale and made them available. Now we can set it as our choice. Enter the language info in a file called locale.conf.\nnano /etc/locale.conf \nEnter…\nLANG=en_US.UTF-8\n…into this file, and exit nano as before.\n\n\n\n\nNext up is time zones. We can see a list of the selections using this commands.\nls /usr/share/zoneinfo\n\nYou can see the options under each region using ls on one of the folders shown by the above command, ie…\nls /usr/share/zoneinfo/America\n…to see which region you’re in:\n\nWe then set the desired zone to our system localtime. This command creates a “symbolic link” from the zone info data, and associates that link with a new config file called etc/localtime. I’m in the northeastern US, so for me it’s:\nln –s /usr/share/zoneinfo/America/New_York /etc/localtime\nThis should just happen silently.\nNext is the “hardware” clock of our VM. Problems here can lead to trouble updating down the road, so it’s worth checking that it’s synced if you run into issues.\nhwclock --systohc --utc\nThis command is also silent when successful.\n\n\n\nNow we create a password for the root user.\npasswd # set your root password\nHere is what your screen should look like:\n\nPractically speaking, I think it’s worth making a main user at this point, though it isn’t technically required.\nuseradd -m -g users -G wheel -s /bin/bash username\nThe -m flag creates a /home directory for the new user. The -g flag specifies the group to add the user to. The -G flag refers to auxiliary groups to add the user to. In this case, they’re added to the wheel group, which will let them be a full admin. /bin/bash specifies what shell the user will have. It’s typical to use bash. Don’t forget to change username to the name you want.\n\nGive the user a password:\npasswd username\n\nNow, we allow our users in the wheel group to use sudo. The visudo command makes sure the edits to the file are syntactically legit so you don’t screw it up with a typo.\nEDITOR=nano visudo\nFind and un-comment the following line:\n%wheel ALL=(ALL) ALL\n\n\n\n\nNow to name the machine:\nnano /etc/hostname\nEnter the name you want and exit nano.\n\nNow we need to setup the network inside chroot so we can install the bootloader (up until now, we’ve been getting sweet internet goodness from the host machine, but we’re hidden away in chroot at the moment). We will install a network manager to enable this. This is a divergence from previous tutorials as well. Run the following command and hit enter to accept the install.\npacman -S dhcpcd\nWe’ve install the network manager, but we need to turn it on. We can do so using the systemctl command.\nsystemctl enable dhcpcd\n\nNow we can install grub, which will allow us to boot into our system. os-prober is sometimes installed at this step. It detects other operating systems if you dual boot, though that won’t be an issue with a VM. It’s good to know it exists however.\npacman –S grub os-prober\n\nNow that we have grub, we can install to our /dev/sda drive where is will be able to “see” the rest of our system:\ngrub-install /dev/sda\nIt should tell you it installed with no errors. This command will create a config file for grub from which we can customize it later if desired.\ngrub-mkconfig –o /boot/grub/grub.cfg\nYou should see something like this:\n\nThis is important: what it says it’s detecting is the Linux kernel we installed earlier. This means grub knows where to look when it’s time to boot. We can now exit the chroot environment:\nexit\nWe should have a working base system now. To test this, reboot:\nreboot \nYou should be back at the boot screen. Now, try booting into the existing OS to see our system.\n\nYou should see the grub login screen.\n\nSelect “Arch Linux”. You’ll be asked to log in via terminal. Type your username and password.\n\nWe’re good! If you want a desktop, you can choose whichever you like, and set it up. I’m going to use Gnome for this because it’s my go-to, but the point of Arch is that you can use whatever you want and configure it however you like. This will usually entail installing the desktop, and, if it’s not included in the desktop package, a display manager. First the install (this takes a while - Gnome is full-size desktop):\nsudo pacman -S gnome\nYou may see the following prompt (or a few like it):\n\nThey’re dependencies used by Gnome. Accepting the defaults is fine (In this case, it’s an audio component use in the Gnome package).\nNext, enable the Gnome display manager like this:\nsystemctl enable gdm.service\nThis will allow us to use the desktop after boot. Reboot, and you will be greeted with a the Gnome login screen.\nreboot\nYou should now have a working VM. I’d recommend checking out the Arch Wiki on tweaking the virtual machine for your host OS to allow for fullscreen, shared clipboard, and shared drives.\n\n\n\nhttps://wiki.archlinux.org/index.php/Installation_guide \nhttps://www.howtoforge.com/tutorial/install-arch-linux-on-virtualbox/ \n# now retired apparently  https://bigdaddylinux.com/easily-install-arch-linux-in-virtualbox/"
  },
  {
    "objectID": "posts/2020-03-30-arch-linux-virtual-box/index.html#pros-and-cons-or-arch",
    "href": "posts/2020-03-30-arch-linux-virtual-box/index.html#pros-and-cons-or-arch",
    "title": "How to install Arch Linux on a VirtualBox VM",
    "section": "",
    "text": "TL;DNR: Arch is great is you want a well-documented, minimal system that you can customize, has up-to-date software, and will teach you about Linux. It’s not great if you want something up and running quickly with out-of-the-box features or something that doesn’t update often.\nThe benefits and downsides or Arch have been debated extensively (ad nauseam?) all over the internet. This is my take on summarizing them in as briefly as possible while still giving a good enough overview that people can choose for themselves (this is purely from a practical standpoint and doesn’t get into more philosphical stuff like systemd).\n\n\n\nMinimal - Arch installs a minimal system without even so much as a desktop environment and not a lot of stuff to slow your system down. The user doesn’t have the hassle of tracking down extra software and figuring if it’s bloat or important.\n\n\nCustomizable - This minimal approach means you can install whatever you want. In particular, you get to choose a desktop without depending on that has been made into a flavor/edition/spin.\n\n\nWell-documented - The much-hyped Arch Wiki is, in fact, great.\n\n\nTransparent - It’s often said Arch is good if you want to learn more about GNU/Linux and how it works. Arch is helpful for this in my experience - I learned a lot tinkering under the hood.\n\n\nCurrent - Arch is a rolling release distro know for getting updates quickly.\n\n\n\n\n\n(Relatively) Complex to install - Arch has a more involved installation process that other distros; the minimal system doesn’t come without a trade-off. It’s less geared to getting a system setup quickly and easily than other distros.\n\n\nRequires some maintenance - every now and then (a handful of times a year maybe) a package will requires you to manually run a command or two to update properly. They’ll be noted on the front page of the Arch website, but if you let them build up, things can get weird. In theory, this increases the chances that some update will screw up your machine and require you too boot into a live medium and fix it."
  },
  {
    "objectID": "posts/2020-03-30-arch-linux-virtual-box/index.html#procedure",
    "href": "posts/2020-03-30-arch-linux-virtual-box/index.html#procedure",
    "title": "How to install Arch Linux on a VirtualBox VM",
    "section": "",
    "text": "Head over to the Arch Linux download page and either torrent yourself an ISO or download one from one of the mirrors.\n\n\nVirtualBox can be downloaded here. For Windows/Mac people, select the installer from the list. When on Linux I prefer to go with my distro-specific package manager.\n\n\nThere are a few steps to the VM setup but they’re simple and the defaults will work in most cases. Once you’ve got VirtualBox running, click “New” to get things started.\n\nGive your machine a snappy name! VirtualBox will detect what system it is if it starts with “Arch”. If it doesn’t, you can select it from the dropdown. Choose “Arch Linux (64-bit)”.\n\nVBox will ask what you want for memory size. I usually double this just to make sure the machine isn’t sluggish, then click “Next”.\n\nWe want to create a disk, not load an existing one. Just click “Create”.\n\nWe can accept the default drive type and click “Next”.\n\nI use dynamically allocated by default, and it will work fine for our purposes.\n\nI put the size up to 24 GB in case we want to build out the system and try it for a daily driver for a while or something. It’s just a VM, so it’s no big deal!\n\nWe will now see the machine on the VBox menu on the left side of the screen.\n\nBefore we go ahead and start it and select our ISO, a few little tweaks make the VBox experience a lot more pleasant and fast. Click “Settings”. You’ll see a menu like this:\n\nNavigate to “System”.\n\nChoose the “Processor” tab, and increase the number of CPUs the VM is allowed to use (assuming you have more than one on the host machine). Then move over to “Display” on the menu on the left side of the screen.\n\nI like to increase the video to max as well. Exit the “Settings” menu by clicking “Ok”. We’re ready to fire up the machine and connect it to our Arch ISO. In the top menu click the green “Start” button.\n\nVBox will now ask you where your bootable media is. Click the folder icon. You’ll see a screen like this:\n\nClick the “Add” symbol in the left of the menu, navigate to wherever you downloaded your ISO, and select it with “Choose”, then click “Start”.\n\nSelect the first option, which boots us into a live Arch system. When you boot in, a series of message will flash by and then you’ll see something like this:\n\nNote: If the terminal seems cluttered, pressing ctl-l between commands will clear the screen. If the amount of black space on your screen looks different than mine it’s probably just that I’ve done this.\nHere goes!\n\n\n\n\n\n\nThis is where we prepare the hard drive for installation. There are many ways to do this, with separate partitions for boot, swap, etc, but for a simple test VM, we’re just going to install it all into one partition. We prepare it using the following command.\ncfdisk\nThis starts a partitioning tool. You’ll be given an option to select “label type” by moving the arrows up and down. We’re looking for dos.\n\nNext you will see a screen like the one below. You can move left and write with arrow keys to make a selection. Move to “New”, and hit “enter” to make a new partition of the virtual hard rive.\n\nHit enter to use all 24G.\n\nSelect “primary” when asked for type (extended partitions allow for sub-dividing space, which isn’t a concern since we’re using the whole virtual disk for one thing).\n\nNow move to the “Bootable” option on the left side of the screen and hit enter.\n\nNavigate to “Write” and hit enter to make these changes actually happen. If we were on a real machine, this would be the point of no return where we had erased the underlying system.\n\nYou will have to type “yes” to consent to the operation. The top of the screen should show a partition of 24G on /dev/sda1 with a * under “Boot”.\n\nNow we can head over to “Quit” and exit the partition editor tool.\n\n\n\n\nNext, we assign the file system type to the newly partitioned drive partition. ext4 is the default file system for Linux.\nmkfs.ext4 /dev/sda1\n\nYou will see some output from the results of the command. Now we mount the drive so we can install the base system:\nmount /dev/sda1 /mnt\n\n\n\nArch uses a utility called pacstrap utility to install the base Arch system. If you’re looking for an update, this is one place where things are different from previous tutorials. In previous versions only base and base-devel were needed here. Failing to include linux and linux-firmware will cause a failure to boot later on because this is the actual Linux kernel and associated content. This step will most likely take several minutes.\npacstrap /mnt base base-devel linux linux-firmware\nInitially, you’ll see:\n\n…and then something like:\n\nThis is the normal display of Arch syncing and installing packages, the equivalent of the apt update, etc procedure in Debian based systems. Hit enter to continue when prompted.\nWhen it’s done, it will look like this:\n\nWe now need to create a file system table. This is a record of our hard drive partitions assigned identifiers for efficient look-up. If you’re curious, you can view the help for the command like this:\ngenfstab -h\n\nTo use it, and redirect the output to the proper spot on our system, use this command:\ngenfstab /mnt &gt;&gt; /mnt/etc/fstab\nNow we use the Arch version of chroot. This creates an isolated environment that doesn’t have access to the main system in case we biff something (the extra arguments are where to put this environment and what shell to use).\narch-chroot /mnt /bin/bash\nYou should notice a change in the terminal prompt:\n\nWe will install nano, and simple text-editor, within chroot so we can do some manual configuration. pacman is the package manager for Arch. It’s fast, simple, and dearly beloved. -S means “sync”, as in “sync these packages with my machine”. This is also a divergence from previous tutorials.\npacman -S nano\n\nHit enter to accept the default and proceed with the installation. This should be quick.\n\n\n\nNow we’re free to edit some files:\nnano /etc/locale.gen\n\nWe’re doing this setup for English language systems, use whatever you’re looking for. Basically we’re just un-commenting the language locale we want. If you’re configuring an English language system, scroll until you find…\n#en_US.UTF-8 UTF-8\n…and un-comment it.\n\nPress ctr-x to exit nano. You’ll be asked if you want to save the modified buffer.\n\nType y, then hit enter when asked for the file name to save it as (it will default to the one we gave it using the nano command). We can now run the locale-gen command to generate the locale information.\nlocale-gen\n\nWe’ve generated to locale and made them available. Now we can set it as our choice. Enter the language info in a file called locale.conf.\nnano /etc/locale.conf \nEnter…\nLANG=en_US.UTF-8\n…into this file, and exit nano as before.\n\n\n\n\nNext up is time zones. We can see a list of the selections using this commands.\nls /usr/share/zoneinfo\n\nYou can see the options under each region using ls on one of the folders shown by the above command, ie…\nls /usr/share/zoneinfo/America\n…to see which region you’re in:\n\nWe then set the desired zone to our system localtime. This command creates a “symbolic link” from the zone info data, and associates that link with a new config file called etc/localtime. I’m in the northeastern US, so for me it’s:\nln –s /usr/share/zoneinfo/America/New_York /etc/localtime\nThis should just happen silently.\nNext is the “hardware” clock of our VM. Problems here can lead to trouble updating down the road, so it’s worth checking that it’s synced if you run into issues.\nhwclock --systohc --utc\nThis command is also silent when successful.\n\n\n\nNow we create a password for the root user.\npasswd # set your root password\nHere is what your screen should look like:\n\nPractically speaking, I think it’s worth making a main user at this point, though it isn’t technically required.\nuseradd -m -g users -G wheel -s /bin/bash username\nThe -m flag creates a /home directory for the new user. The -g flag specifies the group to add the user to. The -G flag refers to auxiliary groups to add the user to. In this case, they’re added to the wheel group, which will let them be a full admin. /bin/bash specifies what shell the user will have. It’s typical to use bash. Don’t forget to change username to the name you want.\n\nGive the user a password:\npasswd username\n\nNow, we allow our users in the wheel group to use sudo. The visudo command makes sure the edits to the file are syntactically legit so you don’t screw it up with a typo.\nEDITOR=nano visudo\nFind and un-comment the following line:\n%wheel ALL=(ALL) ALL\n\n\n\n\nNow to name the machine:\nnano /etc/hostname\nEnter the name you want and exit nano.\n\nNow we need to setup the network inside chroot so we can install the bootloader (up until now, we’ve been getting sweet internet goodness from the host machine, but we’re hidden away in chroot at the moment). We will install a network manager to enable this. This is a divergence from previous tutorials as well. Run the following command and hit enter to accept the install.\npacman -S dhcpcd\nWe’ve install the network manager, but we need to turn it on. We can do so using the systemctl command.\nsystemctl enable dhcpcd\n\nNow we can install grub, which will allow us to boot into our system. os-prober is sometimes installed at this step. It detects other operating systems if you dual boot, though that won’t be an issue with a VM. It’s good to know it exists however.\npacman –S grub os-prober\n\nNow that we have grub, we can install to our /dev/sda drive where is will be able to “see” the rest of our system:\ngrub-install /dev/sda\nIt should tell you it installed with no errors. This command will create a config file for grub from which we can customize it later if desired.\ngrub-mkconfig –o /boot/grub/grub.cfg\nYou should see something like this:\n\nThis is important: what it says it’s detecting is the Linux kernel we installed earlier. This means grub knows where to look when it’s time to boot. We can now exit the chroot environment:\nexit\nWe should have a working base system now. To test this, reboot:\nreboot \nYou should be back at the boot screen. Now, try booting into the existing OS to see our system.\n\nYou should see the grub login screen.\n\nSelect “Arch Linux”. You’ll be asked to log in via terminal. Type your username and password.\n\nWe’re good! If you want a desktop, you can choose whichever you like, and set it up. I’m going to use Gnome for this because it’s my go-to, but the point of Arch is that you can use whatever you want and configure it however you like. This will usually entail installing the desktop, and, if it’s not included in the desktop package, a display manager. First the install (this takes a while - Gnome is full-size desktop):\nsudo pacman -S gnome\nYou may see the following prompt (or a few like it):\n\nThey’re dependencies used by Gnome. Accepting the defaults is fine (In this case, it’s an audio component use in the Gnome package).\nNext, enable the Gnome display manager like this:\nsystemctl enable gdm.service\nThis will allow us to use the desktop after boot. Reboot, and you will be greeted with a the Gnome login screen.\nreboot\nYou should now have a working VM. I’d recommend checking out the Arch Wiki on tweaking the virtual machine for your host OS to allow for fullscreen, shared clipboard, and shared drives.\n\n\n\nhttps://wiki.archlinux.org/index.php/Installation_guide \nhttps://www.howtoforge.com/tutorial/install-arch-linux-on-virtualbox/ \n# now retired apparently  https://bigdaddylinux.com/easily-install-arch-linux-in-virtualbox/"
  },
  {
    "objectID": "posts/2019-08-30-seasonality-in-gun-deaths/index.html",
    "href": "posts/2019-08-30-seasonality-in-gun-deaths/index.html",
    "title": "Seasonality in gun deaths",
    "section": "",
    "text": "Last year, I read an article on FiveThirtyEight about gun deaths in the US. Intrigued, and looking for some practice, I started poking around in the dataset myself. I noticed what seemed to an odd drop in February of each year, and it got me curious about seasonality in the dataset. I’ve returned to it to give it a proper look. If you’re curious, you can see the project here."
  },
  {
    "objectID": "posts/2019-01-31-more-rigorous-ash/index.html",
    "href": "posts/2019-01-31-more-rigorous-ash/index.html",
    "title": "A more rigorous ASH",
    "section": "",
    "text": "A while ago I shared a first run and an epitope analysis tool called ASH (Antigen Selection Heuristic). The tool aims to provide a rough estimate of chemical distinctness in two aligned protein sequences. I’ve since gone over the code, organized the repository, fixed a small bug, and added unit tests. The repo can be found here, and it includes a walk-through. Please let me know if you have any questions or feedback!"
  },
  {
    "objectID": "posts/2018-09-30-python-imports/index.html",
    "href": "posts/2018-09-30-python-imports/index.html",
    "title": "Python imports: Sample Project + Explanations",
    "section": "",
    "text": "Salutations!\nA friend of mine recently asked me for some pointers on Python import statements, and I thought I would share what I came up with. The import command allows us to use code from other libraries, directories, source files, etc. It’s a fundamental tools that’s been around since way before Python (think “#include &lt;stdio.h&gt;”, and further). The structure is easier to demonstrate if we break from our go-to Jupyter format, so have a look at the project here. Have a look at the code in utils.py, OtherUtils/utils\\_one\\_dir_down.py, and then read the mains!"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html",
    "href": "posts/2018-07-20-scala/index.html",
    "title": "A handshake with Scala",
    "section": "",
    "text": "Scala is a JVM language that seeks to marry functional and object-oriented programming styles. It was was Designed by Martin Odersky and first appeared in 2004. While I think many projects that aspire to “best of both worlds” status end up ugly and chimeric, my explorations with Scala have revealed a really thoughtful and lovely creation that elegantly merges different paradigms. While it isn’t one of the top-tier languages for for production code job hunting (think Java, C++, Python, JavaScript, etc), it made the GitHub top 15 list for most common languages recently. I suspect much of this is because it’s becoming popular as a language for data science as a happy medium language between the breezy-to-write-but-relatively-slow Python and the incumbent production veteran Java that is faster but comes with a notoriously verbose syntax (still very much worth learning though!). Scala aims to be approachable and quick to develop as well as fast (some tests show it performing at Java speed or better), thus avoiding the need to “productionize” slower, exploratory code if a project grows (The name “Scala” is a shortening of the “Scalable Language”, after all). Nobody is going to be putting Java out of business any time soon (just ask the flashier start-up favorite Ruby, but Scala joins Python, Java, and R as a helpful tool for data and general purpose coding. After spending a good amount of time with the Scala home page, “Scala for the Impatient”by Cay S. Horstmann, and Bruce Eckel’s classic “Thinking in Java”, I opted to create an overview for my own edification and for some people I know who are newcomers to the language. Happy Hacking.\n// hello world in Scala \nobject helloWorld {\n    def main(args: Array[String]): Unit = {\n        println(\"Hello World\")       \n    }\n}\n\n\ndefined object helloWorld\nIn Scala, a program takes place in a “singleton object”. This is an object of which only one iteration can exist. This is slightly different than Java, where it would be written as “class helloWorld {”.\nNext we see a “main” method defined like in Java. Also like Java, this function takes an Array of Strings as an argument ( args: Array[String]) ). The more unusual part is “Unit”. This is the Scala’s version of “void”, which means that the function doesn’t return anything (it just executes the code inside it).\nAfter that, there is a println() call that will look familiar to Java coders that prints the message.\nBecause we are using a notebook, we can write snippets of code outside an object like this, but keep in mind if we were writing a program in a more typical format it would need to be inside that object.\nFrom now on, we can just write:\nprintln(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#declaration",
    "href": "posts/2018-07-20-scala/index.html#declaration",
    "title": "A handshake with Scala",
    "section": "Declaration",
    "text": "Declaration\nIn Scala, variables are declared as “Val” or “Var”, for “Value” or “Variable”. A item declared as a “Val” cannot be changed (it is said to be “immutable”, where as a “Var” can (it is “mutable”). This a key part of Scala’s functional programming background.\n\n// mutable \nvar x = 1\n\n// immutable \nval y = 2\n\n\nx: Int = 1\ny: Int = 2\n\n\n\nThis means we could change x but we could not change y:\n\n// this is perfectly legal \nx = x + 1\nprintln(x)\n\n// this would kick a \"reassignment to val\" error \n// y = y + 1\n\n2\n\n\nThe next line of code demonstrates that Scala’s type declarations are “backwards” compared to more traditional languages. Scala would say “val age: Int = 10” not “Int age = 10”. According to Cay S. Horstmann, the author or “Scala for the Impatient”, this makes it easier to read the code when large, complicated functions are used. While this isn’t typical, Scala’s designers aren’t the only ones who think so:\nhttps://blog.golang.org/gos-declaration-syntax\nIt takes a bit of getting used to but is not a huge deal, and the clarity pays off later. I think of this as the programming equivalent of Romance languages that say “The dress red” instead of “The red dress”.\n\nval nameDeclared: String = \"Charli Declared\" // not \"val String: name = \"Charli\"\n\n\nnameDeclared: String = \"Charli Declared\"\n\n\n\nAlso, Scala has type inference, so we could use this shorthand:\n\n// no type declaration\nval nameInferred = \"Charli Inferred\"\nprintln(nameDeclared + \" \" + nameInferred)\n\nCharli Declared Charli Infered\n\n\n\nnameInffered: String = \"Charli Infered\""
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#types",
    "href": "posts/2018-07-20-scala/index.html#types",
    "title": "A handshake with Scala",
    "section": "Types",
    "text": "Types\nGiven it’s descent from Java and it’s intent to build large systems securely, Scala uses strong, static typing.\n\nval wholeNumber: Int = 10\n\nval decimalNumber: Double = 10.1\n\n// specify float \nval floatDecimalNumber: Float = 10.1f\n\nval bigNumber: Long = 100000000\n\n// note double quotes \nval string: String = \"Heya\"\n\n// note single quotes \nval aLetter: Char = 'A'\n\nval answer: Boolean = true\n\n// will be convere\nval doubleOverridesInt = wholeNumber + decimalNumber\n\n\nwholeNumber: Int = 10\ndecimalNumber: Double = 10.1\nfloatDecimalNumber: Float = 10.1F\nbigNumber: Long = 100000000L\nstring: String = \"Heya\"\naLetter: Char = 'A'\nanswer: Boolean = true\ndoubleOverridesInt: Double = 20.1\n\n\n\nScala types are built as classes, so it offers some convenience methods for conversion:\n\n100.toString\n\n\nres60: String = \"100\""
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#arithmetic-operators",
    "href": "posts/2018-07-20-scala/index.html#arithmetic-operators",
    "title": "A handshake with Scala",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\nArithmetic operators are fairly traditional in Scala:\nExponents are handled via Scala’s math library.\n\nvar x: Int = 10\n\n// call the power function from 'math'\nscala.math.pow(x,2)\n\n\nx: Int = 10\nres61_1: Double = 100.0"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#strings-iteration",
    "href": "posts/2018-07-20-scala/index.html#strings-iteration",
    "title": "A handshake with Scala",
    "section": "Strings + Iteration",
    "text": "Strings + Iteration\nLet’s look at some simple string manipulations.\n\nval name: String = \"Jade\"\n\n// println at an index\nprintln(name(0))\n\nJ\n\n\n\nname: String = \"Jade\"\n\n\n\nNote that the () is used, not the traditional []. This is due to Scala’s functional outlook, looking at the String as a function that maps an element to the String location of that element.\nLet’s change the case. Notice there are no () at the end of this method calls. If the method has parenthesis, they are optional if arguments are not being passed (if the method is declared with no parenthesis, however, you must follow suit).\n\n// change to upper case\nname.toUpperCase\n\n// to lower \nname.toLowerCase()\n\n\nres63_0: String = \"JADE\"\nres63_1: String = \"jade\"\n\n\n\nIterating over a string reveals some interesting traits of Scala. The String class has a “foreach” method, which, paired with the “default variable” and a bit of syntax, allows for iteration.\n\n// foreach is a method of the String name \nname.foreach{ letter =&gt;\n    println(letter)\n}\n\nJ\na\nd\ne\n\n\nWe can also use and index. It has an R-like arrow assignment to the i, and then an obvious, English-like syntax for “0 until the end of the length” of the name.\n\n// for i in until the end of the length \nfor(i &lt;- 0 until name.length) {\n    println(\"Letter at index \" + i + \" is \" + name(i))\n}\n\nLetter at index 0 is J\nLetter at index 1 is a\nLetter at index 2 is d\nLetter at index 3 is e\n\n\nWe can also use and index. It has an R-like arrow assignment to the i, and then an obvious, English-like syntax for “0 until the end of the length” of the name.\n\nfor(i &lt;- name.indices) {\n    println(name(i))\n}\n\nJ\na\nd\ne\n\n\nIn Scala 2.1+ you can interpolate strings with the s + $ syntax\n\nprintln(s\"Hello there, my name is $name\")\n\nHello there, my name is Jade\n\n\nThis also works with floats, using a C-style syntax specifying the number of decimals:\n\nval height: Double = 1.5\n\n// note the f\nprintln(f\"height: $height%.5f\")\n\nheight: 1.50000\n\n\n\nheight: Double = 1.5"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#arrays",
    "href": "posts/2018-07-20-scala/index.html#arrays",
    "title": "A handshake with Scala",
    "section": "Arrays",
    "text": "Arrays\nScala arrays are traditional in tha they are static; once you declare one to be a certain length, it can’t be changed. Interestingly, the individual elements of the array can be change:\n\n// declare and array - this uses type inference\nval numbers = Array(2,4,6,8,10)\n\n// could also be\n// val numbers: Array[Int] = Array(2,4,6,8,10)\n\n// access it by index (starting at 0)\nprintln(numbers(4))\n\n// it also has the foreach method\nnumbers.foreach { number =&gt;\n    println(number)\n}\n\n// update by index \nnumbers(1) = 2\n\n10\n2\n4\n6\n8\n10\n\n\n\nnumbers: Array[Int] = Array(2, 2, 6, 8, 10)\n\n\n\nThe Array class has the basic methods you might expect:\n\nnumbers.sum\nnumbers.min\nnumbers.max\n\n\nres70_0: Int = 28\nres70_1: Int = 2\nres70_2: Int = 10"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#mapping-arrays",
    "href": "posts/2018-07-20-scala/index.html#mapping-arrays",
    "title": "A handshake with Scala",
    "section": "Mapping Arrays",
    "text": "Mapping Arrays\nGiven Scala’s functional heritage, it’s common to see map function used to transform data structures.\n\n// declare an array\nval numbers: Array[Int] = Array(3,6,9,12,15)\n\n// map each \"x\" in the list to \"x * 2\" \nval doubledNumbers = numbers.map(x =&gt; x * 2)\n\n\nnumbers: Array[Int] = Array(3, 6, 9, 12, 15)\ndoubledNumbers: Array[Int] = Array(6, 12, 18, 24, 30)\n\n\n\nYou might be wondering if we can add conditionals to this basic idea to get generators or comprehensions. You bet, using yield.\n\n// create a new array\nval numbers = Array(5,10,15,20,25,30)\n\n// conditional in parenthesis if \nval over15 = for(element &lt;- numbers if element &gt; 15) yield element\n\n\nnumbers: Array[Int] = Array(5, 10, 15, 20, 25, 30)\nover15: Array[Int] = Array(20, 25, 30)"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#dynamic-arrays",
    "href": "posts/2018-07-20-scala/index.html#dynamic-arrays",
    "title": "A handshake with Scala",
    "section": "Dynamic Arrays",
    "text": "Dynamic Arrays\nWhat about a dynamic array? Scala’s standard library provides a Java-like ArrayBuffer:\n\n// import the data structure \nimport scala.collection.mutable.ArrayBuffer  \n\n// declare one, empty \nval dynamic = ArrayBuffer[Int]()\n\nprintln(dynamic.length)\n\n// note handy \"by\" option letting us count by \"2\"\nfor(i &lt;- 2 to 10 by 2) {\n    dynamic += i\n}\nprintln(dynamic.length)\n\n0\n5\n\n\n\n\nimport scala.collection.mutable.ArrayBuffer  \n// declare one, empty \ndynamic: ArrayBuffer[Int] = ArrayBuffer(2, 4, 6, 8, 10)\n\n\n\nGenerally speaking, Scala is very syntactically consistent; the basic methods have the same names as the regular array:\n\n\ndynamic.sum\ndynamic.min\ndynamic.max\n\n\nres74_0: Int = 30\nres74_1: Int = 2\nres74_2: Int = 10"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#logic-comparisons-and-control-flow",
    "href": "posts/2018-07-20-scala/index.html#logic-comparisons-and-control-flow",
    "title": "A handshake with Scala",
    "section": "Logic, Comparisons, and Control Flow",
    "text": "Logic, Comparisons, and Control Flow\nThe control flow in Scala is traditional and straightforward, strongly resembling and C-family language. The comparison operators will also seem familiar.\n\nval numbers = Array(1,2,3,4,5,6,7,8,9,10)\n\nnumbers.foreach{ number =&gt;\n    // check for even/odd with \"==\"\n    if(number % 2 == 0){\n        println(\"The number is even\")\n    }\n    // scala uses ! for negative comparisons\n    else if(number % 2 != 0){\n        println(\"The number is odd\")\n    }\n    // final condition, not activated here \n    else{\n        println(\"Does not compute!\")\n    }\n}\n\nThe number is odd\nThe number is even\nThe number is odd\nThe number is even\nThe number is odd\nThe number is even\nThe number is odd\nThe number is even\nThe number is odd\nThe number is even\n\n\n\nnumbers: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#functions",
    "href": "posts/2018-07-20-scala/index.html#functions",
    "title": "A handshake with Scala",
    "section": "Functions",
    "text": "Functions\nA simple function in Scala looks a bit like lambda expressions in other languages and can be defined like this:\n\n// think of =&gt; as \"yields\"\nval addTwoNumbers = (x: Int, y: Int) =&gt; x + y \n\naddTwoNumbers(4,5)\n\n\naddTwoNumbers: (Int, Int) =&gt; Int = ammonite.$sess.cmd76$Helper$$Lambda$2850/0x0000000800905a78@2ea8ff5a\nres76_1: Int = 9\n\n\n\nUsing def is for making a method (in this case of the main object)\n\n// notice there is a \":\" now \ndef introduceYourself(name: String): Unit = {\n    println(s\"Hello there, my name is $name\") // note s + $ for string interpolation - we'll see this again\n}\n\nintroduceYourself(\"Russ\")\n\nHello there, my name is Russ\n\n\n\ndefined function introduceYourself"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#default-arguments",
    "href": "posts/2018-07-20-scala/index.html#default-arguments",
    "title": "A handshake with Scala",
    "section": "Default Arguments",
    "text": "Default Arguments\nScala uses a Python-like default arguments to avoid method overloading (we use () because there is an arguments list in this function):\n\n// Simply add ' = \"Bonnie\"' and you're good to go\ndef introduceYourself(name: String = \"Bonnie\"): Unit = {\n    println(s\"Hello there, my name is $name\")\n}\n\nintroduceYourself() // defaults to Bonnie \nintroduceYourself(\"NotBonnie\") // uses input we specify\n\nHello there, my name is Bonnie\nHello there, my name is NotBonnie\n\n\n\ndefined function introduceYourself"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#maps-paired-data",
    "href": "posts/2018-07-20-scala/index.html#maps-paired-data",
    "title": "A handshake with Scala",
    "section": "Maps (Paired Data)",
    "text": "Maps (Paired Data)\n\n// store keys and values \nvar stars = Map(\"Lee\" -&gt; \"Pace\", \"Kerry\" -&gt; \"Bishe'\", \"MaKenzie\" -&gt; \"Davis\")\n// or\n// Map[String, String] = Map(\"Lee\" -&gt; \"Pace\", \"Kerry\" -&gt; \"Bishe'\", \"MaKenzie\" -&gt; \"Davis\")\n\nprintln(stars(\"Lee\"))\n\nPace\n\n\n\nstars: Map[String, String] = Map(\n  \"Lee\" -&gt; \"Pace\",\n  \"Kerry\" -&gt; \"Bishe'\",\n  \"MaKenzie\" -&gt; \"Davis\"\n)\n\n\n\nIt is easy to add new entries:\n\n// the += syntax carries over\nstars += (\"Scoot\" -&gt; \"McNairy\")\nstars += (\"Toby\"  -&gt; \"Huss\")\n\n// using keys yields and array which also has \"foreach\"\nstars.keys.foreach{ actor =&gt;\n    println(actor + \" \" + stars(actor))\n}\n\nScoot McNairy\nKerry Bishe'\nMaKenzie Davis\nToby Huss\nLee Pace\n\n\nThere is also a “values” method:\n\nstars.values\n\n\nres81: Iterable[String] = Iterable(\"McNairy\", \"Bishe'\", \"Davis\", \"Huss\", \"Pace\")\n\n\n\nIt’s also simple to reverse a mapped/paired structure using mapping:\n\n// simply yield \"v\", \"k\", for every \"k\", \"v\"\nvar reversedStars = for((k, v) &lt;- stars) yield (v, k)\n\n\nreversedStars: Map[String, String] = HashMap(\n  \"Pace\" -&gt; \"Lee\",\n  \"McNairy\" -&gt; \"Scoot\",\n  \"Bishe'\" -&gt; \"Kerry\",\n  \"Davis\" -&gt; \"MaKenzie\",\n  \"Huss\" -&gt; \"Toby\"\n)"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#object-oriented-programming",
    "href": "posts/2018-07-20-scala/index.html#object-oriented-programming",
    "title": "A handshake with Scala",
    "section": "Object Oriented Programming",
    "text": "Object Oriented Programming\nWe’ve mostly seen simple imperative programming and hints of Scala’s functional origin but it also an Object Oriented language.\n\nCase Classes\nThe most simple version of OO Scala is the case class. While a bit foreign to Java folks, they are quite handy. Case classes are generally small, simple, immutable structures, like points on a graph. Let’s define one:\n\n// define a case class with an x and y value \ncase class Point(x:Int, y:Int)\n\n\ndefined class Point\n\n\n\nThat’s it. It’s disarmingly simple compared to a Java object, but it works. Part of this is because it assumes that you want immutable fields unless otherwise specified (part of the functional meets OO approach). Creating one is straightforward and largely familiar to Java folks:\n\n// make a new Point\nval myLocation = new Point(1,2)\n\n// access the members\nmyLocation.x\nmyLocation.y\n\n//// can't do this:\n// myLocation.x = 5\n//// unless it was defined like this, specifying they could vary\n// case class Point(var x:Int, var y:Int)\n\n\nmyLocation: Point = Point(1, 2)\nres84_1: Int = 1\nres84_2: Int = 2\n\n\n\n\n\nTraditional Classes\nWe will make a simple Person class that has a private value (available only to the class) to represent its age and a String for a name. We will include methods for it to get older and to display the current age. Classes are public by default.\n\n// define a class\nclass Person {\n    // set a field\n    private var age = 0\n    var name = \"\"\n    \n    // method to increase age\n    def getOlder { age += 1 }\n    \n    // method to say age\n    def sayAge { println(age) }\n}\n\n// make a new instance \nval rando = new Person\n\n// show age before and after method call \nrando.sayAge\nrando.getOlder\nrando.sayAge\n\n// give the person a name \nrando.name = \"Abakus\"\nprintln(rando.name)\n\n0\n1\nAbakus\n\n\n\ndefined class Person\nrando: Person = ammonite.$sess.cmd85$Helper$Person@6162ffa6\n\n\n\nWhat if we wanted to make mandatory fields? We can include them in the parenthesis in a move that will look familiar to Python coders.\n\n// define a person with a name and an age\nclass Person(val name: String, var age: Int)\n\n//// this would crash \"not enough arguments for constructor\":\n//&gt; val kanye = new Person(name = \"Kanye\")\n///...because it doesn't have the arguments that we told it were needed.\n\n// make a new instance of the class named kanye\nval kanye = new Person(name = \"Kanye\", age = 35)\n\n// show values \nkanye.name\nkanye.age\n\n\ndefined class Person\nkanye: Person = ammonite.$sess.cmd86$Helper$Person@360cbb22\nres86_2: String = \"Kanye\"\nres86_3: Int = 35\n\n\n\n\nConstructors\nConstructors allow the writer to dictate behavior that takes place whenever an instance of the class is created (for the uninitiated, the footnotes of this page contain a brief explanation). It’s common, for instance, for an object that reads a file to do so upon to construction. This saves the time of creating the object and then calling a method to have it read the file. In C++ and Java, the constructor is a method of the class that has the same name as the class. In Scala however, it just executes the code immediately inside the class definition. Let’s take a look at a simple example:\n\nclass TinyHuman {\n    // this is constructor behavior\n    println(\"I have been born and am alive!\")\n}\n\n// this will print it's greeting even thought we didn't call anything\nval newborn = new TinyHuman\n\nI have been born and am alive!\n\n\n\ndefined class TinyHuman\nnewborn: TinyHuman = ammonite.$sess.cmd87$Helper$TinyHuman@7e1bdf48\n\n\n\n\n\nGetters and Setters\nGetters and Setters are built into classes by Scala (for the unfamiliar, the footnotes of this page contain a brief explanation). While this might seem wild, the programmer can control them using “var”, “val”, and the “private” keyword to restrict how they are used, allowing full control of the class. The auto-generator methods are “foo” and “foo_” in the Java Virtual Machine code made by compiling Scala.\n\nclass Year {\n    var month = 1 // starts in Jan \n}\n\nval thisYear = new Year()\nthisYear.month            // this will call the internal \"foo\", in this case, \"month\"\nthisYear.month = 2        // this calls the \"foo_\", in this case \"month_\"\n\n\ndefined class Year\nthisYear: Year = ammonite.$sess.cmd88$Helper$Year@182a8837\nres88_2: Int = 1\n\n\n\nScala will also allow you to define “foo” and “foo_” yourself if you want to (or just foo).\nTo make Scala write only a getter method declare that value with “val”. Because vals cannot change, it will not bother with setter method.\nTo make Scala create a getter and a setter, declare an attribute with “var” - an attribute that can change will require a getter and a setter.\n\n\nPrivate Attributes\nPrivate attributes work in a Java-like way. Here, we use private to make an attribute that can be incremented by the user but not set to whatever they want as they can’t access it directly:\n\nclass Year {\n    private var month = 1\n    def incrementMonth { month += 1 }\n    def showCurrentMonth = println(month)\n}\n\nval now = new Year\n\nnow.incrementMonth\nnow.showCurrentMonth\n\n2\n\n\n\ndefined class Year\nnow: Year = ammonite.$sess.cmd89$Helper$Year@33d58403\n\n\n\n\n\nConstructors\nScala generates a standard constructor for all the classes you define.\nWe can do so using what Scala calls a “Primary Constructor”. This is like a constructor in other languages except that is is implicitly created as part of the class definition.\n\nclass Child {\n    println(\"I am alive!\")\n    println(\"I have a constructor and it is the best!\")\n}\n\n// the message will just occur\nval kiddie = new Child\n\nI am alive!\nI have a constructor and it is the best!\n\n\n\ndefined class Child\nkiddie: Child = ammonite.$sess.cmd90$Helper$Child@965084a\n\n\n\n\n\nInheritance\nScala provides a predictable means of single inheritance (Like Java, Scala does not implement multiple inheritance).\n\n// class and method definition in a single line?\nclass Kid { def talk(): Unit = { println(\"I say the damnest things\") }}\n\nclass PreTeen extends Kid{}\n\n// will automatically have the method from Kid even though it is a preteen\nval newKid = new PreTeen\n\nnewKid.talk\n\nI say the damnest things\n\n\n\ndefined class Kid\ndefined class PreTeen\nnewKid: PreTeen = ammonite.$sess.cmd91$Helper$PreTeen@a4d13c1\n\n\n\nJava coders: note that Scala constructors do not require the use of super(), though the languages does support it for access superclass methods in general and it is used the same way as in Java.\n\n\nComposition\nScala offers “mixins” for composing classes based on functionality.\n\n// define a lover that loves\ntrait lover {\n  def love(): Unit = {\n    println(\"I am a lover!\")\n  }\n}\n\n// define a fighter that fights \ntrait fighter {\n  def fight(): Unit = {\n    println(\"I am a fighter!\")\n  }\n}\n\n// mix them together to make a rouge \nclass rouge(name: String) extends lover with fighter {\n  // the rouge can call both of methods from the mixins\n  def enGarde(): Unit = {\n    love\n    fight\n    println(\"I am \" + name + \"!\")\n  }\n}\n\nval zoro = new rouge(\"Zoro\")            // make a new rouge\nzoro.enGarde                            // put foes on notice!\n\nI am a lover!\nI am a fighter!\nI am Zoro!\n\n\n\ndefined trait lover\ndefined trait fighter\ndefined class rouge\nzoro: rouge = ammonite.$sess.cmd92$Helper$rouge@22a12e9e\n\n\n\n\n\nOverriding a method\nScala supports the “override” keyword to specify a new method behavior. Let’s make a subspecies of Rouge with his own enGarde() protocol:\n\nclass revengeSeeker(name: String, warning: String) extends rouge(name) {\n  // override the definition of enGarge\n  override def enGarde(): Unit = {                                        \n    love \n    fight\n    println(\"I am \" + name + \" \" + warning)\n  }\n}\n\nval inigo = new revengeSeeker(name = \"Inigo Montoya\", warning = \"Prepare to die!\")\ninigo.enGarde\n\nI am a lover!\nI am a fighter!\nI am Inigo Montoya Prepare to die!\n\n\n\ndefined class revengeSeeker\ninigo: revengeSeeker = ammonite.$sess.cmd93$Helper$revengeSeeker@4b5258dc"
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#footnotes",
    "href": "posts/2018-07-20-scala/index.html#footnotes",
    "title": "A handshake with Scala",
    "section": "Footnotes",
    "text": "Footnotes\nWhat’s a Constructor?\nFor the unfamiliar, a constructor is a special method that activates when a class is created. They keep the user from having to specify behavior for each object created after it is initialized."
  },
  {
    "objectID": "posts/2018-07-20-scala/index.html#what-are-getters-and-setters",
    "href": "posts/2018-07-20-scala/index.html#what-are-getters-and-setters",
    "title": "A handshake with Scala",
    "section": "What are Getters and Setters?",
    "text": "What are Getters and Setters?\nGetters and Setters are worth a discussion in their own right, but for a working knowledge, they are methods to access or change object attributes. They are used when there is a reason to conceal the traits from the end-user. A good example of when to use a Getter would be in a class where a Person class with and “age” attribute that should not be able to get younger (as provided in “Scala for the Impatient”). It’s easy for the programmer of the class to write code to keep the age from running if it doesn’t fit the criteria designated by the Setter."
  },
  {
    "objectID": "posts/2018-03-12-find-and-replace/index.html",
    "href": "posts/2018-03-12-find-and-replace/index.html",
    "title": "Find and replace in Python",
    "section": "",
    "text": "Using a dict to perform find and replace tasks makes a lot of sense; they’re simple to implement and they allow us to easily store the target with our desired replacement in one spot. There are a few hidden traps to keep be aware of, but they’re easily avoided and we can be back on track reaping the benefits with just a little forethought."
  },
  {
    "objectID": "posts/2018-03-12-find-and-replace/index.html#how-to-use-a-python-dict-for-findreplace-functions",
    "href": "posts/2018-03-12-find-and-replace/index.html#how-to-use-a-python-dict-for-findreplace-functions",
    "title": "Find and replace in Python",
    "section": "How to use a Python dict for find/replace functions",
    "text": "How to use a Python dict for find/replace functions\nWe’re going to go through several examples of find-and-replace tasks of varying complexity. We’ll also find some potential hiccups and how to avoid them. We’re going to use a simple string that is a coherent message if the code works and is hard to read if it isn’t to see if our efforts are working."
  },
  {
    "objectID": "posts/2018-03-12-find-and-replace/index.html#simple-character-character-substitution",
    "href": "posts/2018-03-12-find-and-replace/index.html#simple-character-character-substitution",
    "title": "Find and replace in Python",
    "section": "Simple character-character substitution",
    "text": "Simple character-character substitution\nReplacing one character with another is pretty straightforward using a function. This function takes the string and the dict as arguments, iterates through them, and makes the replacements. You’ll notice it has an “if” statement. This is to make sure that the item exists in our dict or else we get a keyError. We can’t replace something will the right pair in the dict if the item isn’t in the dict, and, if you think about it, that’s what we’re trying to do if we iterate over every letter and look it up in the dict regardless of whether or not it’s there.\nNote: I’m going to comment every line on this one to make sure we’re on the same page as this function will serve as the basis of all the ones coming after it. After that, I’ll just highlight what is different about the function.\n\n# here is our target string, contaminated with \"X\"s\ns = \"HEREXISXAXSAMPLEXSTRING\"\n\n# dictionary pairs \"X\" with \"-\"\nd = {\"X\":\"-\"}\n\n# define a functions that takes a string and a dict\ndef find_replace(string, dictionary):\n    # is the item in the dict?\n    for item in string:\n        # iterate by keys\n        if item in dictionary.keys():\n            # look up and replace\n            string = string.replace(item, dictionary[item])\n    # return updated string\n    return string\n\n# call the funciton\nfind_replace(s,d)\n\n'HERE-IS-A-SAMPLE-STRING'\n\n\nWe can see that all the “X”s have been replaced by “-”, their paired value."
  },
  {
    "objectID": "posts/2018-03-12-find-and-replace/index.html#replacing-multi-character-values",
    "href": "posts/2018-03-12-find-and-replace/index.html#replacing-multi-character-values",
    "title": "Find and replace in Python",
    "section": "Replacing multi-character values",
    "text": "Replacing multi-character values\nThis follows the same basic pattern, but uses a simple regex function. We will get rid of “ABC”s instead of “X”s. We import the re library and modify our string and dict for the test.\n\nimport re\n\ns = \"HEREABCISABCAABCSAMPLEABCSTRING\"\n\nd = {\"ABC\":\"-\"}\n\ndef find_replace_multi(string, dictionary):\n    for item in dictionary.keys():\n        # sub item for item's paired value in string\n        string = re.sub(item, dictionary[item], string)\n    return string\n\nfind_replace_multi(s, d)\n\n'HERE-IS-A-SAMPLE-STRING'"
  },
  {
    "objectID": "posts/2018-03-12-find-and-replace/index.html#replacing-single-and-multi-character-patternsoh-wait-crap",
    "href": "posts/2018-03-12-find-and-replace/index.html#replacing-single-and-multi-character-patternsoh-wait-crap",
    "title": "Find and replace in Python",
    "section": "Replacing single and multi character patterns…oh wait, crap…",
    "text": "Replacing single and multi character patterns…oh wait, crap…\nThere is a final kink in this however. What would happen if one of the single character values we wanted to replace occurs within the multi-character string?\n\n# the middle \"ABC\" has been replaced with just an \"C\"\ns = \"HEREABCISABCACSAMPLEABCSTRING\"\n\n# ...which we see \nfind_replace_multi(s,d)\n\n'HERE-IS-ACSAMPLE-STRING'\n\n\nAlas! It seems we have a variation. We assumed all the problematic strings were “ABC”s, but one is just a “C”. No problem, right? We just update the dictionary to get rid of “C” as well. Not quite. Here is the thing that goes wrong with these types of functions: we don’t know what order the substitutions are happening in. So we might remove the “C” from “ABC” (leaving it as just “AB”) first thing, then when we look for “ABC”s to sub, there wont be any. Our program would end without doing anything, and we would still have the “AB”.\nFor another example, imagine we are replacing “Hello” with 1, and “He” with 2. Our string could be “He Said Hello”. If “He” gets changed first, we would have “2 Said 2llo”. When we go to look for hello? There is no hello."
  },
  {
    "objectID": "posts/2018-03-12-find-and-replace/index.html#so-what-do-we-do",
    "href": "posts/2018-03-12-find-and-replace/index.html#so-what-do-we-do",
    "title": "Find and replace in Python",
    "section": "So what do we do?",
    "text": "So what do we do?\nMy solution for this problem is to make sure we iterate through the dictionary keys from LARGEST by length to SMALLEST by length to ensure that we don’t replace any pieces of them by accident. Using the He/Hello example, we substitute the “Hello” first so none of the substrings (“He”) can interfere. We can do this by using the sorted() function and using the reverse = True options (it goes smallest to largest by default). Check it.\n\ns = \"HEREABCISABCACSAMPLEABCSTRING\"\n\nd ={\"C\":\"-\", \"ABC\":\"-\"}\n\ndef find_replace_multi_ordered(string, dictionary):\n    # sort keys by length, in reverse order\n    for item in sorted(dictionary.keys(), key = len, reverse = True):\n        string = re.sub(item, dictionary[item], string)\n    return string\n\nfind_replace_multi_ordered(s, d)\n\n'HERE-IS-A-SAMPLE-STRING'\n\n\nLooks as if we are on the right track. Because we replaced the biggest strings first (“ABC”), and the the smallest (“C”), we ensure the “C” in “ABC” didn’t get messed with. I’ll write one more test to be sure. To confirm that the largest is going first, and not simply that “ABC” is going before “C”, we’ll add another value to the dict (the rest of the code stays the same). I’ll use “CSAMPLEABC”, because the only way for that to be replaced is if it goes before “C” and “ABC” as BOTH of those strings are in it.\n\ns = \"HEREABCISABCACSAMPLEABCSTRING\"\n\nd ={\"C\":\"-\", \"ABC\":\"-\", \"CSAMPLEABC\":\"-:)-\"}\n\ndef find_replace_multi_ordered(string, dictionary):\n    # sort keys by length, in reverse order\n    for item in sorted(dictionary.keys(), key = len, reverse = True):\n        string = re.sub(item, dictionary[item], string)\n    return string\n\nfind_replace_multi_ordered(s, d)\n\n'HERE-IS-A-:)-STRING'\n\n\nAnd there we have it! Our final function. It’s worth noting that, while I made the initial character-character find-and-replace function to work us up to this one, we final function will work for simple character-character substitutions too, so we only need the one, final version for all the tasks here. I’d advise doing so because it has error avoidance with the iteration and the “if” statement."
  },
  {
    "objectID": "posts/2018-02-06-simple-date-conversion/index.html",
    "href": "posts/2018-02-06-simple-date-conversion/index.html",
    "title": "Simple date-string conversion",
    "section": "",
    "text": "Simple Date Strings Conversion: split-process-join¶\nSomeone recently asked me about converting dates in number form to dates their names to make approximate ranges that are easier to read. For example, “01/01/2017 to 04/01/2017” would be “January to February” (in the context of what I was being asked, the years were the columns in a dataframe, so I wasn’t concerned with the year.\nFirstly, when confronted with this type of challenge, it is best to do a little background reading. If the times are in a certain format, for instance, there might be packages that allow you to manipulate and convert them already written, like Python’s datetime module. For this post, we will just assume we have raw strings and have to deal with them.\nThat said, let’s see what we are dealing with.\n\ndate = \"01/01/2017 to 04/01/2017\"\n\nNote that the information is all in one string, as opposed to a list with the dates in their own entries.\nThe first thing that might come to mind is a regular expression. They’re a powerful tool, and they might be useful in a problem like this. I ended finding a way to it without them. In general, I try to avoid them for simple tasks because trying to add a regex to a program is like busting out your pepper spray in a mosh pit. Quick, easy, life saver? Spark that turns a dicey situation into a full-on incident? Well-intended plan that backfires causing hours of pain? You just don’t know.\nI used a “split-process-join” approach.\nIn order to simplify the process, I split the string on ” to ” so we can deal with each date on its own.\n\n# use the split function \nsplit_date = date.split(\" to \")\n\nprint(split_date)\n\n['01/01/2017', '04/01/2017']\n\n\nNow we an iterate over the list and use the .startswith() with method as this is a list of strings. I use the the range() function instead of “for x in y” because we want to refer to list elements by their number.\n\n# for each index in length of list...\nfor i in range(len(split_date)):\n    \n    # give it a name to for readability\n    entry = split_date[i]\n    \n    # if the entry there starts with \"01/\" -&gt; \"January\"\n    if entry.startswith(\"01/\"):\n        \n        # replace the item in the array with the month\n        split_date[i] = \"January\"\n        \n    # as above with \"February\"\n    if entry.startswith(\"04/\"):\n       \n        # put \"February\" there\n        split_date[i] = \"February\"\n        \n# see the output \nprint(split_date)\n\n['January', 'February']\n\n\nTo finish the process, we simply need to join the entries back on the ” to “. Because .split() and .join() and string methods in Python, the join is admittedly awkward looking.\n\n# on \" to \" join split_date\nfinal = \" to \".join(split_date)\n\n# show results \nprint(final)\n\nJanuary to February\n\n\nAnd we’re there:) All in once place, and from the top, that gives us:\n\ndate = \"01/01/2017 to 04/01/2017\"\n\nsplit_date = date.split(\" to \")\n\nfor i in range(len(split_date)):\n    entry = split_date[i]\n    if entry.startswith(\"01/\"):\n        split_date[i] = \"January\"\n    if entry.startswith(\"04/\"):\n        split_date[i] = \"February\"\n\nfinal = \" to \".join(split_date)\n\nprint(final)\n\nJanuary to February\n\n\nObviously, this is just a proof of concept that only works for two months, but it is easy to see how it could made into a function. What would your next step be? I’d go with a function using a dict.\n\ndef number_to_name(date):\n    # create a dic that stores the pairs\n    d = {\"01/\":\"January\",  \"02/\":\"February\", \"03/\":\"March\", \n         \"04/\":\"April\"  ,  \"05/\":\"May\",      \"06/\":\"June\",\n         \"07/\":\"July\",     \"08/\":\"August\",   \"09/\":\"September\",\n         \"10/\":\"October\",  \"11/\":\"November\", \"12/\":\"December\" }\n    \n    split_date = date.split(\" to \")\n    for i in range(len(split_date)):\n        entry = split_date[i]\n        # the first 3 chars denote the month\n        month = entry[:3]\n        # sub them from dict\n        split_date[i] = d[month]\n    # return after joining \n    return \" to \".join(split_date)               # ugly, but saves a line\n                                         \nprint(number_to_name(\"01/01/2017 to 04/01/2017\"))\n\nJanuary to April\n\n\nIt might make sense to define the dict outside the function so it doesn’t recreate it every time in practice. Feel free to take this and do whatever to it, and I am open to hearing how people would improve it."
  },
  {
    "objectID": "posts/2018-01-09-twitter-metadata-classifier/index.html",
    "href": "posts/2018-01-09-twitter-metadata-classifier/index.html",
    "title": "Trump v. Clinton: Twitter metadata classifier",
    "section": "",
    "text": "Hello all!\nI did a project for a data science class in which I classified tweets as being from @realDonaldTrump vs @HillaryClinton. I found a dataset on this on Kaggle. Because words and context trip up even the most clever programs on occasion, I decided to see if I could write and entirely numerical classifier. It read length of words, average words per sentence, that sort of thing. I wrote a Python function that engineers these features and a script to implement it that adds them to the original dataset. Below is the video:\n\n\nPlease feel free to get in touch if you’d like to see a copy of the PDF."
  },
  {
    "objectID": "posts/2017-12-20-zip-and-dict/index.html",
    "href": "posts/2017-12-20-zip-and-dict/index.html",
    "title": "zip and dict",
    "section": "",
    "text": "The peanut-butter and jelly of paired data: zip() and dict()\nPaired data in Python is user-friendly and efficient, and I find myself using it all the time. Sometimes in the course of a workflow, we end up with with multiple simple lists of information that would be easier managed as paired data. If you find yourself in this situation, there are two simple tools that work in a really intuitive way to “zip” the lists together such that the first item of list A is paired with the first item of list B. If the lists were [“Bruce”, “Peter”] and [“Wayne”, “Paker”] the out put would match them to {“Bruce”:”Wayne”, “Peter”:”Parker”}. *note that Python will also accept {“Miles”:”Morales”} 🙂\nLet’s say we have two lists of names and want to store them as paired data in a dictionary. All we need to do to get a nice, tidy, dictionary out of it is the following (I’ll show the running code and then go through it afterwards with a more realistic use case).\n\n# here are our two lists\nfirst = [\"A\",\"B\",\"C\"]\nsecond = [\"1\",\"2\",\"3\"]\n\n# make a dict from the output of zipping the two iterables\nthird = dict(zip(first, second))\n\n# iterate thorough and show we got what we were looking for\nfor item in third:\n    print(item, third[item])\n\nA 1\nB 2\nC 3\n\n\nThat’s it. What’s going on here? Let’s take a closer look.\nPython has a “zip” function that will return a zip object of tuples (don’t worry if that sounds weird; it just means it is a zip class and has our data stored in a way such that it can’t be modified). It takes iteratable objects like lists as arguments, in our case, just two. It goes through the lists in order, and “zips” them together, first item to first item, second to second, and so on.\n\n# here are our to lists\nfirst = [\"Christopher\",\"Jo-Vaughn\",\"Sean\",\"Sir Robert Bryson\"]\nlast = [\"Wallace\",\"Scott\",\"Carter\",\"Hall II\"]\n\n# call the zip function that takes iteratable structure \nnames = zip(first, last)\n\n# if we print it out, we see we have a zip object and memory address\nprint(names)\nprint(type(names))\n\n&lt;zip object at 0x7f3039c46000&gt;\n&lt;class 'zip'&gt;\n\n\nWhile they are in the object, we can’t use them as paired data directly. Fortunately, the dict() function will take a list of tuples and return a dictionary.\n\n# use the dict function on the zip object\nnames = dict(names)\n\n# print out the evidence\nfor item in names:\n    print(item, names[item])\n\nChristopher Wallace\nJo-Vaughn Scott\nSean Carter\nSir Robert Bryson Hall II\n\n\nNote that this will also work with ints and floats. That’s all there is to it. Thanks for reading."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "[ CV ]",
    "section": "",
    "text": "For computational and data science projects, see my portfolio."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "[ What I Work On ]",
    "section": "",
    "text": "I’m interested in statistical methods to reduce disparities and inefficiencies in healthcare. The current form this takes is biostatistical and reinforcement learning approaches to understanding and improving health outcomes in opioid use disorder.\nBefore starting my PhD, this took the form of trying to identify biomarkers for Alzheimer’s Dementia, Heart Failure, and COVID-19. The overarching goal was to find reliable markers of disease or to compare existing methods to determine which diagnostic approach was optimal in terms of expense or invasivness."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "[ Drops of Jupyter ]",
    "section": "",
    "text": "Tidbits on statistics, programming, science, and their collision with the world.\n\n \n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMultiple testing correction for family-wise error-rate and false discovery rate\n\n\n\n\n\nRe-implementing for knowledge and profit(?).\n\n\n\n\n\n\nDec 17, 2022\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nStats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling\n\n\n\n\n\nTransferring the basics of my R modeling knowledge back to my first language.\n\n\n\n\n\n\nDec 13, 2022\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nStats Python in a Hurry Part 2: Visualization\n\n\n\n\n\nTransferring my R data viz knowledge back to my first language.\n\n\n\n\n\n\nDec 12, 2022\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nStats Python in a Hurry Part 1: Data Wrangling\n\n\n\n\n\nTransferring my R data-munging knowledge back to my first language.\n\n\n\n\n\n\nDec 11, 2022\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nSummarizing a review of contributors to the opioid epidemic\n\n\n\n\n\n\n\nOpioids\n\n\nResearch\n\n\n\n\nA concise summary of Cerdá, et al with some additional fundamentals.\n\n\n\n\n\n\nDec 10, 2022\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nA Survival Guide for Students Starting an MS in Bioinformatics\n\n\n\n\n\n\n\nAcademia\n\n\nAdvice\n\n\n\n\nSome lessons learned in grad school.\n\n\n\n\n\n\nSep 29, 2021\n\n\nThadyan\n\n\n\n\n\n\n  \n\n\n\n\nRygor: a Retraction Watch database companion\n\n\n\n\n\n\n\nTools\n\n\n\n\nA command line tool to check a list of citations against The Retraction Watch Database\n\n\n\n\n\n\nJan 19, 2021\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nIn Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited\n\n\n\n\n\n\n\nSociopolitical\n\n\n\n\nSome more wordclouds to celebrate the day.\n\n\n\n\n\n\nJan 18, 2021\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nAn interactive Bayseian app for interpretation of SARS-CoV-2 antibody tests\n\n\n\n\n\n\n\nTools\n\n\nPublications\n\n\nStatistics\n\n\nJavaScript\n\n\n\n\nA webapp to complement our recent paper.\n\n\n\n\n\n\nJan 8, 2021\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nFact Check: The Quote Heather Mac Donald presents when discussing Johnson et al comes from a passage of a pre-print that does not appear in the published work.\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nMore bad form.\n\n\n\n\n\n\nJul 11, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nEvidence Heather Mac Donald Presented A Pre-print Claim That Was Remove In Peer Review As Scientific\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nMore details on a previously observed biff from HMD.\n\n\n\n\n\n\nJul 10, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nLetter - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nApplying some more skepticism to a recent WSJ Op-ed, because it deserves it.\n\n\n\n\n\n\nJul 9, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nVideo - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nApplying some skepticism to a recent WSJ Op-ed\n\n\n\n\n\n\nJun 27, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nA Case study in Silent Data Corruption in an RNA-Seq Experiment\n\n\n\n\n\n\n\nR\n\n\nBioinformatics\n\n\nStatistics\n\n\nDifferential Expression\n\n\nTools\n\n\n\n\nHow a subtle bug and misleading error message can transform your RNA-Seq data.\n\n\n\n\n\n\nApr 27, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nHow to install Arch Linux on a VirtualBox VM\n\n\n\n\n\n\n\nLinux\n\n\n\n\nTesting Arch before you wreck your computer installing it.\n\n\n\n\n\n\nMar 30, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nIn Honor of MLK 2020: ‘Letter from Birmingham Jail’ Wordcloud Example in R\n\n\n\n\n\n\n\nSociopolitical\n\n\n\n\nMarking the occasion with word clouds.\n\n\n\n\n\n\nJan 20, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nSeasonality in gun deaths\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\n\n\nPracticing data analysis on some CDC gun data.\n\n\n\n\n\n\nAug 30, 2019\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nClosures in Python\n\n\n\n\n\n\n\nPython\n\n\nFunctional Programming\n\n\n\n\nMakeshift type checking in Python and when I find it handy.\n\n\n\n\n\n\nMar 28, 2019\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nA more rigorous ASH\n\n\n\n\n\n\n\nAntigens\n\n\nPython\n\n\n\n\nBuiling on the ASH tool from a previous post.\n\n\n\n\n\n\nJan 31, 2019\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nPerl for the impatient\n\n\n\n\n\n\n\nPerl\n\n\n\n\nA dangerously fast introduction to a dangerously concise language.\n\n\n\n\n\n\nOct 4, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nPython imports: Sample Project + Explanations\n\n\n\n\n\n\n\nPython\n\n\n\n\nAnswering a question on using imports in Python pograms\n\n\n\n\n\n\nSep 30, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nObject-oriented Python: an overview\n\n\n\n\n\n\n\nPython\n\n\nObject-Oriented Programming\n\n\n\n\nAn overview of object-oriented programming in Python.\n\n\n\n\n\n\nAug 30, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nA handshake with Scala\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nSome light-hearted oop\n\n\n\n\n\n\n\nPython\n\n\nObject-Oriented Programming\n\n\n\n\n…or: circumstantial evidence that I am still alive.\n\n\n\n\n\n\nMay 10, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nFind and replace in Python\n\n\n\n\n\n\n\nPython\n\n\nData Processing\n\n\n\n\nLike ctrl-f with for Python.\n\n\n\n\n\n\nMar 12, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nASH: Antigen Selection Heuristic\n\n\n\n\n\n\n\nAntigens\n\n\nPython\n\n\n\n\nA prototype to select antigenic peptides.\n\n\n\n\n\n\nFeb 20, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nSimple date-string conversion\n\n\n\n\n\n\n\nPython\n\n\nData Processing\n\n\n\n\nAnswering a question an dates and strings.\n\n\n\n\n\n\nFeb 6, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nMatrin Luther King Day\n\n\n\n\n\n\n\nSociopolitical\n\n\nR\n\n\nNLP\n\n\n\n\nMarking the occasion with word clouds.\n\n\n\n\n\n\nJan 16, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nTrump v. Clinton: Twitter metadata classifier\n\n\n\n\n\nClassifiying tweets as Hilary or Trump.\n\n\n\n\n\n\nJan 9, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nPython for Perl users\n\n\n\n\n\n\n\nPython\n\n\nPerl\n\n\n\n\nAll the kids are using Python now.\n\n\n\n\n\n\nDec 21, 2017\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nzip and dict\n\n\n\n\n\n\n\nPerl\n\n\nPython\n\n\n\n\nThe peanutbetter and jelly of paired data.\n\n\n\n\n\n\nDec 20, 2017\n\n\nThadryan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thadryan J. Sweeney, MS",
    "section": "",
    "text": "I’m a Firefighter and Human Services Professional turned Data Scientist interested in statistical approaches to understanding and optimizing biomedical systems large and small. My goal is to conduct that leads to more efficient use of resources, more personalized care, and reduces friction and suffering. You might call it  Biomedical Moneyball .\nI’m currently pursing a PhD in Quantitative Biomedical Sciences, where I am advised by  Wesley Marrero  of the  Operations Research Group. My research focuses on finding optimal policy-level treatment strategies for opioid use disorder treatment using reinforcement learning as well as traditional biostatistics. I’m also working on more general epidemiological projects related to disparities in access to treatment for opioid use disorder.\nBefore starting my doctorate, I worked at Mass General Hospital evaluating biomarkers in the  Alzheimer’s Clinical & Translational Research Unit. and the  Cardiovascular Research Center.\nI also taught an introductory class in the Bioinformatics MS  program at Northeastern University.\nWhen I’m not doing science-y stuff, I play RPGs or make cartoons.\nOf the links below, Mastodon is my preferred platform."
  },
  {
    "objectID": "posts/2017-12-21-python-for-perl-users/index.html",
    "href": "posts/2017-12-21-python-for-perl-users/index.html",
    "title": "Python for Perl users",
    "section": "",
    "text": "Recently, I created a quick summary of Python for Perl users for a class, so I thought I would share it here. Perl is a bit of a throwback, so for the uninitiated, it’s is a high level level language developed to parse text in unix-like systems.  Quirky and known for being quick to write, if awkward to read, its regular expression and text parsing capability set the standard for languages that followed. It grew in popularity and was implemented for general purpose computing and web use, becoming known as “the duct tape that holds the Internet together”. Though its uses has decreased sharply in the era of Python, JavaScript, and Ruby, it played an instrumental role in the Human Genome Project, so it still enjoys use in Bioinformatics.\nThis repository has a Jupyter notebook of Python code and explanations, and a file of Perl scripts for contrasting some of the strategies. Enjoy!"
  },
  {
    "objectID": "posts/2018-01-16-mlk/index.html",
    "href": "posts/2018-01-16-mlk/index.html",
    "title": "Matrin Luther King Day",
    "section": "",
    "text": "Hello Everyone!\nI would like to wish everyone a happy MLK day. It isn’t much, but as an act of respect and commemoration, I though I would share an example of how a beautiful speech can, with relative ease, be turned into a striking visual. We will, of course, be working with the monolithic “I Have a Dream” speech.\nI used the R programming language for this, as I have worked more extensively with text with it than Python, and don’t use it as often and want to keep up my skills. If you’d like to see the rundown, here is the workbook, if not, the final product is below!\nEnjoy, and once again, happy Martin Luther King day!\n\n\n\nwordcloud\n\n\nNote: This post was published on Jan 15 at approximately 9pm. The error on the date was due to a time settings curiosity I had not seen before."
  },
  {
    "objectID": "posts/2018-02-20-ash/index.html",
    "href": "posts/2018-02-20-ash/index.html",
    "title": "ASH: Antigen Selection Heuristic",
    "section": "",
    "text": "Hello!\nI have been interested in antigens/epitopes for some time now, and this is my prototype of a tool to help and informed user take a closer look at targets of interest for projects that involve them. It works using a simple scale that compares kmers on a residue to residue basis, and scores them for distinctness based on the presence of hydrophilic or hydrophobic residues and residues of structural complexity, which are widely known factors in immonogencity. Still, in the name of diligence, sources that informed these decisions can be seen below at the bottom of the post. The github to the project is here . There is a walkthrough, a readme, and a test package if you want to give it a spin. I’m actively working on this and thus very open to constructive feedback and am happy to answer questions!\nhttp://www.abcam.com/protocols/antigens\nhttp://www.abcam.com/protocols/tips-for-designing-a-good-peptide-immunogen\nhttps://www.genscript.com/peptide_design_guideline.html\n\n\nAntigen Design: Considering the Target"
  },
  {
    "objectID": "posts/2018-05-10-oop-lighthearted/index.html",
    "href": "posts/2018-05-10-oop-lighthearted/index.html",
    "title": "Some light-hearted oop",
    "section": "",
    "text": "Hello there! I am writing to let you know that I have cleared the summit of a couple of beastly projects and will be back to working on the site at at the usual semi-regular pace soon. I am also working on some pieces that are a little larger and more detailed than usual, but the delays should (hopefully) be offset with quality. In the meantime, enjoy some easy-going code humor that doesn’t want to hurt no one as evidence I am still around.\n\n### \"trait\" classes ###\nclass Picker(object):\n    \n    def pick(self):\n        print(\"I'm a picker...\")\n    \nclass Grinner(object):\n    \n    def grin(self):\n        print(\"I'm a grinner...\")\n    \nclass Lover(object):\n    \n    def love(self):\n        print(\"I'm a lover...\")\n    \nclass Sinner(object):\n    \n    def sin(self):\n        print(\"I'm a sinner.\")\n    \nclass Joker(object):\n    \n    def joke(self):\n        print(\"I'm a joker...\")\n    \nclass Smoker(object):\n    \n    def smoke(self):\n        print(\"I'm a smoker...\")\n        \nclass MidnightToker(object):\n    \n    def midnight_toke(self):\n        print(\"I'm a midnight toker.\")\n\n\n\n### composed class ### \nclass SteveMillerObj(Picker, Grinner, Lover, \n                     Sinner, Joker, Smoker, MidnightToker):\n    \n    def thoughts_on_hurting_someone(self):\n        print(\"I don't want to hurt no one.\")\n        \n    def how_loving_is_obtained(self):\n        print(\"I get my loving on the run.\")\n\n\n\n### usage ##\nSteve_Miller = SteveMillerObj()\n\n# use traits\nSteve_Miller.pick()\nSteve_Miller.grin()\nSteve_Miller.love()\nSteve_Miller.sin()\n\n# explains ethos part 1\nSteve_Miller.thoughts_on_hurting_someone()\n\n# use traits \nSteve_Miller.joke()\nSteve_Miller.smoke()\nSteve_Miller.midnight_toke()\n\n# explain ethos part 2\nSteve_Miller.how_loving_is_obtained()\n\nI'm a picker...\nI'm a grinner...\nI'm a lover...\nI'm a sinner.\nI don't want to hurt no one.\nI'm a joker...\nI'm a smoker...\nI'm a midnight toker.\nI get my loving on the run."
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html",
    "href": "posts/2018-08-30-oop/index.html",
    "title": "Object-oriented Python: an overview",
    "section": "",
    "text": "Object-Oriented Programming is a huge topic, and a unit dedicated to it has been in the queue for PyWy for some time. In the mean time, here is a crash course to give you an idea of what to expect!"
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html#object-oriented-programming-oop",
    "href": "posts/2018-08-30-oop/index.html#object-oriented-programming-oop",
    "title": "Object-oriented Python: an overview",
    "section": "Object Oriented Programming (OOP)",
    "text": "Object Oriented Programming (OOP)\nOOP is often considered a little weird to learn, but becomes very intuitive with some practice because it is used to model real things. It’s used to imitate things from the real world and store data in the same way they do.\nA “class” is a model or blueprint for how to store that information. You might have a class to imitate “Students”. Each INDIVIDUAL occurrence of a class is called and “object”. If there was a class called “Student”, you and I were students, we would be individual objects of the type “Student”. In Python code this looks like this:\n\n# class \"Student\" is an object\nclass Student(object):\n    # def is Python's version of sub - we're making a function acting on self                 \n    def say_what_you_are(self):          \n        # print out a message \n        print(\"I am a student\")"
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html#initialization",
    "href": "posts/2018-08-30-oop/index.html#initialization",
    "title": "Object-oriented Python: an overview",
    "section": "Initialization",
    "text": "Initialization\nThis is simply a way to say “creation”. We’ve described to the computer how to make a Student, but keep in mind we didn’t actually tell it to do so. We just told it HOW to do so. It’s the blueprint NOT a building.\nNow let’s make an individual object:\n\nJiaojiao = Student() # Python doesn't require you to say new\n\nThat’s it. Notice that there is no output. She’s just sitting there. That’s ok. We told her to be born as a student, we didn’t tell her to do anything. Unless otherwise specified, an object will just sit there, existing. Let’s make her do something:\n\nJiaojiao.say_what_you_are()    # the \".\" notation is used to \"call\" things out of or from an object. \n\nI am a student\n\n\nThat’s all this student class can do. Not very interesting. We will write over thr first definition with a better one. Let’s add an attribute:\n\nclass Student(object):\n    # we've added an attribute \"name\", that starts blank\n    name = \"\" \n    def say_what_you_are(self):\n        print(\"I am a student\")\n\nNow “Student”s have a name, set to nothing by default. Let’s see how we would use that:\n\n# create Ashmi \nAshmi = Student()         \n\n# Will print an empty string. Not helpful!\nprint(Ashmi.name)         \n\n# give her a name, because it starts as nothing. No (), because its not a function/method\nAshmi.name = \"Ashmmi\"    \n\n# we can print her name now\nprint(Ashmi.name)         \n\n\nAshmmi\n\n\nLet’s make the class more usefull by adding an introduce itself. This requires us to understand “self” more. “self” is to explicity tell the object what to do and let it know it’s talking about itself. It’s as if you pass it to itself so it can do things to itself, like how Perl has “$self = shift”. It’s “self” aware, in that it can get to it’s attibutes. This is a really powerful idea becaues it allows us to direct lots of objects with making each one.\n\nclass Student(object):\n\n    name = \"\"\n    \n    # let it know who it will be talking about.\n    def say_what_you_are(self):    \n        print(\"I am a student\")\n\n    # acces the name of self\n    def introduce_yourself(self):         \n        print(\"Hello, I am\", self.name)   \n\nHere is how we can use this and the objects will say their specific name.\n\nNareh = Student()\nNareh.name = \"Nareh\"\n\nAshmi = Student()\nAshmi.name = \"Ashmi\"\n\n# they will give their names with the same method call\nNareh.introduce_yourself()     \nAshmi.introduce_yourself()\n\nHello, I am Nareh\nHello, I am Ashmi"
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html#contructorinitializerbuild",
    "href": "posts/2018-08-30-oop/index.html#contructorinitializerbuild",
    "title": "Object-oriented Python: an overview",
    "section": "Contructor/Initializer/BUILD",
    "text": "Contructor/Initializer/BUILD\nYou make have noticed that it’s annoying to specify the name after the object is created. We can make this automatic with a “contructor” or “initializer”, a special function that triggers automatically whenever an object is initialized. in Perl/Moose this is called BUILD. In Python it’s called “init” to differentiate it form other functions. We will add one. It will take an argument, like any other function might and use it to set the attributes of the class.\n\nclass Student(object):\n    \n    # name is nothing, but will be set by __init__ (BUILD)\n    name = \"\"                   # &lt;-------\n    \n    # define __init__ that acts on \"self\", and takes a \"name\"\n    def __init__(self, name):   \n        self.name = name        \n           \n    # set your name attribute (above) to the name that is given to you \n    def say_what_you_are(self):\n        print(\"I am a student\")\n        \n    def introduce_yourself(self):\n        print(\"Hello, I am\", self.name) \n\nNow we can have behavior for the object from the moment it is created. This is super powerfull because we can give them some instructions and they will get along without us.\n\n# now we can just give the name from the start and don't have to mess with the object\nNareh = Student(\"Nareh\")    \nAshmi = Student(\"Ashmi\")  \n\nNareh.introduce_yourself()\nAshmi.introduce_yourself()\n\nHello, I am Nareh\nHello, I am Ashmi"
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html#inheritance",
    "href": "posts/2018-08-30-oop/index.html#inheritance",
    "title": "Object-oriented Python: an overview",
    "section": "Inheritance",
    "text": "Inheritance\nSometimes it is usefull to have sub catagories that a variations on the same type. This is a fancy word that, fortunately, means the same thing in OPP as it does in te regular world. An new class of objects can “inherit” attribues from an ancestor called a “base class”. Let’s see if there would a way to use this for Student. There are different types of students that share similar traits. We will start with something simple.\n\nclass Student(object):\n    def go_on_coop(self):\n        print(\"I will find a coop!\")\n\nJohn = Student()\nJohn.go_on_coop()\n\nI will find a coop!\n\n\nNothing new here yet. But now we’ll make a subclass of Student calles a MastersStudent that does more specific things than a general Student. It will automatically get things that a student has because we will pass the “Student” class to its definition not just any generic object.\n\n# notice we pass in Student not object!!!!!!!\nclass GradStudent(Student):               \n    # something more specific \n    def complain_about_undergrads(self):\n        print(\"Stupid undergrads!\")\n\nLet’s make an MS student:\n\n# make a MastersStudent\nSara = GradStudent()          \n# we know she can complain about undergrads because we coded that above\nSara.complain_about_undergrads() \n\nStupid undergrads!\n\n\nWe’re not suprised when we see she can use complain_about_undergrads() because we specifically told her how. But guess what else Sara can do:\n\n# she \"inherited\" this from the generic Student class\nSara.go_on_coop()     \n\nI will find a coop!\n\n\nSarah can use the methods from both classes becaues she is from MastersStudent that inherits things from regular Student. If we had written “class MastersStudent(object):”, it would still make a class, but not one that had access to things that Student does."
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html#multiple-inheritance",
    "href": "posts/2018-08-30-oop/index.html#multiple-inheritance",
    "title": "Object-oriented Python: an overview",
    "section": "Multiple Inheritance",
    "text": "Multiple Inheritance\nWe don’t have to stop there. Let’s get a level more specific.\n\n# a PhD Student is a type of MastersStudnet, not just any Student\nclass PhdStudent(GradStudent):\n    # they write dissertations (theortically)\n    def write_dissertation(self):     \n        print(\"Write, write, write!\")\n\nPhD Student gets “complain_about_undergrads()” from “GradStudent”, but it also gets “go_on_coop()” from GradStudent because GradStudnet gets it from Student!\n\n# make a PhdStudent\nChuck = PhdStudent()                  \n# look a all the shit I can do!\nChuck.go_on_coop()\n# even though you didn't have to tell me in my class\nChuck.complain_about_undergrads()    \nChuck.write_dissertation()\n\nI will find a coop!\nStupid undergrads!\nWrite, write, write!\n\n\nIf we really wanted to go nuts, we could add another level:\n\nclass PostDoc(PhdStudent):\n    def moar_phd_type_stuff(self):\n        print(\"What the hell is wrong with me?\")\n        \nMurillo = PostDoc()\n# can do ALL OF IT!!!\nMurillo.go_on_coop()\nMurillo.complain_about_undergrads()\nMurillo.write_dissertation()\nMurillo.moar_phd_type_stuff()\n\nI will find a coop!\nStupid undergrads!\nWrite, write, write!\nWhat the hell is wrong with me?\n\n\nThis saves us a lot of repetitive writing. But what if we wanted the tree to fork?\n\n### a class of masters student based on student \nclass MastersStudent(Student):\n    # MastersStudent - they're still based on Student\n    def panic_about_life(self):    \n        print(\"I should have just done a Phd\")\n\n\n### a class of PhD student based on student \n# PhdStudent - they're still based on Student\nclass PhdStudent(Student):            \n    def panic_about_life(self):\n        print(\"I should have stopped at my Master's\")\n\nNow these different types of subtypes will both share all the things that Student has, but the will have slightly different behavior when it is time to panic(). Observe:\n\n# make a MastersStudent\none_version_of_somone = MastersStudent()    \n# call the methods \none_version_of_somone.go_on_coop()         \none_version_of_somone.panic_about_life()\n\nI will find a coop!\nI should have just done a Phd\n\n\nSame thing but with a PhD:\n\n# make a PhdStudent\nother_version_of_somone = PhdStudent()      \n # will be the same\nother_version_of_somone.go_on_coop()\n# will be DIFFERENT!\nother_version_of_somone.panic_about_life()  \n\nI will find a coop!\nI should have stopped at my Master's\n\n\nNotice how they both can go on co-op and it is the same, but when they use panic because we redefined it.\nHere is another good example of “Polymorphic” behavior:\nhttps://stackoverflow.com/questions/3724110/practical-example-of-polymorphism"
  },
  {
    "objectID": "posts/2018-08-30-oop/index.html#compositiontraits",
    "href": "posts/2018-08-30-oop/index.html#compositiontraits",
    "title": "Object-oriented Python: an overview",
    "section": "Composition/Traits",
    "text": "Composition/Traits\nSometimes we want to mix and match traits without inheriting all of them. It make sense in one context for a class called “BiologicalLifeForm” pass on traits like “Eat”, “Breathe”, and “Reproduce”. But if you had an Animals class and wanted to make “Birds” and “Dogs”, it wouldn’t make sense to have dogs that had the “Fly” attribute.\nTo keep it in our student example, suppose we didn’t want our PhD students to complain about undergrads anymore because they have to give lectures with them, but still let our MS students do it. We could make a “complainer” trait and give it to the MS students but not the PhDs. Perl calls these “roles”. Most OOP systems call them “traits”. Python doesn’t have traits per se, you often just make another small class and “mix” it together with other ones to get what you want. For our purposes, We can make each “trait” into it’s own class and pick only what we want.\n\n### here is a \"trait\"\nclass GradStudent(object):\n    # a class of a grad student that will be the GradStudent \"trait\"\n    def do_things(self):\n        print(\"I study!\")\n        \n### here is the other \nclass Complainer(object):         \n    # a class of a complainer that will be the complainer \"trait\"\n    def complain_about_undergrads(self):\n        print(\"Seriously, they are the WORST\")\n\nLet’s mix and match! We’ll make a class the has one trait, and another that has both.\n\n### a class \"composed\" of one trait\nclass PhdStudent(GradStudent):  \n    # PhdStudents are GradStudents - we add \"mix in\" one trait to make the class\n    def say_hi(self):    \n        print(\"I am a PhD student. I can't kvetch about undergrads. That makes no sense.\")\n        \n### a class \"composed\" of two traits - GradStudent and Complainer  \nclass MastersStudent(GradStudent, Complainer): \n    # Masters Students are GradStudents AND Complainers\n    # # we wix in two traits to make the class\n    def say_hi(self):\n        \n        print(\"I am a MS student, I CAN kvetch about undergrads. Watch:\")\n\n\n# make a MastersStudent\nus = MastersStudent()            \n# use their traits \nus.say_hi()                      \nus.complain_about_undergrads()\n\nI am a MS student, I CAN kvetch about undergrads. Watch:\nSeriously, they are the WORST\n\n\n\n# make a PhdStudent\nthem = PhdStudent()               \n# will work\nthem.say_hi()\n# will not if uncommented - didn't get composed with complain_about_undergrads()\n# them.complain_about_undergrads() \n\nI am a PhD student. I can't kvetch about undergrads. That makes no sense.\n\n\nIt’s a little abstract in theory, but very useful in practice. It’s where design comes in - it’s not always clear which is best, or both might be just fine. Gotta tinker. It’s just odd because it requires to look at coding in abstraction not just technique. For more examples, imagine you were modeling the characters in a story. I might make three traits:\nLover\nFighter\nAsshole\nThe Hero of the story could be a Lover + Fighter. The Villian of the story would be a Fighter + Asshole. That way you could keep a trait from going where you don’t want it. A less abstract example would be in something like our final. You could make traits like:\nReader\nWriter\nGenerator\nDisplayer\nObjects that read in sequence data and made new subseq objects might have the traits “Reader” and “Generator”, and have the _getGenbankSeqs method. The ones that wrote new fasta files might be “Writer” and have writeFasta. You could add “Displayer” to whatever one you wanted to see terminal output from (it would probably have printResults)."
  },
  {
    "objectID": "posts/2018-10-04-perl-for-the-impatient/index.html",
    "href": "posts/2018-10-04-perl-for-the-impatient/index.html",
    "title": "Perl for the impatient",
    "section": "",
    "text": "Maybe you want something a little more purpose-built than Bash to mine some text and need a refresher. Perhaps you’re in Bioinformatics and want to extract some patterns from DNA (the original weird programming language). Maybe the eccentric, neck-bearded, regex-wizard in your office just retired and you’ve inherited a couple of scripts. Whatever the reason, you need to get (re)acquainted with Perl. I’ve got you covered with a slew of in-context examples of working code.\nPerl was originally developed in 1987 (just like yours, truly) by Larry Wall (unlike yours, truly). Before the language took off, he originally set out to automate the processing of text reports on Unix systems. Perl accordingly boasts a lot of support for text processing. Perl espouses the “There’s more than one way to do it” philosophy, often referred to as TMTOWTDI, (pronounced “Tim Toady”), which favors flexibility over standardization (contrast with Python’s “There should be one, obvious way to do it”). The benefits of this approach are intuitive: providing many ways to do things increases the chances that one will “click” for the user, and that the approach used with be just right for the specific task at hand (The drawbacks manifest in difficulty standardizing approaches). This perspective, along with a thriving on-line community & code repository  CPAN (which still exists), brought Perl a great deal of popularity. It simultaneously earned it a reputation as a “Swiss-army chainsaw” or, given its usage in the early days of the web (Perl predates Rails, and even Ruby itself), “the duct tape that holds the internet together”.\nPerl also established itself as an influential language in the trend toward high-level programing languages as the cost of programmers started to become more of a burden than the cost of machines. It’s cheaper to pay someone for a few hours of Perl than a few days of C if high-performance execution isn’t a factor, and machines speeding up pushed this calculus in Perl’s favor more and more often. When the Human Genome Project launched in 1990, formalizing the collision of Biology and Computer Science, Perl was the high level language of choice, with Python in its infancy and the whimsical Ruby not yet born. Perl excelled especially in searching for patterns in text, (how DNA is generally represented) helping the match stick. Continuing with the trend of ease-of-use, it’s also easier to convince a skeptical Biologist to put a “$” in front of text and numbers than to, say, use pointers.\nPerl’s flexibility will allow you to make yourself a mess (it is, after all, a feature, not a bug according to it’s creator), but a competent programmer can make clean, robust systems with it (almost of as fast as a novice could make ugly, fragile ones in a cleaner language). You’ll also have it at your disposal out-of-the-box on the majority us Unix based systems (including many Macs).\nIf you’re curious, you can check out the repository here."
  },
  {
    "objectID": "posts/2019-03-28-closures-in-python/index.html",
    "href": "posts/2019-03-28-closures-in-python/index.html",
    "title": "Closures in Python",
    "section": "",
    "text": "A closure is technique that allows functions to bundle variables in their local scope for access at a later time. If there was a variable x with the value of ten in the namespace for instance, even after that code executed, we could refer back to the namespace and x would still be 10. That’s a little dry; fortunately the idea is easier to illustrate in code using a function with a function inside it.\n\ndef add_ten_to_things():\n    # inside this function is a scope\n    # we'll call it \"outer scope\"\n    x = 10\n    def inner(y):\n        # this function has a scope\n        # we'll call it \"inner scope\"\n        return x + y\n    return inner\n\nNotice how in the inner function we refer to the outer scope (to the variable x) and then return the inner scope? This is the mechanism that allows closures to work: that x is now packaged into the returned inner(). Let’s see how it can be accessed:\n\n# we call the original add_ten_to_things() function that returns the inner function/namespace\nadd_ten = add_ten_to_things()\n\n# we pass something for \"y\" and it will still be able to add 10 to it\nprint(add_ten(20))\n\n30\n\n\nBecause 10 is stored in the local namespace of the outer function that produces the new function, 10 can still be accessed by it. A function that makes functions that add ten to things isn’t especially useful, however:\n\ndef add_x_to_things(x):\n    def adder(y):\n        return x + y\n    return adder\n\nincrease_by_five = add_x_to_things(5)\nincrease_by_ten = add_x_to_things(10)\n\none_plus_five = increase_by_five(1)\none_plus_ten = increase_by_ten(6)\n\nprint(one_plus_five, one_plus_ten)\n\n6 16\n\n\nYou’d be forgiven for wondering what the point of that would be. It makes more sense once you realize you can pass arguments to the outer function and that they will be accessible to the inner one. To illustrate, consider the following example, where a function can make several types of greeter builders depending on what is passed to the outer function.\n\n# the outer function\ndef greeter_maker(greeting):\n    # inner function\n    def greeter(name):\n        print(greeting + \",\", name)\n    # outer function returns the inner \n    return greeter\n\n\nformal_greeter = greeter_maker(\"Hello, pleased to meet you\")\ncasual_greeter = greeter_maker(\"Heyo\")\n\nIn action:\n\ncasual_greeter(\"Josh\")\nformal_greeter(\"Sruthi\")\n\nHeyo, Josh\nHello, pleased to meet you, Sruthi\n\n\nAnother builder:\nWhile this show some flexibility that could be made into something useful, it can be hard to see just what a powerful idea this idea is without some context, so we will look at one more example. Consider the following class:\n\nclass Person(object):\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\nThis is an extremely simple Python class that models a Person with a name and age. It’s probably obvious to any hypothetical user of this class that “name” is a string and “age’ is an integer (or at least a number or some kind). However, as Python is dynamically typed, there is nothing in the language itself to enforce this. Consider the following:\n\n# everything goes according to plan...\nchristian = Person(\"Christian Slater\", 49)\n\n# whoops, wrong order...\nrami = Person(37, \"Rami Malek\")\n\n# gets nane \nprint(christian.name)\n# not what we though we'd get\nprint(rami.name)\n\nChristian Slater\n37\n\n\nThis program will run just fine until sometime downstream when someone tries to do a computation a involving rami’s name or age but has the wrong type of data (ie, age += 1 and finds it’s a string). Consider the following usage of a closure to prevent this (this is much simpler than it seems once you see it in context so hang on):\n\n# takes a dictionary of attributes-&gt;types, and a model\ndef typechecking_builder(required_params, model):\n    \n    def _builder(attributes_passed):\n        # iterate through the dict that was passed to the outer function\n        for k in attributes_passed.keys():\n            current_param = attributes_passed[k]\n            type_required = required_params[k]\n            # ensure they're what we've been told to expect\n            try:\n                assert type(current_param) == type_required\n                print(\"Type check passed for\", current_param, \"of type\", type_required)\n            # raise an error if they're not\n            except AssertionError:\n                print(\"\\tfound\", current_param, \"of type\", type(current_param), \"Required:\", type_required)\n                raise TypeError(\"Type check failed for:\", k) # whatever you want to do for exception handling\n    # return the type checking function\n    return _builder\n\nWe can now make a builder that will type check for us, telling the function what it should expect when it is called:\n\n# make people, thier name is a string, thier age is an int, they're of type Person\nperson_builder = typechecking_builder({\"name\": str, \"age\":int}, Person)\n\nWhen we use this, it will check the types we passed:\n\ncarly = person_builder({\"name\": \"Carly Chaikin\", \"age\": 28})\n\nType check passed for Carly Chaikin of type &lt;class 'str'&gt;\nType check passed for 28 of type &lt;class 'int'&gt;\n\n\nNow, if we make the simple mistake from before, we get feedback right away:\n\nsunita = person_builder({\"name\": 32, \"age\": \"Sunita Mani\"})\n\n    found 32 of type &lt;class 'int'&gt; Required: &lt;class 'str'&gt;\n\n\nTypeError: ('Type check failed for:', 'name')\n\n\n…and every Person we build will be type checked!"
  },
  {
    "objectID": "posts/2019-03-28-closures-in-python/index.html#closures-in-python-what-they-are-and-a-practical-application-for-type-checking-objects",
    "href": "posts/2019-03-28-closures-in-python/index.html#closures-in-python-what-they-are-and-a-practical-application-for-type-checking-objects",
    "title": "Closures in Python",
    "section": "",
    "text": "A closure is technique that allows functions to bundle variables in their local scope for access at a later time. If there was a variable x with the value of ten in the namespace for instance, even after that code executed, we could refer back to the namespace and x would still be 10. That’s a little dry; fortunately the idea is easier to illustrate in code using a function with a function inside it.\n\ndef add_ten_to_things():\n    # inside this function is a scope\n    # we'll call it \"outer scope\"\n    x = 10\n    def inner(y):\n        # this function has a scope\n        # we'll call it \"inner scope\"\n        return x + y\n    return inner\n\nNotice how in the inner function we refer to the outer scope (to the variable x) and then return the inner scope? This is the mechanism that allows closures to work: that x is now packaged into the returned inner(). Let’s see how it can be accessed:\n\n# we call the original add_ten_to_things() function that returns the inner function/namespace\nadd_ten = add_ten_to_things()\n\n# we pass something for \"y\" and it will still be able to add 10 to it\nprint(add_ten(20))\n\n30\n\n\nBecause 10 is stored in the local namespace of the outer function that produces the new function, 10 can still be accessed by it. A function that makes functions that add ten to things isn’t especially useful, however:\n\ndef add_x_to_things(x):\n    def adder(y):\n        return x + y\n    return adder\n\nincrease_by_five = add_x_to_things(5)\nincrease_by_ten = add_x_to_things(10)\n\none_plus_five = increase_by_five(1)\none_plus_ten = increase_by_ten(6)\n\nprint(one_plus_five, one_plus_ten)\n\n6 16\n\n\nYou’d be forgiven for wondering what the point of that would be. It makes more sense once you realize you can pass arguments to the outer function and that they will be accessible to the inner one. To illustrate, consider the following example, where a function can make several types of greeter builders depending on what is passed to the outer function.\n\n# the outer function\ndef greeter_maker(greeting):\n    # inner function\n    def greeter(name):\n        print(greeting + \",\", name)\n    # outer function returns the inner \n    return greeter\n\n\nformal_greeter = greeter_maker(\"Hello, pleased to meet you\")\ncasual_greeter = greeter_maker(\"Heyo\")\n\nIn action:\n\ncasual_greeter(\"Josh\")\nformal_greeter(\"Sruthi\")\n\nHeyo, Josh\nHello, pleased to meet you, Sruthi\n\n\nAnother builder:\nWhile this show some flexibility that could be made into something useful, it can be hard to see just what a powerful idea this idea is without some context, so we will look at one more example. Consider the following class:\n\nclass Person(object):\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\nThis is an extremely simple Python class that models a Person with a name and age. It’s probably obvious to any hypothetical user of this class that “name” is a string and “age’ is an integer (or at least a number or some kind). However, as Python is dynamically typed, there is nothing in the language itself to enforce this. Consider the following:\n\n# everything goes according to plan...\nchristian = Person(\"Christian Slater\", 49)\n\n# whoops, wrong order...\nrami = Person(37, \"Rami Malek\")\n\n# gets nane \nprint(christian.name)\n# not what we though we'd get\nprint(rami.name)\n\nChristian Slater\n37\n\n\nThis program will run just fine until sometime downstream when someone tries to do a computation a involving rami’s name or age but has the wrong type of data (ie, age += 1 and finds it’s a string). Consider the following usage of a closure to prevent this (this is much simpler than it seems once you see it in context so hang on):\n\n# takes a dictionary of attributes-&gt;types, and a model\ndef typechecking_builder(required_params, model):\n    \n    def _builder(attributes_passed):\n        # iterate through the dict that was passed to the outer function\n        for k in attributes_passed.keys():\n            current_param = attributes_passed[k]\n            type_required = required_params[k]\n            # ensure they're what we've been told to expect\n            try:\n                assert type(current_param) == type_required\n                print(\"Type check passed for\", current_param, \"of type\", type_required)\n            # raise an error if they're not\n            except AssertionError:\n                print(\"\\tfound\", current_param, \"of type\", type(current_param), \"Required:\", type_required)\n                raise TypeError(\"Type check failed for:\", k) # whatever you want to do for exception handling\n    # return the type checking function\n    return _builder\n\nWe can now make a builder that will type check for us, telling the function what it should expect when it is called:\n\n# make people, thier name is a string, thier age is an int, they're of type Person\nperson_builder = typechecking_builder({\"name\": str, \"age\":int}, Person)\n\nWhen we use this, it will check the types we passed:\n\ncarly = person_builder({\"name\": \"Carly Chaikin\", \"age\": 28})\n\nType check passed for Carly Chaikin of type &lt;class 'str'&gt;\nType check passed for 28 of type &lt;class 'int'&gt;\n\n\nNow, if we make the simple mistake from before, we get feedback right away:\n\nsunita = person_builder({\"name\": 32, \"age\": \"Sunita Mani\"})\n\n    found 32 of type &lt;class 'int'&gt; Required: &lt;class 'str'&gt;\n\n\nTypeError: ('Type check failed for:', 'name')\n\n\n…and every Person we build will be type checked!"
  },
  {
    "objectID": "posts/2020-01-20-mlk/index.html",
    "href": "posts/2020-01-20-mlk/index.html",
    "title": "In Honor of MLK 2020: ‘Letter from Birmingham Jail’ Wordcloud Example in R",
    "section": "",
    "text": "A few years ago, I wrote an MLK day post on creating a wordcloud from the legendary “I Have a Dream” speech. I thought I would continue the tradition by doing another for Letter From Birmingham Jail. The code is available here. It’s a simple project, but can be a compelling way to quickly highlight themes from the original text to pique the interest of readers.\n\n\n\nwordcloud"
  },
  {
    "objectID": "posts/2020-04-27-r-type-error/index.html",
    "href": "posts/2020-04-27-r-type-error/index.html",
    "title": "A Case study in Silent Data Corruption in an RNA-Seq Experiment",
    "section": "",
    "text": "During a recent differential gene expression analysis I had a few issues converge in such a way that the code would run fine from top to bottom but would silently compromise the analysis and produce bogus results. Essentially it was a combination of a bugged error message in a package I was using, a bad row in an input file I was given, and some weird behavior of R (and of course, my initial carelessness in not noticing sooner). I caught this issues with some included QC functions.\nIt seemed plausible that these issues could join forces to trip people up now and then, so I figured I would document it in case people wanted to keep an eye out for it.\nIf you’re not interested in this sort of analysis but use R, there is still a short takeaway summarized here:\n\n(x &lt;- c(\"1\", \"10\", \"100\"))\n\n[1] \"1\"   \"10\"  \"100\"\n\n\n\n(x &lt;- as.factor(x))\n\n[1] 1   10  100\nLevels: 1 10 100\n\n\n\n(x &lt;- as.integer(x))\n\n[1] 1 2 3\n\n\nWhoops! Those numbers have been totally changed.\nThe repo for the analysis here, and the PDF is below:\nDownload\n\nNOTE: It appears more recent versions of DESeq2 don’t do this!\n\n\nNOTE: Edited for clarity, 2022-12-13"
  },
  {
    "objectID": "posts/2020-07-09-the-myth-of-police-racism-debunk2/index.html",
    "href": "posts/2020-07-09-the-myth-of-police-racism-debunk2/index.html",
    "title": "Letter - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors",
    "section": "",
    "text": "UPDATE: One of the articles misused in the piece has been retracted, with the authors calling out Mac Donald by name as misusing their work.\n\n\nUPDATE (07/11/2020): The mysterious quote has been found and unleashed a new error. Brief rundown here, details to follow.\n\nA few weeks ago, I published a video and a summary of some pretty egregious lying with statistics that was published in The Wall Street Journal. I was not the only person to observer this. Since then, authors of one of the studies cited have asked for a retraction, saying they were careless in ways that allowed for misuse of their work. They also call out Heather Mac Donald, the author of the Op-ed, by name.\nI’d written an essay-style version of my original critique as well, which follows:\nThe recent article “The Myth of Systemic Police Racism”[1] contains numerous scientific and journalistic errors that invalidate its core argument and suggest negligence or a desire to mislead the reader.\nFirstly, It cites studies about police shootings in isolation to draw conclusions about all policing activities, which is fundamentally unsound. Similarly the author includes a study of shootings by the Philadelphia Police Department. This does not entitle you to make claims about “Police” as an entity - it entitles you to make claims about the Philadelphia Police Department.\nOne of the papers cited[2], a study by Johnson et al, does not make or support the claim that “Police Racism” is a “myth” - it only deals with shootings. Furthermore, two more recent papers analyzed the results of the Johnson paper. The first found, examining its claim “that their results hold across subgroups of victims”[2]:\n“.. an alternative model that focused on young (age 20 y), unarmed male victims that showed no signs of mental health problems and were not suicidal in a county with equal proportions of Black and White citizens… suggested that victims with these characteristics are 13.67 times more likely to be Black than White.”[3]\nThe other[4] found an issue saying “its approach is mathematically incapable of supporting its central claims” given lack of ability to account for a key probabilistic formula. The authors address this in a correction letter clarifying their intent to state “As the proportion of White officers in a fatal officer-involved shooting increased, a person fatally shot was not more likely to be of a racial minority”[5]. This is an interesting finding but not in line with the claim made in the op-ed: “no significant evidence of antiblack disparity in the likelihood of being fatally shot by police”[1]. Strangely, the author presents this as a quote but it does not appear in the paper cited.\nThe authors of the follow-up study brought their findings to light months ago. When I accessed the Johnson paper on June 7th, both papers criticizing it and the correction were linked to in the top of the paper so that readers would see new evidence and place the paper in context; they were published in January. The WSJ piece was published June 2nd.\nPuzzlingly, the author cites Roland G. Fryer, Jr. One of his most famous findings[6] includes a dataset in which police shootings show no racial disparity, but he found that nearly all other uses of force did. In particular, Fryer states ”Even when officers report civilians have been compliant and no arrest was made, blacks are 21.2 percent more likely to endure some form of force in an interaction”[7]. The second sentence of his abstract[7] mentions racial disparities - it’s hard to imagine a diligent researcher not knowing about this.\nThe best defence of the article is that it’s an opinion piece. This is insufficient given the confidence with which the author makes their claim and the severity of the scientific errors they make in defending it. The author employs statistical evidence that has been strongly called into question months ago, or is only obliquely and selectively related to their position. Most unforgivably it cites research that goes against its own position out of negligence or a deliberate effort to deceive. Because the piece presumes to inform the reader using quantitative evidence, it should be held to a higher quantitative standard. Given the importance of the subject at hand and considering the speed at which misinformation spreads, ignoring these issues is irresponsible.\nCitations:\n\n[1]“The Myth of Systemic Police Racism - WSJ.” 2 Jun. 2020, https://www.wsj.com/articles/the-myth-of-systemic-police-racism-11591119883. Accessed 18 Jun. 2020.\n[2] “Officer characteristics and racial disparities … - PNAS.” https://www.pnas.org/content/116/32/1877. Accessed 8 Jun. 2020.\n[3] “Young unarmed nonsuicidal male victims of fatal use … - PNAS.” https://www.pnas.org/content/117/3/1263. Accessed 8 Jun. 2020.\n[4] “Making inferences about racial disparities in police … - PNAS.” https://www.pnas.org/content/117/3/1261. Accessed 8 Jun. \n[5]“Correction for Johnson et al., Officer characteristics … - PNAS.” https://www.pnas.org/content/117/16/9127. Accessed 18 Jun. 2020.\n[6] “Surprising New Evidence Shows Bias in Police Use of Force ….” 12 Jul. 2016, https://www.nytimes.com/2016/07/12/upshot/surprising-new-evidence-shows-bias-in-police-use-of-force-but-not-in-shootings.html. Accessed 8 Jun. 2020.\n[7] “An Empirical Analysis of Racial Differences in Police … - NBER.” https://scholar.harvard.edu/fryer/publications/empirical-analysis-racial-differences-police-use-force Accessed 9 Jun. 2020."
  },
  {
    "objectID": "posts/2020-07-11-fact-check-quote/index.html",
    "href": "posts/2020-07-11-fact-check-quote/index.html",
    "title": "Fact Check: The Quote Heather Mac Donald presents when discussing Johnson et al comes from a passage of a pre-print that does not appear in the published work.",
    "section": "",
    "text": "In a recent Op-ed Heather Mac Donald presents the following quote:\n\n“no significant evidence of antiblack disparity in the likelihood of being fatally shot by police”\n\nThis quote does not appear in Johnson et al, the study she cites.\nThe quote comes from a pre-print (before peer review) reply to a letter criticizing the methodology.\nReading the pre-print and the published version reveals the quote appears in the pre-print (uploaded August 16, 2019) but not in the published literature (published, January, 21, 2020).\n\nnote: thanks to @ulymelendez for tracking this down."
  },
  {
    "objectID": "posts/2021-01-19-retraction-watch/index.html",
    "href": "posts/2021-01-19-retraction-watch/index.html",
    "title": "Rygor: a Retraction Watch database companion",
    "section": "",
    "text": "I recently saw a tweet about retracted papers that still pick up citations so I thought I’d create a tool to help researchers double check themselves. I call it Rygor. It takes a file of paper names, auotomatically submits them to RWDB, and generates a report stating if it found each paper to be at risk for matching a retracted one. I may add more features to it depending on time, such as automatically pulling th citations from a web page, etc, depending on the availability of structured citation data, APIs, etc."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html",
    "href": "posts/2021-09-29-survival-guide/index.html",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "",
    "text": "I’d been toying with the idea of making a survival guide for my students for a few semesters for, and I decided to actually sit down and do it this time. It’s written based on my experience in the MS program where I now teach the introductory course. While it’s written with students going into a particular Bioinformatics MS program in mind, I suspect the lessons apply to a lot of programs. I also imagine a lot of it would apply to a more general data science student as well, and a few things that could be handy for grad students in other fields. I could be wrong but I decided to run it up the flag pole in case it’s useful to anyone else. Learn from my biffs and triumphs.\nUpdated 8-19-2022"
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#you-are-not-an-imposter",
    "href": "posts/2021-09-29-survival-guide/index.html#you-are-not-an-imposter",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "You are not an imposter",
    "text": "You are not an imposter\nIt’s sometimes intimidating to be around the kind of people you meet in the Boston biotech scene. It’s a bustling, affluent area packed with hugely successful companies, top schools, and really smart, driven people.\nIt’s important to keep in mind you’re not the only regular person who doesn’t get it in a sea of geniuses. Nobody is as sharp as they are at their best all the time - you can be a famous professor and still spill coffee on stuff and start talking in a zoom meeting without turning on your microphone all the time.\nEven the biggest rock stars started somewhere, and it’s more likely they got where they are by making the most of the opportunities they had, working in a persistent, intelligent way, and getting a little lucky than by being a magical genius you could never hope to comprehend.\nSuccess is not guaranteed but keep in mind that you wouldn’t be here if the committee didn’t think you had a strong chance to hang in the scene.\n\nA note on ‘math people’\nMath has been terrifying bio people from time immemorial (and now they have code to deal with too). I was far from a math person, having struggled in undergrad, feeling as though the “why” was missing from the content, before eventually forcing my way into decent standing. The stats used in bioinformatics tend to come from practical concerns, so the “why” is easier to see. It’s still hard sometimes (you’re never really done learning), but it’s gone from something I dreaded to something I can use practically and enjoy, and I find it’s one of the main ways I contribute to my team now. The bottom line is that not feeling like a ‘math person’ doesn’t rule you out of a productive career for a few reasons:\n\n(Bio)statistics is a quirky subfield of math - you can’t necessarily assume that having a tough time in trig is going to translate to a tough time here.\nYou can make computers do the grunt work so you can focus on the concepts. We often use code libraries for statistical procedures. This doesn’t mean you can stop trying to understand what is happening but it does mean you can be effective without remembering what every Greek character in a formula stands for. It also means there are fewer errors along the lines of “Drat, I forgot to carry the 1”. Understanding what is an appropriate statistical procedure for your situation (or knowing when you don’t know, and where to look to find out) is more important than being able to write out matrix multiplication by hand.\n\nWhen I was finishing my MS I took a PhD level stats course in each of my final terms, getting an A and a B while working at a co-op full time. I’m still waiting to feel like a “math person”. I’m not, however, waiting to be able to make meaningful contributions in my lab.\n\n\nBioinformatics is hard\nI’m not saying most people think this field is a cake-walk or anything, but it’s easy to forget just what we have to juggle. You’ll eventually be expected to know a lot of what a statistician knows, a lot of what a biologist knows, and a lot of what a software developer knows. You don’t have to be a stats whiz to see the numbers get a little concerning here. It’s normal to come across something you’re not familiar with (or don’t remember well) and to look it up. Which brings me to…"
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#adopt-the-mindset-of-a-perpetual-student",
    "href": "posts/2021-09-29-survival-guide/index.html#adopt-the-mindset-of-a-perpetual-student",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "Adopt the mindset of a perpetual student",
    "text": "Adopt the mindset of a perpetual student\nYou might think of yourself as a bioinformatics apprentice now. In a way, if you succeed in the field, you’ll be one for the rest of your life (or at least your career). There is just too much to know.\n\nYour job is to be good at learning stuff now\nFor my first publication, my contribution to the paper was co-writing an app in a language I’d never used before. What’s interesting about this is actually how uninteresting it is - this kind of thing happens all the time. Some people say “I’m starting a new project, time to learn a new technology!” and are only half joking. Stay open minded about learning new stuff and get used to searching and reading documentation carefully. The is true for the biological and statistical aspects of the field as well - new methods and findings are being reported all the time.\n\n\nAggressively pursue mentorship\nOne important factor to consider when looking at jobs and co-ops is the availability of a mentor. This is so important to me that when offered a transfer to a new department in my work, I told them my acceptance was contingent on them finding me a biostats mentor. Ask about who you’ll be able to go to when you get stuck, or who will help you learn the job. It doesn’t have to be one person either - in a field like this you might have somebody you got to to for help with biology concepts and someone else for code. This is fine, just make sure you know these people exist and that you’ll be able to access some of their time. If offered two jobs, one at the most famous institution on earth with an unavailable grouch, and one somewhere you’ve never heard of with someone who cares about teaching, take the second one. You can get fancy later - get good first, and it will be easier.\n\n\nRead\nGrad school is very concentrated in terms of acquiring new information, which is part of the point. However, our field is composed of a lot of nitpicky details - programming minutia, subtle statistical distinctions, complex biological pathways. This isn’t even to mention the technical processes of sequencing itself. It’s easy to miss the big picture. Reading this book helped me see the forest through the trees (and I find myself reviewing sections). Review-style papers in scientific journals also help catch you up on a topic at a high level. Additionally, take some time to read articles on biology, medicine, and computation that appear in quality magazines and attend talks that strike your interest (you’ll probably have a chance to do so at a lof of companies, and will be bombarded with invites at most academic institutions).\n\n\nAsk questions\nIt can sometimes be anxiety-inducing to ask a question, especially if you think it’s a basic one. I know I felt this way sometimes (and still do). It’s usually worth it though, and more often than not, in my experience, people aren’t going to give you a hard time about it."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#treat-co-op-like-a-real-job",
    "href": "posts/2021-09-29-survival-guide/index.html#treat-co-op-like-a-real-job",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "Treat co-op like a real job",
    "text": "Treat co-op like a real job\nCo-ops vary in their scope and responsibility so it’s not always clear how much of a chance to run with your own project you’ll have. Either way, make the most of the networking opportunities and dive into whatever task your given - you never know if it will lead to jobs or more learning opportunities.\n\nConsider not taking classes during co-op if it’s paid\nIf your co-op pays you enough to live on, consider making it your complete professional focus. I was a TA while on co-op but, at least for me, that was easier than taking a difficult class. The time where I was taking really difficult coursework and working was very demanding. I feel I could have been a better student and worker if it was spread out better."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#mingle",
    "href": "posts/2021-09-29-survival-guide/index.html#mingle",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "Mingle",
    "text": "Mingle\nOne of the biggest benefits of grad school is the networking. Introduce yourself to other people and treated them respectfully. The people you meet, both faculty and classmates, can inform you about job opportunities and become future coworkers, friends, or collaborators (all of these have happened to me)."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#be-intentional-with-your-time-and-energy",
    "href": "posts/2021-09-29-survival-guide/index.html#be-intentional-with-your-time-and-energy",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "Be intentional with your time and energy",
    "text": "Be intentional with your time and energy\n\nGet organized\nDevelop a consistent way of capturing and organizing tasks. It doesn’t matter what it is, it doesn’t have to be perfect, and it doesn’t have to be set it stone, but it does need to exist. Find something that works and tweak it as needed. The same goes for planning your days and weeks - do not make a habit of reacting to your day or you’ll be on your heels, stressed, and doing less than your best work for the next 2-3 years.\n\n\nGet good\nThe most important book I’ve ever read is called So Good They Can’t Ignore you by Cal Newport, who got famous by being good at the kind of things people do in grad school. I read it in 2012 and I still consider the ideas in it when making all my career decisions. Consider reading it, or find a detailed summary.\n\nPractice\nThis might seen obvious but is easy to overlook - learning new languages, programming or otherwise, requires practice. Passive consumption of material is often needed to start a new skill but you won’t ever get good enough to hire without pushing yourself to new levels by enacting what you know in a repeated, focused way. There is nothing wrong with repeating exercises that still challenge you. Don’t look at coding like collecting information you need to know, think of it as practicing for a recital. In the internet age, simply knowing stuff takes a back seat to being able to do something with what you know to an even greater degree because most information is a search away.\n\n\n\nFocus\nDoing well at things that are hard generally requires focus and skill. Grad school is hard (in and of itself) and so are the individual things you will do. Make time to really dive into them without interruptions. More on this (another key book for me) is here, also from Newport.\n\nMultitasking is, by and large, a myth\n“Multitasking” on substantive mental work is not real. What people think of as multitasking is just switching between tasks in alternation. This costs more energy than doing two things one at a time. Your mental energy is perhaps the most valuable currency you have in terms of getting graduate-level work done - don’t pay extra taxes on it when you don’t need to.\n\n\nUse social media thoughtfully\nSocial media apps are an attempt, by some of the most world’s most successful companies, to take your attention away from where you intended to put it. They are deliberately designed, using the same psychological principles that went into the slot machine, to produce addictive behaviors and maximize the time you spend on them. This principle is in direct tension with working intentionally and focusing on difficult material. It’s hard to find two hours a day, but that’s how much time most Americans spend on social media. The cost of this is greater if it makes it harder to focus even when you’re not on it (by training your brain to stop what it’s doing and look for a reward). I’m not saying using them has no benefits or that they will turn you into a zombie, but it’s worth being intentional about how you engage with them, attempting to get the benefits of socializing with minimal drawbacks. For me this looks like checking the platform I like one or twice a day in a browser for a few minutes, but not having it on a mobile device during the work week. The 30 seconds I spend installing it and logging in on Saturdays are nothing next to the mental health and productivity benefits I felt after using it on my terms, not the terms of the people who profit from me (why would they have my best interest in mind?). Some people might be fine with their habits as-is and what works is personal, so experiment if you feel the need to - you can read more about this kind of thing here.\n\n\n\nGet stuck as soon as possible\nThis might sound weird, but when you’re working on an assignment, think of your job as getting stuck as soon as possible. When you’re planning your work, assume you will get stuck and need to wait on a reply from a (probably very busy) classmate, TA, or instructor. Getting as far as you can as soon as you can helps you get things in on time and also make it easier to help you - if we’re sending a rushed reply or trying to squeeze in a meeting, we’re not going to have as much time to get into the material in detail. I’m not saying you can’t ask for help as things get closer to being due, just that it’s a better experience for both parties if you start early enough to know if you need a hand."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#take-care-of-yourself",
    "href": "posts/2021-09-29-survival-guide/index.html#take-care-of-yourself",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "Take care of yourself",
    "text": "Take care of yourself\n\nSleep\nI once asked one of my undergrad professors for advice on becoming a better student. He is a really smart dude - he had a seemingly inexhaustible knowledge of cell and molecular bio, could lecture beautifully for hours from a single page of notes, and had published as a post-doc at a top university. I assumed he had some workflow that I could never dream up. I was surprised by the simplicity of his answer when he told me he wished he slept more during school. He felt he would have retained more information. Even the highest performers answer to their physiology. I’m not saying I made it though grad school without a few all-nighters (or real late nights), especially balancing a job for most of it. But they were rare. Prevent as many as you can by being strategic.\n\n\nMaintain a compartment for your life\nGrad school is great at invading your life. I found though, that by planning carefully and focusing hard, I didn’t have to give up everything. If playing board games with your friends on the weekend is what gets you through the day, plan around it. You’re probably going to take a few hits in terms of what you have the time and energy for, but, with care, your outlets can take care of you, if you take care of them.\n\n\nAsk for help\nIf you’re struggling personally, help is available and you should use it. Grad school is hard enough without leaving help on the table."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#youre-a-quant-now",
    "href": "posts/2021-09-29-survival-guide/index.html#youre-a-quant-now",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "You’re a quant now",
    "text": "You’re a quant now\nCongrats, nerd.\n\nTake statistics seriously (and early)\nA lot of work in our field is executed by code but motivated by statistics. I personally loved learning to code (and still do) - I love stats too, but it was less intuitive to get started. Code will sometimes tell you when you’re wrong with an error or a warning - a statistical test will not, silently letting you draw bogus conclusions. Thus, it pays to take it seriously, not just a means to and end. Give these classes the attention they deserve, and…\n\n\nConsider a data science elective (DA5020/5030)\nThere is statistics in the program, but it’s an important thing and there is too much to learn in one semester. Additionally, these additional courses may also expose you to the nuts and bolts of getting datasets in out of a database and messing around with them, which isn’t always covered in more theoretical stats courses. They may also include “machine learning” related topics in addition to more “classical” statistics. That being said..\n\n\nMachine learning is built on statistical foundations\nPlease do not be the person who doesn’t know what logistic regression is for but tries to solve every problem using a neural network. It’s good to explore, and neural nets are awesome, so you should learn about them eventually. But they’re made of pieces of statistics - it’s been said a neural net is just a bunch of logistic regressions standing on top of one another in a trench coat - and you’ll want to know what those pieces are. Misunderstanding and net can lead to sorrow, so be wary of putting the cart before the horse. They are one one of many tools, all of which have different uses, strengths, and weaknesses (the drawbacks of a neural nets beyond the scope of this article but are important enough to look into).\n\n\nLearn (make your peace with?) R\nThe R programming language is notoriously quirky. It’s also built from the ground up to eat statistics of basically any complexity for breakfast. The phrases I’ve uttered in painstakingly becoming a proficient R users would make a British soccer (er, football) fan blush. In fact, I learned it only because I was tricked and forced into doing so by an out-of-date course description that promised me sweet, beautiful Python. I was pretty salty at the time, but this is easily the best annoying thing that ever happened to my career. People all over the place are just waiting to blow up the slack channel asking you for plots, and R is galaxy-class tool for getting them the goods. A few things:\n\nYou won’t escape R for differential expression, and lots of people are going to ask you to do this for them.\nLook up sleuth, limma-voom, and DESeq2 if you think this these experiments are coming your way on co-op. If it’s not obvious which is right for the experiment, I go for limma-voom.\n\n\nUse the tidyverse\nThis is essentially a dialect of R at this point. Using tidyverse R vs “base” R is the kind of thing people fight on twitter or get ideological about, but ignore that. It’s more practical to start with the tidyverse for most people (way less quirky and obtuse, extremely well documented). dplyr and ggplot2 should be among the first components you learn.\n\n\nCheck your types\nDo this in any language, but especially R. Part of why makes R weird is the “type system” it uses, so read up on how to check what types are in your dataframe (class, and similar functions). Read about the factor type. We can talk more about R later - let me know if you’re interested, but I’m pushing it in terms of the scope of this article already."
  },
  {
    "objectID": "posts/2021-09-29-survival-guide/index.html#misc-on-being-noticed-being-practical-and-not-shooting-yourself-in-the-foot-at-least-not-too-often-or-without-meaning-to",
    "href": "posts/2021-09-29-survival-guide/index.html#misc-on-being-noticed-being-practical-and-not-shooting-yourself-in-the-foot-at-least-not-too-often-or-without-meaning-to",
    "title": "A Survival Guide for Students Starting an MS in Bioinformatics",
    "section": "Misc: On being noticed, being practical, and not shooting yourself in the foot (at least not too often, or without meaning to)",
    "text": "Misc: On being noticed, being practical, and not shooting yourself in the foot (at least not too often, or without meaning to)\n\nGit is part of your life now\nUsing version control (which is usually git) is now part of being a functioning adult for you. If you’re writing anything that’s bigger than a throw-away, one-off script, make it into a git repo (I made one just for this article). If you’re not yet experienced enough to tell if it’s a throw-away, one-off script, put it in git just in case. If you think you’re experienced enough to tell, you might be mistaken, so it couldn’t hurt to put it into git anyway. Commit and push your work all the time. It’s difficult to overstate the importance of version control to a good workflow. Think of it like backing up the data on your phone, except that you can instantly go back to every version of your data. And make “branches” of your data where you try and save different things in different ways, so two or more working versions can exist. And, if you like one version better, you can make it the new official version. Or merge them both into one. Or send it to a friend, and have them make a version, which will exist without messing with your version. I once needed to exactly recreate a plot of randomly partitioned data from 6 months prior for the VP of Data Science at my co-op. I’d forgotten how I did it and deleted the code from my computer. If I hadn’t been using git carefully, I’d have been toast. If you’re on a project of non-trivial size or importance and someone tells you that you don’t need version control, it should feel like they just told you “I don’t need to brush my teeth”.\nUPDATE: A while back I made a video of a very simple git workflow. You can check it out here.\n\n\nRead the error message, then paste it into a search engine\nSeriously, the degree to which this will make your life easier is almost absurd. You’re going to be seeing error messages in several languages throughout the semester and many more in your career. This is where a lot of your time blocks will come from. If the code is syntactically incorrect, it simply won’t run, and there is no way to know if it will take you ten seconds or ten hours to fix it. Luckily, I learned this early on when I took an error I had to a TA and he simply copied out of my terminal and into a search bar. A post on how to fix it was in the first few results. The internet is bursting with people who want to know why their code is broken, and there are many forums of questions with answers to common questions. Get used to navigating them.\n\n\nIf a post bails you out, bookmark it\nGetting stuck is a pain. Getting stuck on something you’ve been stuck on before but can’t recall how to fix is a huge pain. Bookmark or clip posts that save your butt.\n\n\nUse a virtual machine or spare laptop if you want to tinker with a Linux system\nI found it fun to explore Linux - different software, distros (I use Arch BTW), and desktop environments, and so on. However, if you tell a Linux terminal to do something insane, and you have the authority to do so, it will listen to you. Sometimes this leads to a learning experience such as “Hmm, why is my desktop gone?” (true story) or “Hey, GRUB is broken and my computer boots to a black screen and flashing white cursor, what do I do now?” (true story 3+ times). If you want to break stuff and learn to fix it, that’s awesome, just don’t do it on the machine you’re taking a final on tomorrow.\n\n\nMake something\nIt’s nice to have a portfolio, hosted somewhere like GitHub, where you can show off things you’ve made. If you do a creative final project, clean it up and put it somewhere public so you can flex for future employers. I also found it fun to do coding or data projects on random things I found interesting, and to show those off too. This will set you apart from those who can only show homework they did. You can look at my site or portfolio for a sense of what these might look like, but don’t be afraid to do you own thing."
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "",
    "text": "update: 12-17-22 - added sorting and table."
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#basics",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#basics",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Basics",
    "text": "Basics\nHow to get basic information about the df.\n\nDimensions of a df\nThis is like dim in R.\n\n# rows by columns \ndf.shape\n\n(32, 11)\n\n\n\n\nNumber of rows\nThis is like nrow in R.\n\n# just the rows\nlen(df)\n\n32\n\n\n\n\nNumber of columns\nThis is like ncol in R.\n\n# just the columns\nlen(df.columns)\n\n11\n\n\nYou could also just index the results of df.shape.\n\n\nData types\n\n# a summary of types by column\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32 entries, Mazda RX4 to Volvo 142E\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   mpg     32 non-null     float64\n 1   cyl     32 non-null     int64  \n 2   disp    32 non-null     float64\n 3   hp      32 non-null     int64  \n 4   drat    32 non-null     float64\n 5   wt      32 non-null     float64\n 6   qsec    32 non-null     float64\n 7   vs      32 non-null     int64  \n 8   am      32 non-null     int64  \n 9   gear    32 non-null     int64  \n 10  carb    32 non-null     int64  \ndtypes: float64(5), int64(6)\nmemory usage: 3.0+ KB\n\n\n\n\nSummary\n\n# summary of distributional information\ndf.describe()\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\ncount\n32.000000\n32.000000\n32.000000\n32.000000\n32.000000\n32.000000\n32.000000\n32.000000\n32.000000\n32.000000\n32.0000\n\n\nmean\n20.090625\n6.187500\n230.721875\n146.687500\n3.596563\n3.217250\n17.848750\n0.437500\n0.406250\n3.687500\n2.8125\n\n\nstd\n6.026948\n1.785922\n123.938694\n68.562868\n0.534679\n0.978457\n1.786943\n0.504016\n0.498991\n0.737804\n1.6152\n\n\nmin\n10.400000\n4.000000\n71.100000\n52.000000\n2.760000\n1.513000\n14.500000\n0.000000\n0.000000\n3.000000\n1.0000\n\n\n25%\n15.425000\n4.000000\n120.825000\n96.500000\n3.080000\n2.581250\n16.892500\n0.000000\n0.000000\n3.000000\n2.0000\n\n\n50%\n19.200000\n6.000000\n196.300000\n123.000000\n3.695000\n3.325000\n17.710000\n0.000000\n0.000000\n4.000000\n2.0000\n\n\n75%\n22.800000\n8.000000\n326.000000\n180.000000\n3.920000\n3.610000\n18.900000\n1.000000\n1.000000\n4.000000\n4.0000\n\n\nmax\n33.900000\n8.000000\n472.000000\n335.000000\n4.930000\n5.424000\n22.900000\n1.000000\n1.000000\n5.000000\n8.0000"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#selections",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#selections",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Selections",
    "text": "Selections\nHow to specify which column you want.\n\nSelect a column\nNote that I’m using head() to get the first few rows to keep the output small. It’s not part of the selection of columns.\n\n# access a single column like an object property\ndf.mpg.head()\n\nMazda RX4            21.0\nMazda RX4 Wag        21.0\nDatsun 710           22.8\nHornet 4 Drive       21.4\nHornet Sportabout    18.7\nName: mpg, dtype: float64\n\n\nOr…\n\n# select a column (as a series)\ndf[\"mpg\"].head()\n\nMazda RX4            21.0\nMazda RX4 Wag        21.0\nDatsun 710           22.8\nHornet 4 Drive       21.4\nHornet Sportabout    18.7\nName: mpg, dtype: float64\n\n\n\n# select column (as a dataframe)\ndf[[\"mpg\"]].head()\n\n\n\n\n\n\n\n\nmpg\n\n\n\n\nMazda RX4\n21.0\n\n\nMazda RX4 Wag\n21.0\n\n\nDatsun 710\n22.8\n\n\nHornet 4 Drive\n21.4\n\n\nHornet Sportabout\n18.7\n\n\n\n\n\n\n\n\n\nSelect several columns by string name\n\n# select a column by several names\ndf[[\"mpg\", \"wt\"]].head()\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n\nMazda RX4\n21.0\n2.620\n\n\nMazda RX4 Wag\n21.0\n2.875\n\n\nDatsun 710\n22.8\n2.320\n\n\nHornet 4 Drive\n21.4\n3.215\n\n\nHornet Sportabout\n18.7\n3.440\n\n\n\n\n\n\n\n\n\nSelect from one column through another with names\n\n# select a range by name of start and end \"mph through hp\"\ndf.loc[:, \"mpg\":\"hp\"].head()\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n\n\n\n\n\n\n\n\n\nSelect arbitrarily\nHow to get based on a condition of your choosing. We might want to get only columns that start with “d”, for example.\n\n# get a list\nstarts_with_d = [i for i in list(df.columns) if i.startswith(\"d\")]\n\n# pass the list to pandas\ndf[starts_with_d].head()\n\n\n\n\n\n\n\n\ndisp\ndrat\n\n\n\n\nMazda RX4\n160.0\n3.90\n\n\nMazda RX4 Wag\n160.0\n3.90\n\n\nDatsun 710\n108.0\n3.85\n\n\nHornet 4 Drive\n258.0\n3.08\n\n\nHornet Sportabout\n360.0\n3.15"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#filtering",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#filtering",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Filtering",
    "text": "Filtering\nOperations where we’d used dplyr in R. In Pandas, filtering refers to operations by index, so what I’m thinking of is more like “querying” in Pandas terms.\n\nSingle condition\nPass logic in strings to the query method.\n\ndf.query(\"cyl == 6\")\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n\n\n\n\n\n\n\nMultiple conditions\n\ndf.query(\"cyl == 6 & hp &gt; 105\")\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#sorting",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#sorting",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Sorting",
    "text": "Sorting\n\nAscending\n\ndf.mpg.sort_values().head(10)\n\nLincoln Continental    10.4\nCadillac Fleetwood     10.4\nCamaro Z28             13.3\nDuster 360             14.3\nChrysler Imperial      14.7\nMaserati Bora          15.0\nMerc 450SLC            15.2\nAMC Javelin            15.2\nDodge Challenger       15.5\nFord Pantera L         15.8\nName: mpg, dtype: float64\n\n\n\n\nDescending\nThere is no ‘descending’ per se, just set ascending to False.\n\ndf.mpg.sort_values(ascending = False).head(10)\n\nToyota Corolla    33.9\nFiat 128          32.4\nLotus Europa      30.4\nHonda Civic       30.4\nFiat X1-9         27.3\nPorsche 914-2     26.0\nMerc 240D         24.4\nDatsun 710        22.8\nMerc 230          22.8\nToyota Corona     21.5\nName: mpg, dtype: float64"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#applying-functions",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#applying-functions",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Applying functions",
    "text": "Applying functions\n\nMap a function with no arguments to a single column\n\n# some function to apply\ndef increment(x):\n    return x+1\n\n# could also be done as a lambda expression\ndf[\"ApplyResult\"] = df.mpg.map(increment)\n\ndf[[\"mpg\", \"ApplyResult\"]].head()\n\n\n\n\n\n\n\n\nmpg\nApplyResult\n\n\n\n\nMazda RX4\n21.0\n22.0\n\n\nMazda RX4 Wag\n21.0\n22.0\n\n\nDatsun 710\n22.8\n23.8\n\n\nHornet 4 Drive\n21.4\n22.4\n\n\nHornet Sportabout\n18.7\n19.7\n\n\n\n\n\n\n\n\n\nApply a function with arguments using kwargs\n\n# some function yadda\ndef incrementBy(x, by):\n    return x + by\n\n# could also be done as a lambda expression\ndf[\"ApplyResult\"] = df.mpg.apply(incrementBy, by = 5)\n\ndf[[\"mpg\", \"ApplyResult\"]].head()\n\n\n\n\n\n\n\n\nmpg\nApplyResult\n\n\n\n\nMazda RX4\n21.0\n26.0\n\n\nMazda RX4 Wag\n21.0\n26.0\n\n\nDatsun 710\n22.8\n27.8\n\n\nHornet 4 Drive\n21.4\n26.4\n\n\nHornet Sportabout\n18.7\n23.7"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#missing-values",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#missing-values",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Missing values",
    "text": "Missing values\nExpanding on an example from here. Pandas uses np.nan instead of NA.\n\ndf2 = pd.DataFrame({\n    \"x\": [1,2,3,4,5,np.nan,7,8,np.nan,10,11,12,np.nan],\n    \"y\": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,13]\n})\ndf2\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1.0\n1.0\n\n\n1\n2.0\nNaN\n\n\n2\n3.0\n3.0\n\n\n3\n4.0\n4.0\n\n\n4\n5.0\n5.0\n\n\n5\nNaN\n6.0\n\n\n6\n7.0\nNaN\n\n\n7\n8.0\n8.0\n\n\n8\nNaN\n9.0\n\n\n9\n10.0\n10.0\n\n\n10\n11.0\n11.0\n\n\n11\n12.0\n12.0\n\n\n12\nNaN\n13.0\n\n\n\n\n\n\n\n\nMissing in the whole df\n\n# no NA the whole thing\ndf2.isnull()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\nFalse\nFalse\n\n\n1\nFalse\nTrue\n\n\n2\nFalse\nFalse\n\n\n3\nFalse\nFalse\n\n\n4\nFalse\nFalse\n\n\n5\nTrue\nFalse\n\n\n6\nFalse\nTrue\n\n\n7\nFalse\nFalse\n\n\n8\nTrue\nFalse\n\n\n9\nFalse\nFalse\n\n\n10\nFalse\nFalse\n\n\n11\nFalse\nFalse\n\n\n12\nTrue\nFalse\n\n\n\n\n\n\n\n\n\nMissing in a single column\n\n# just the x column\ndf2.x.isnull()\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5      True\n6     False\n7     False\n8      True\n9     False\n10    False\n11    False\n12     True\nName: x, dtype: bool\n\n\n\n\nReplace NAs in a column\n\n# use the fillna function \ndf2.x = df2.x.fillna(\"MISSING\")\ndf2\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1.0\n1.0\n\n\n1\n2.0\nNaN\n\n\n2\n3.0\n3.0\n\n\n3\n4.0\n4.0\n\n\n4\n5.0\n5.0\n\n\n5\nMISSING\n6.0\n\n\n6\n7.0\nNaN\n\n\n7\n8.0\n8.0\n\n\n8\nMISSING\n9.0\n\n\n9\n10.0\n10.0\n\n\n10\n11.0\n11.0\n\n\n11\n12.0\n12.0\n\n\n12\nMISSING\n13.0\n\n\n\n\n\n\n\n\n\nReplace the NAs in the full dataset\n\n# files the other even though I didn't specify a column\ndf2.fillna(\"MISSING!!!\")\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1.0\n1.0\n\n\n1\n2.0\nMISSING!!!\n\n\n2\n3.0\n3.0\n\n\n3\n4.0\n4.0\n\n\n4\n5.0\n5.0\n\n\n5\nMISSING\n6.0\n\n\n6\n7.0\nMISSING!!!\n\n\n7\n8.0\n8.0\n\n\n8\nMISSING\n9.0\n\n\n9\n10.0\n10.0\n\n\n10\n11.0\n11.0\n\n\n11\n12.0\n12.0\n\n\n12\nMISSING\n13.0\n\n\n\n\n\n\n\n\n\nDrop NAs\nThe official docs are great on this.\n\n# create another sample df\ndf3 = pd.DataFrame({\n    \"x\": [1,np.nan,3,4,5,np.nan,np.nan,8,np.nan,10,11,12,np.nan],\n    \"y\": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,np.nan],\n    \"z\": [1,2,np.nan,4,5,6,np.nan,8,9,10,11,12,np.nan]\n})\ndf3\n\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\nNaN\nNaN\n2.0\n\n\n2\n3.0\n3.0\nNaN\n\n\n3\n4.0\n4.0\n4.0\n\n\n4\n5.0\n5.0\n5.0\n\n\n5\nNaN\n6.0\n6.0\n\n\n6\nNaN\nNaN\nNaN\n\n\n7\n8.0\n8.0\n8.0\n\n\n8\nNaN\n9.0\n9.0\n\n\n9\n10.0\n10.0\n10.0\n\n\n10\n11.0\n11.0\n11.0\n\n\n11\n12.0\n12.0\n12.0\n\n\n12\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nDrop all across the df\n\n# no NAs period \ndf3.dropna()\n\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n3\n4.0\n4.0\n4.0\n\n\n4\n5.0\n5.0\n5.0\n\n\n7\n8.0\n8.0\n8.0\n\n\n9\n10.0\n10.0\n10.0\n\n\n10\n11.0\n11.0\n11.0\n\n\n11\n12.0\n12.0\n12.0\n\n\n\n\n\n\n\n\n\nDrop if they’re all na in that row\n\n# drop if the whole row is NA\ndf3.dropna(how = \"all\")\n\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\nNaN\nNaN\n2.0\n\n\n2\n3.0\n3.0\nNaN\n\n\n3\n4.0\n4.0\n4.0\n\n\n4\n5.0\n5.0\n5.0\n\n\n5\nNaN\n6.0\n6.0\n\n\n7\n8.0\n8.0\n8.0\n\n\n8\nNaN\n9.0\n9.0\n\n\n9\n10.0\n10.0\n10.0\n\n\n10\n11.0\n11.0\n11.0\n\n\n11\n12.0\n12.0\n12.0\n\n\n\n\n\n\n\n\n\nDrop all in certain columns\nThis is so clutch! Much simpler than filtering across in R (though there might be a cleaner way for that)\n\ndf3.dropna(subset = [\"x\", \"y\"])\n\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n2\n3.0\n3.0\nNaN\n\n\n3\n4.0\n4.0\n4.0\n\n\n4\n5.0\n5.0\n5.0\n\n\n7\n8.0\n8.0\n8.0\n\n\n9\n10.0\n10.0\n10.0\n\n\n10\n11.0\n11.0\n11.0\n\n\n11\n12.0\n12.0\n12.0"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#group-summarize",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#group-summarize",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Group & Summarize",
    "text": "Group & Summarize\n\nGroup by a factor level\n\ndf[[\"mpg\", \"disp\", \"cyl\"]].groupby(by = \"cyl\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f2094b7cb10&gt;\n\n\n\n\nGroupby factor level and get the mean\n\ndf[[\"mpg\", \"disp\", \"cyl\"]].groupby(by = \"cyl\").mean()\n\n\n\n\n\n\n\n\nmpg\ndisp\n\n\ncyl\n\n\n\n\n\n\n4\n26.663636\n105.136364\n\n\n6\n19.742857\n183.314286\n\n\n8\n15.100000\n353.100000"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#melting-a-dataframe",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#melting-a-dataframe",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Melting a DataFrame",
    "text": "Melting a DataFrame\n\n# use the melt function specifying ids and value vars\ndf_long = pd.melt(df, id_vars = \"cyl\", value_vars = [\"mpg\", \"wt\"])\ndf_long\n\n\n\n\n\n\n\n\ncyl\nvariable\nvalue\n\n\n\n\n0\n6\nmpg\n21.000\n\n\n1\n6\nmpg\n21.000\n\n\n2\n4\nmpg\n22.800\n\n\n3\n6\nmpg\n21.400\n\n\n4\n8\nmpg\n18.700\n\n\n...\n...\n...\n...\n\n\n59\n4\nwt\n1.513\n\n\n60\n8\nwt\n3.170\n\n\n61\n6\nwt\n2.770\n\n\n62\n8\nwt\n3.570\n\n\n63\n4\nwt\n2.780\n\n\n\n\n64 rows × 3 columns"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#misc",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#misc",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Misc",
    "text": "Misc\n\nCounts of values (like table in R)\n\n# shows how many there are of each factor\ndf.cyl.value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n\nFormatting long chains\n\n# a little ugly but it works\ndf[[\"mpg\", \"disp\", \"cyl\"]].\\\n    query(\"cyl &gt; 4\").\\\n    groupby(by = \"cyl\").\\\n    mean()\n\n\n\n\n\n\n\n\nmpg\ndisp\n\n\ncyl\n\n\n\n\n\n\n6\n19.742857\n183.314286\n\n\n8\n15.100000\n353.100000\n\n\n\n\n\n\n\n\n\nIterating over a df\nFor mutating the dataframe apply/map is recommended, but I’m showing how to do this for completeness.\n\nfor i, j in df.iterrows():\n    print(df[\"mpg\"][i])\n    # j would give you all the columns with just that row\n\n21.0\n21.0\n22.8\n21.4\n18.7\n18.1\n14.3\n24.4\n22.8\n19.2\n17.8\n16.4\n17.3\n15.2\n10.4\n10.4\n14.7\n32.4\n30.4\n33.9\n21.5\n15.5\n15.2\n13.3\n19.2\n27.3\n26.0\n30.4\n15.8\n19.7\n15.0\n21.4\n\n\nhttps://www.statsmodels.org/stable/examples/notebooks/generated/glm.html https://www.geeksforgeeks.org/linear-regression-in-python-using-statsmodels/ https://datagy.io/pandas-iterate-over-rows/ https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-3/index.html",
    "href": "posts/2022-12-13-from-r-to-python-3/index.html",
    "title": "Stats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport numpy as np\nfrom numpy.random import default_rng\n\nfrom scipy import stats\n\n\ndf = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache = True).data\n\n\n\n\n# get the data as a vector\nv = df.mpg\n\nnp.mean(v)\n# (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html)\nstats.mode(v, keepdims = False)\nnp.median(v)\nnp.var(v)\n\n35.188974609374995\n\n\n\n\n\nAfter trying a few things, it looks like numpy is king here.\n\n\nTo get started, create an RNG object that will make sure everything is initialized correctly.\n\nrng = default_rng()\n\nWe can now go through a list of common distributions, simulate from them, and visualize to make sure we’re on the right track.\n\n\n\n\nn = 1000\n\n# because calling the mean and sd the \"mean\" and \"sd\" would be too obvious\nv_norm = rng.normal(loc = 5, scale = 2.5, size = n)\nsns.kdeplot(v_norm)\n\n&lt;AxesSubplot: ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\n# this makes sense lol\nv_unif = rng.uniform(low = 0, high = 1, size = n)\nsns.kdeplot(v_unif)\n\n&lt;AxesSubplot: ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\n# lamda was too long I guess\nv_poisson = rng.poisson(lam = 1, size = n)\nsns.histplot(v_poisson)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n# bernoulli \n# https://stackoverflow.com/questions/47012474/bernoulli-random-number-generator\nv_bernoulli = rng.binomial(n = 1, p = 0.5, size = n)\nsns.histplot(v_bernoulli)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n# binomial\nv_binomial = rng.binomial(n = 4, p = 0.15, size = n)\nsns.histplot(v_binomial)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n# this makes sense\nv_negative_binomial = rng.negative_binomial(n = 1, p = 0.25, size = n)\nsns.histplot(v_negative_binomial)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\nAnother routine thing that will come up a lot is linear regression. It’s not as obvious as R, but it’s pretty straight-forward.\n\n# you can use formula is you use smf \nlinear_model = smf.ols(formula = 'wt ~ disp + mpg', data = df).fit()\n\nWe can get the results using sume summary() method, and it will look pretty familiar to R users.\n\nlinear_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nwt\nR-squared:\n0.836\n\n\nModel:\nOLS\nAdj. R-squared:\n0.824\n\n\nMethod:\nLeast Squares\nF-statistic:\n73.65\n\n\nDate:\nFri, 27 Jan 2023\nProb (F-statistic):\n4.31e-12\n\n\nTime:\n10:21:49\nLog-Likelihood:\n-15.323\n\n\nNo. Observations:\n32\nAIC:\n36.65\n\n\nDf Residuals:\n29\nBIC:\n41.04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.5627\n0.699\n5.094\n0.000\n2.132\n4.993\n\n\ndisp\n0.0043\n0.001\n3.818\n0.001\n0.002\n0.007\n\n\nmpg\n-0.0663\n0.023\n-2.878\n0.007\n-0.113\n-0.019\n\n\n\n\n\n\nOmnibus:\n0.431\nDurbin-Watson:\n1.028\n\n\nProb(Omnibus):\n0.806\nJarque-Bera (JB):\n0.579\n\n\nSkew:\n0.187\nProb(JB):\n0.749\n\n\nKurtosis:\n2.458\nCond. No.\n2.52e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.52e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nThe next logical step is logistic regression. No surprises here.\n\n# fit the model\nlogistic_model = smf.glm(\"am ~ wt + mpg\", data = df,\n  family = sm.families.Binomial()).fit()\n\nlogistic_model.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nam\nNo. Observations:\n32\n\n\nModel:\nGLM\nDf Residuals:\n29\n\n\nModel Family:\nBinomial\nDf Model:\n2\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-8.5921\n\n\nDate:\nFri, 27 Jan 2023\nDeviance:\n17.184\n\n\nTime:\n10:21:49\nPearson chi2:\n32.7\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.5569\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n25.8866\n12.194\n2.123\n0.034\n1.988\n49.785\n\n\nwt\n-6.4162\n2.547\n-2.519\n0.012\n-11.407\n-1.425\n\n\nmpg\n-0.3242\n0.239\n-1.354\n0.176\n-0.794\n0.145\n\n\n\n\n\n\n\n\nThe next basic anaylsis I wanted to recreate was ANOVA. This is handled nicely by statsmodels, looking more or less like the previous models.\n\n# fit the initial model\nanova_model = smf.ols(\"cyl ~ mpg + disp\", data = df).fit()\n\nanova = sm.stats.anova_lm(anova_model)\nanova\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nmpg\n1.0\n71.801048\n71.801048\n132.393794\n2.496891e-12\n\n\ndisp\n1.0\n11.346399\n11.346399\n20.921600\n8.274021e-05\n\n\nResidual\n29.0\n15.727553\n0.542329\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nAfter an ANOVA, some sort of post-hoc test is usually preformed. This isn’t as obvious as the ones above, requiring us to specify monovariate vectors instead of using a formla.\n\n# specify the groups without the formual\ntukey_results = sm.stats.multicomp.pairwise_tukeyhsd(endog = df[\"mpg\"],\n  groups = df[\"cyl\"])\n\nprint(tukey_results)\n\n Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\n     4      6  -6.9208 0.0003 -10.7693 -3.0722   True\n     4      8 -11.5636    0.0 -14.7708 -8.3565   True\n     6      8  -4.6429 0.0112  -8.3276 -0.9581   True\n-----------------------------------------------------"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-3/index.html#summary-stats",
    "href": "posts/2022-12-13-from-r-to-python-3/index.html#summary-stats",
    "title": "Stats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling",
    "section": "",
    "text": "# get the data as a vector\nv = df.mpg\n\nnp.mean(v)\n# (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html)\nstats.mode(v, keepdims = False)\nnp.median(v)\nnp.var(v)\n\n35.188974609374995"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-3/index.html#simulating-data",
    "href": "posts/2022-12-13-from-r-to-python-3/index.html#simulating-data",
    "title": "Stats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling",
    "section": "",
    "text": "After trying a few things, it looks like numpy is king here.\n\n\nTo get started, create an RNG object that will make sure everything is initialized correctly.\n\nrng = default_rng()\n\nWe can now go through a list of common distributions, simulate from them, and visualize to make sure we’re on the right track.\n\n\n\n\nn = 1000\n\n# because calling the mean and sd the \"mean\" and \"sd\" would be too obvious\nv_norm = rng.normal(loc = 5, scale = 2.5, size = n)\nsns.kdeplot(v_norm)\n\n&lt;AxesSubplot: ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\n# this makes sense lol\nv_unif = rng.uniform(low = 0, high = 1, size = n)\nsns.kdeplot(v_unif)\n\n&lt;AxesSubplot: ylabel='Density'&gt;\n\n\n\n\n\n\n\n\n\n# lamda was too long I guess\nv_poisson = rng.poisson(lam = 1, size = n)\nsns.histplot(v_poisson)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n# bernoulli \n# https://stackoverflow.com/questions/47012474/bernoulli-random-number-generator\nv_bernoulli = rng.binomial(n = 1, p = 0.5, size = n)\nsns.histplot(v_bernoulli)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n# binomial\nv_binomial = rng.binomial(n = 4, p = 0.15, size = n)\nsns.histplot(v_binomial)\n\n&lt;AxesSubplot: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n# this makes sense\nv_negative_binomial = rng.negative_binomial(n = 1, p = 0.25, size = n)\nsns.histplot(v_negative_binomial)\n\n&lt;AxesSubplot: ylabel='Count'&gt;"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-3/index.html#modeling",
    "href": "posts/2022-12-13-from-r-to-python-3/index.html#modeling",
    "title": "Stats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling",
    "section": "",
    "text": "Another routine thing that will come up a lot is linear regression. It’s not as obvious as R, but it’s pretty straight-forward.\n\n# you can use formula is you use smf \nlinear_model = smf.ols(formula = 'wt ~ disp + mpg', data = df).fit()\n\nWe can get the results using sume summary() method, and it will look pretty familiar to R users.\n\nlinear_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nwt\nR-squared:\n0.836\n\n\nModel:\nOLS\nAdj. R-squared:\n0.824\n\n\nMethod:\nLeast Squares\nF-statistic:\n73.65\n\n\nDate:\nFri, 27 Jan 2023\nProb (F-statistic):\n4.31e-12\n\n\nTime:\n10:21:49\nLog-Likelihood:\n-15.323\n\n\nNo. Observations:\n32\nAIC:\n36.65\n\n\nDf Residuals:\n29\nBIC:\n41.04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.5627\n0.699\n5.094\n0.000\n2.132\n4.993\n\n\ndisp\n0.0043\n0.001\n3.818\n0.001\n0.002\n0.007\n\n\nmpg\n-0.0663\n0.023\n-2.878\n0.007\n-0.113\n-0.019\n\n\n\n\n\n\nOmnibus:\n0.431\nDurbin-Watson:\n1.028\n\n\nProb(Omnibus):\n0.806\nJarque-Bera (JB):\n0.579\n\n\nSkew:\n0.187\nProb(JB):\n0.749\n\n\nKurtosis:\n2.458\nCond. No.\n2.52e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.52e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nThe next logical step is logistic regression. No surprises here.\n\n# fit the model\nlogistic_model = smf.glm(\"am ~ wt + mpg\", data = df,\n  family = sm.families.Binomial()).fit()\n\nlogistic_model.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nam\nNo. Observations:\n32\n\n\nModel:\nGLM\nDf Residuals:\n29\n\n\nModel Family:\nBinomial\nDf Model:\n2\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-8.5921\n\n\nDate:\nFri, 27 Jan 2023\nDeviance:\n17.184\n\n\nTime:\n10:21:49\nPearson chi2:\n32.7\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.5569\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n25.8866\n12.194\n2.123\n0.034\n1.988\n49.785\n\n\nwt\n-6.4162\n2.547\n-2.519\n0.012\n-11.407\n-1.425\n\n\nmpg\n-0.3242\n0.239\n-1.354\n0.176\n-0.794\n0.145\n\n\n\n\n\n\n\n\nThe next basic anaylsis I wanted to recreate was ANOVA. This is handled nicely by statsmodels, looking more or less like the previous models.\n\n# fit the initial model\nanova_model = smf.ols(\"cyl ~ mpg + disp\", data = df).fit()\n\nanova = sm.stats.anova_lm(anova_model)\nanova\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nmpg\n1.0\n71.801048\n71.801048\n132.393794\n2.496891e-12\n\n\ndisp\n1.0\n11.346399\n11.346399\n20.921600\n8.274021e-05\n\n\nResidual\n29.0\n15.727553\n0.542329\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nAfter an ANOVA, some sort of post-hoc test is usually preformed. This isn’t as obvious as the ones above, requiring us to specify monovariate vectors instead of using a formla.\n\n# specify the groups without the formual\ntukey_results = sm.stats.multicomp.pairwise_tukeyhsd(endog = df[\"mpg\"],\n  groups = df[\"cyl\"])\n\nprint(tukey_results)\n\n Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\n     4      6  -6.9208 0.0003 -10.7693 -3.0722   True\n     4      8 -11.5636    0.0 -14.7708 -8.3565   True\n     6      8  -4.6429 0.0112  -8.3276 -0.9581   True\n-----------------------------------------------------"
  },
  {
    "objectID": "unpublished-posts/2017-09-26-idioms/index.html",
    "href": "unpublished-posts/2017-09-26-idioms/index.html",
    "title": "Idioms about idioms",
    "section": "",
    "text": "I am sitting in a coffee shop at school, theoretically so I can clear some e-todos and paperwork from my ledger, but I just realized something weird so I have to write a blog post.\nIt all started with me contemplating the term “pythonic”. For those who haven’t heard it before, it’s jargon from the Python programming community to describe something that fits the style and technique of Python well. One might say something “pythonic” is “idiomatic” Python.\nTaking a step back, an idiom in normal language according to The Notorious O.E.D is:\n1. A group of words established by usage as having a meaning not deducible from those of the individual words (e.g., rain cats and dogs, see the light).\n2. A characteristic mode of expression in music or art.\n‘they were both working in a neo-impressionist idiom’\nA classic idiom that is surprisingly well-conserved* all over the damn place is “When pigs fly”. True to definition 1A, this conveys no real information about what you’re talking about except that which a familiar listener has come to understand it does.\nAn example of the 2A sort would be all the references to having “99 Xs but a Y ain’t one” that appear in pop culture since Jay-Z released “99 Problems”, making a phrase Ice-T said in one song part of every day speech . If you want to get old school with it, think “I woke up this morning” (Da dum dum da) in a blues song (Did someone leave the radio on? It’s getting a little ‘NPR’ in here!).\nSometimes someone or something quirky is described as ‘idiosyncratic’, like Bob Dylan’s voice (or maybe Bob Dylan in general). You can follow this train of thinking all the way down to a specific person’s “idiolect” if you want to go as hard as this guy. In either case, we’re talking about traits that belong to a subset, even if that subset is one person.\nIt’s worth noting that something doesn’t have to be idiomatic to be correct, it’s just not what someone who is fully localized might say. On the street corner, this might be the difference between:\n“Pardon me. I require help to find the subway”\nand:\n“Excuse me, which way’s the train?”\nYou know what they both mean and neither are incorrect, but you could probably tell which one of these folks was on vacation. Anyway…\nBack to the coding example, something pythonic is python written the way a ‘fluent’ user would write it, and that embodies the traits Python is designed to facilitate. This often means it is concise, simple, and easy to read. It might even be an expression that is so common nobody knows who wrote it first, just like with human languages.\nExamples, you say? Take a look at the following code:\n# print out each letter of a string\nname = \"Guido\"\nnumber = 0\nwhile number &gt; len(name):\n    print(name[number])\n    number = number + 1\n\nThis code achieves what it set out to do; there is nothing really ‘wrong’ with it (you’d still know they guy needs to get to the train, if you will). We all write stuff like this once in a while, and if we’re just trying to get a quick idea out for a prototype, it’s probably not worth slowing down to mess with. But it’s decidedly not pythonic.\nFor starters, Python would be more likely to sidestep the counter all together. The ‘While’ loop could easily be the more widely understood ‘For’, and the incrementer in line X could be written more concisely . Let’s give it a try.\nname = “Guido”\nfor letter in name:\n    print (letter)\n\nBoom. We’re down from 5 lines to 3. You could get down to 2 if you wanted to, (put the print() statement right after the : in the line above it) but this is the more typical.\nThe ‘for’ takes care of the iteration for us, and we take advantage of a default variable we can define as we see fit, ‘letter’. This would run with any string there, but why not take advantage of the human friendly syntax opportunity? It’s worth noting that ‘i’, is often used and is idiomatic in programming in general (the incrementer, in this case, is replaced by the for-loop but would be more idiomatically written “number + =1”).\nThat gets the point across, but the example is simple and easy to fix. Let’s look at something a little more true to life. Try this on for size:\n# filter out names that are 3 characters or less.\nnames = [\"Ada\", \"Mary\", \"Bev\", \"Margaret\", \"Rosalind\"]\nshort_names = []\nfor name in names:\n    if len(name) &lt;= 3:\n        short_names.append(name)\n\nThis snippet sets out to do something slightly more complicated even though it’s also 5 lines long: there is a condition of the list containing unprocessed entries and a check of length. The reason I wrote it this way is because it’s less obvious how it could be made more pythonic, but it can be done, and using an especially python-y move: a list comprehension.\nObserve:\nnames = [\"Ada\", \"Mary\", \"Bev\", \"Margaret\", \"Rosalind\"]\nshort_names = [name for name in names if len(name) &lt;= 3]\n\nWhoa, now! Is it all even there?\nThese can seem a little intimidating at first, but they are super handy**. If you just read through slowly, all of the same idea are expressed, just more smoothly. This is probably the ‘expression’ (idiom) that more experienced users would use, so it is worth noting.\nThe ‘=’ and the [] define the new list as a list, and the condition is basically typed in (somewhat awkward) English and that’s it.\nA skeptical reader might be wondering about the philosophy behind this. The Zen of Python states that it is better to be explicit that implicit, but this feels a bit like a shortcut, no? At least, that’s a question that came to my mind; maybe it is glaringly obviously to everyone else on the planet.\nHere’s the thing though: all of the logical components of the first code are carried over into the second. Python isn’t trying to ‘guess’ what we mean from context (nothing is implicit). The only thing that is different is how the command is structured, and that structure is more succinct. I like to read them in English, as well as one can, to illustrate the point.\nThe first one would look something like this:\n“Here is a list of names called names”\n“Create an empty list name short_names”\n“…for each name in names…”\n“…if the length is less than or equal to 3..”\n“…add that name to the short name list.”\nThe second:\n“Here is a list of names called names”\n“Create a list called short names that has each name from names if its length is three or less”\nThere are no important words in the first example that the second doesn’t have. Because all of the same ideas are there, it’s just shorter, I say it fits the philosophy.\nIdioms pop up all over the place in code, in all sorts of languages, and have done so since well before Python was invented. I’m willing to bet most programmers have seen this, or something like it, even if they never wrote a line of C code in their life.\nint main() { for (int i = 0; i &lt; 10; i++) {     printf(“The number is %d”, i); } return 0;\nThat ‘for (int i = 0; i &lt; 10; i++)’ expression, in one form or another, appears EVERYWHERE. C, C++, C#, Java, Perl, Groovy, JavaScript - it’s there. Hell, even Golang, a mere ten years old at the time of this writing and designed to balance old school power with new school syntax has a modified version of that old chestnut***.\nIn closing I share the random thought that started this rant: Saying something is pythonic is basically using an idiom to describe how idiomatic something is. Huh.\nI hope this has been illuminating and/or interesting, and thanks for reading!\nFootnotes:\n \n*The expressions ‘well conserved’ is used all the time in evolutionary biology to describe a protein that varies vary little from species to species. Idiom!\n \n**For example, if you needed a quick Python program to describe Jay-Z’s problems, you can say a lot in one line, much like the man himself:\ncurrent_probs = [prob for prob in old_probs if prob != “B*tch”]\n \n***this is an idiom."
  },
  {
    "objectID": "about.html#research-focus",
    "href": "about.html#research-focus",
    "title": "[ What I Work On ]",
    "section": "",
    "text": "I’m interested in statistical methods to reduce disparities and inefficiencies in healthcare. The current form this takes is biostatistical and reinforcement learning approaches to understanding and improving health outcomes in opioid use disorder.\nBefore starting my PhD, this took the form of trying to identify biomarkers for Alzheimer’s Dementia, Heart Failure, and COVID-19. The overarching goal was to find reliable markers of disease or to compare existing methods to determine which diagnostic approach was optimal in terms of expense or invasivness."
  },
  {
    "objectID": "about.html#previous-work",
    "href": "about.html#previous-work",
    "title": "[ What I Work On ]",
    "section": "Previous Work",
    "text": "Previous Work\nBefore starting my PhD, my work fell roughly into five (often overlapping) spheres.\n\nGeneral Research Programming\nI worked on  apps  that complement  publications  to make their content easier to use and increase engagement.\n\n\nOriginal Software\nI worked on creating new software for unmet needs. One example is our  Bayesian COVID Diagnostics app. This serves to complement our  manuscript  on lateral flow assays for SARS-CoV-2 antibodies as well as providing the user with an easy-to-use tool to avoid the pitfalls of black-and-white thinking about diagnostics and counter-intuitive conditional probability. Another is  ContrApption, which is a JavaScript-powered R package that allows to user to create an interactive widget for exploring the results of differential expression experiments or any dataset with similar structure.\n\n\nStatistics for biomarkers\nI worked on statistical analysis for biomarkers of potential use in Alzheimer’s Dementia and Neurological symptoms of COVID-19. Publications related to this can be found on my  Google Scholar  page.\n\n\nStatistics and genomics for basic science\nFor an example of this, check out our  manuscript  on stress-dependent signatures of cellular and extracellular tRNA-derived small RNAs (tDRs). We’re interested in this from a basic science perspective as well as a potential avenue of biomarkers for diseases.\n\n\n“Hacktivism”\nI had the privilege of contributing to a  web tool  that allows users to interact with a dataset that ranks the states in the US based on how well they support the financial well-being of survivors of domestic abuse. I did this as a member of  RagTag  in support of  FREEFROM."
  }
]