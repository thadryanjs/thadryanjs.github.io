[
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "",
    "text": "update: 12-17-22 - added sorting and table."
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#basics",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#basics",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Basics",
    "text": "Basics\nHow to get basic information about the df.\n\nDimensions of a df\nThis is like dim in R.\n\n# rows by columns \ndf.shape\n\n(32, 11)\n\n\n\n\nNumber of rows\nThis is like nrow in R.\n\n# just the rows\nlen(df)\n\n32\n\n\n\n\nNumber of columns\nThis is like ncol in R.\n\n# just the columns\nlen(df.columns)\n\n11\n\n\nYou could also just index the results of df.shape.\n\n\nData types\n\n# a summary of types by column\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nIndex: 32 entries, Mazda RX4 to Volvo 142E\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   mpg     32 non-null     float64\n 1   cyl     32 non-null     int64  \n 2   disp    32 non-null     float64\n 3   hp      32 non-null     int64  \n 4   drat    32 non-null     float64\n 5   wt      32 non-null     float64\n 6   qsec    32 non-null     float64\n 7   vs      32 non-null     int64  \n 8   am      32 non-null     int64  \n 9   gear    32 non-null     int64  \n 10  carb    32 non-null     int64  \ndtypes: float64(5), int64(6)\nmemory usage: 3.0+ KB\n\n\n\n\nSummary\n\n# summary of distributional information\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      count\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.000000\n      32.0000\n    \n    \n      mean\n      20.090625\n      6.187500\n      230.721875\n      146.687500\n      3.596563\n      3.217250\n      17.848750\n      0.437500\n      0.406250\n      3.687500\n      2.8125\n    \n    \n      std\n      6.026948\n      1.785922\n      123.938694\n      68.562868\n      0.534679\n      0.978457\n      1.786943\n      0.504016\n      0.498991\n      0.737804\n      1.6152\n    \n    \n      min\n      10.400000\n      4.000000\n      71.100000\n      52.000000\n      2.760000\n      1.513000\n      14.500000\n      0.000000\n      0.000000\n      3.000000\n      1.0000\n    \n    \n      25%\n      15.425000\n      4.000000\n      120.825000\n      96.500000\n      3.080000\n      2.581250\n      16.892500\n      0.000000\n      0.000000\n      3.000000\n      2.0000\n    \n    \n      50%\n      19.200000\n      6.000000\n      196.300000\n      123.000000\n      3.695000\n      3.325000\n      17.710000\n      0.000000\n      0.000000\n      4.000000\n      2.0000\n    \n    \n      75%\n      22.800000\n      8.000000\n      326.000000\n      180.000000\n      3.920000\n      3.610000\n      18.900000\n      1.000000\n      1.000000\n      4.000000\n      4.0000\n    \n    \n      max\n      33.900000\n      8.000000\n      472.000000\n      335.000000\n      4.930000\n      5.424000\n      22.900000\n      1.000000\n      1.000000\n      5.000000\n      8.0000"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#selections",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#selections",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Selections",
    "text": "Selections\nHow to specify which column you want.\n\nSelect a column\nNote that I’m using head() to get the first few rows to keep the output small. It’s not part of the selection of columns.\n\n# access a single column like an object property\ndf.mpg.head()\n\nMazda RX4            21.0\nMazda RX4 Wag        21.0\nDatsun 710           22.8\nHornet 4 Drive       21.4\nHornet Sportabout    18.7\nName: mpg, dtype: float64\n\n\nOr…\n\n# select a column (as a series)\ndf[\"mpg\"].head()\n\nMazda RX4            21.0\nMazda RX4 Wag        21.0\nDatsun 710           22.8\nHornet 4 Drive       21.4\nHornet Sportabout    18.7\nName: mpg, dtype: float64\n\n\n\n# select column (as a dataframe)\ndf[[\"mpg\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n    \n    \n      Mazda RX4 Wag\n      21.0\n    \n    \n      Datsun 710\n      22.8\n    \n    \n      Hornet 4 Drive\n      21.4\n    \n    \n      Hornet Sportabout\n      18.7\n    \n  \n\n\n\n\n\n\nSelect several columns by string name\n\n# select a column by several names\ndf[[\"mpg\", \"wt\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      wt\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      2.620\n    \n    \n      Mazda RX4 Wag\n      21.0\n      2.875\n    \n    \n      Datsun 710\n      22.8\n      2.320\n    \n    \n      Hornet 4 Drive\n      21.4\n      3.215\n    \n    \n      Hornet Sportabout\n      18.7\n      3.440\n    \n  \n\n\n\n\n\n\nSelect from one column through another with names\n\n# select a range by name of start and end \"mph through hp\"\ndf.loc[:, \"mpg\":\"hp\"].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n    \n    \n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n    \n    \n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n    \n  \n\n\n\n\n\n\nSelect arbitrarily\nHow to get based on a condition of your choosing. We might want to get only columns that start with “d”, for example.\n\n# get a list\nstarts_with_d = [i for i in list(df.columns) if i.startswith(\"d\")]\n\n# pass the list to pandas\ndf[starts_with_d].head()\n\n\n\n\n\n  \n    \n      \n      disp\n      drat\n    \n  \n  \n    \n      Mazda RX4\n      160.0\n      3.90\n    \n    \n      Mazda RX4 Wag\n      160.0\n      3.90\n    \n    \n      Datsun 710\n      108.0\n      3.85\n    \n    \n      Hornet 4 Drive\n      258.0\n      3.08\n    \n    \n      Hornet Sportabout\n      360.0\n      3.15"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#filtering",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#filtering",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Filtering",
    "text": "Filtering\nOperations where we’d used dplyr in R. In Pandas, filtering refers to operations by index, so what I’m thinking of is more like “querying” in Pandas terms.\n\nSingle condition\nPass logic in strings to the query method.\n\ndf.query(\"cyl == 6\")\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      Valiant\n      18.1\n      6\n      225.0\n      105\n      2.76\n      3.460\n      20.22\n      1\n      0\n      3\n      1\n    \n    \n      Merc 280\n      19.2\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.30\n      1\n      0\n      4\n      4\n    \n    \n      Merc 280C\n      17.8\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.90\n      1\n      0\n      4\n      4\n    \n    \n      Ferrari Dino\n      19.7\n      6\n      145.0\n      175\n      3.62\n      2.770\n      15.50\n      0\n      1\n      5\n      6\n    \n  \n\n\n\n\n\n\nMultiple conditions\n\ndf.query(\"cyl == 6 & hp > 105\")\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      Merc 280\n      19.2\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.30\n      1\n      0\n      4\n      4\n    \n    \n      Merc 280C\n      17.8\n      6\n      167.6\n      123\n      3.92\n      3.440\n      18.90\n      1\n      0\n      4\n      4\n    \n    \n      Ferrari Dino\n      19.7\n      6\n      145.0\n      175\n      3.62\n      2.770\n      15.50\n      0\n      1\n      5\n      6"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#sorting",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#sorting",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Sorting",
    "text": "Sorting\n\nAscending\n\ndf.mpg.sort_values().head(10)\n\nLincoln Continental    10.4\nCadillac Fleetwood     10.4\nCamaro Z28             13.3\nDuster 360             14.3\nChrysler Imperial      14.7\nMaserati Bora          15.0\nMerc 450SLC            15.2\nAMC Javelin            15.2\nDodge Challenger       15.5\nFord Pantera L         15.8\nName: mpg, dtype: float64\n\n\n\n\nDescending\nThere is no ‘descending’ per se, just set ascending to False.\n\ndf.mpg.sort_values(ascending = False).head(10)\n\nToyota Corolla    33.9\nFiat 128          32.4\nLotus Europa      30.4\nHonda Civic       30.4\nFiat X1-9         27.3\nPorsche 914-2     26.0\nMerc 240D         24.4\nDatsun 710        22.8\nMerc 230          22.8\nToyota Corona     21.5\nName: mpg, dtype: float64"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#applying-functions",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#applying-functions",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Applying functions",
    "text": "Applying functions\n\nMap a function with no arguments to a single column\n\n# some function to apply\ndef increment(x):\n    return x+1\n\n# could also be done as a lambda expression\ndf[\"ApplyResult\"] = df.mpg.map(increment)\n\ndf[[\"mpg\", \"ApplyResult\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      ApplyResult\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      22.0\n    \n    \n      Mazda RX4 Wag\n      21.0\n      22.0\n    \n    \n      Datsun 710\n      22.8\n      23.8\n    \n    \n      Hornet 4 Drive\n      21.4\n      22.4\n    \n    \n      Hornet Sportabout\n      18.7\n      19.7\n    \n  \n\n\n\n\n\n\nApply a function with arguments using kwargs\n\n# some function yadda\ndef incrementBy(x, by):\n    return x + by\n\n# could also be done as a lambda expression\ndf[\"ApplyResult\"] = df.mpg.apply(incrementBy, by = 5)\n\ndf[[\"mpg\", \"ApplyResult\"]].head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      ApplyResult\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      26.0\n    \n    \n      Mazda RX4 Wag\n      21.0\n      26.0\n    \n    \n      Datsun 710\n      22.8\n      27.8\n    \n    \n      Hornet 4 Drive\n      21.4\n      26.4\n    \n    \n      Hornet Sportabout\n      18.7\n      23.7"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#missing-values",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#missing-values",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Missing values",
    "text": "Missing values\nExpanding on an example from here. Pandas uses np.nan instead of NA.\n\ndf2 = pd.DataFrame({\n    \"x\": [1,2,3,4,5,np.nan,7,8,np.nan,10,11,12,np.nan],\n    \"y\": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,13]\n})\ndf2\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n    \n    \n      1\n      2.0\n      NaN\n    \n    \n      2\n      3.0\n      3.0\n    \n    \n      3\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n    \n    \n      5\n      NaN\n      6.0\n    \n    \n      6\n      7.0\n      NaN\n    \n    \n      7\n      8.0\n      8.0\n    \n    \n      8\n      NaN\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n    \n    \n      12\n      NaN\n      13.0\n    \n  \n\n\n\n\n\nMissing in the whole df\n\n# no NA the whole thing\ndf2.isnull()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      False\n      False\n    \n    \n      1\n      False\n      True\n    \n    \n      2\n      False\n      False\n    \n    \n      3\n      False\n      False\n    \n    \n      4\n      False\n      False\n    \n    \n      5\n      True\n      False\n    \n    \n      6\n      False\n      True\n    \n    \n      7\n      False\n      False\n    \n    \n      8\n      True\n      False\n    \n    \n      9\n      False\n      False\n    \n    \n      10\n      False\n      False\n    \n    \n      11\n      False\n      False\n    \n    \n      12\n      True\n      False\n    \n  \n\n\n\n\n\n\nMissing in a single column\n\n# just the x column\ndf2.x.isnull()\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5      True\n6     False\n7     False\n8      True\n9     False\n10    False\n11    False\n12     True\nName: x, dtype: bool\n\n\n\n\nReplace NAs in a column\n\n# use the fillna function \ndf2.x = df2.x.fillna(\"MISSING\")\ndf2\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n    \n    \n      1\n      2.0\n      NaN\n    \n    \n      2\n      3.0\n      3.0\n    \n    \n      3\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n    \n    \n      5\n      MISSING\n      6.0\n    \n    \n      6\n      7.0\n      NaN\n    \n    \n      7\n      8.0\n      8.0\n    \n    \n      8\n      MISSING\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n    \n    \n      12\n      MISSING\n      13.0\n    \n  \n\n\n\n\n\n\nReplace the NAs in the full dataset\n\n# files the other even though I didn't specify a column\ndf2.fillna(\"MISSING!!!\")\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n    \n    \n      1\n      2.0\n      MISSING!!!\n    \n    \n      2\n      3.0\n      3.0\n    \n    \n      3\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n    \n    \n      5\n      MISSING\n      6.0\n    \n    \n      6\n      7.0\n      MISSING!!!\n    \n    \n      7\n      8.0\n      8.0\n    \n    \n      8\n      MISSING\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n    \n    \n      12\n      MISSING\n      13.0\n    \n  \n\n\n\n\n\n\nDrop NAs\nThe official docs are great on this.\n\n# create another sample df\ndf3 = pd.DataFrame({\n    \"x\": [1,np.nan,3,4,5,np.nan,np.nan,8,np.nan,10,11,12,np.nan],\n    \"y\": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,np.nan],\n    \"z\": [1,2,np.nan,4,5,6,np.nan,8,9,10,11,12,np.nan]\n})\ndf3\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      NaN\n      NaN\n      2.0\n    \n    \n      2\n      3.0\n      3.0\n      NaN\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      5\n      NaN\n      6.0\n      6.0\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      8\n      NaN\n      9.0\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0\n    \n    \n      12\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nDrop all across the df\n\n# no NAs period \ndf3.dropna()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0\n    \n  \n\n\n\n\n\n\nDrop if they’re all na in that row\n\n# drop if the whole row is NA\ndf3.dropna(how = \"all\")\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      1\n      NaN\n      NaN\n      2.0\n    \n    \n      2\n      3.0\n      3.0\n      NaN\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      5\n      NaN\n      6.0\n      6.0\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      8\n      NaN\n      9.0\n      9.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0\n    \n  \n\n\n\n\n\n\nDrop all in certain columns\nThis is so clutch! Much simpler than filtering across in R (though there might be a cleaner way for that)\n\ndf3.dropna(subset = [\"x\", \"y\"])\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      1.0\n      1.0\n      1.0\n    \n    \n      2\n      3.0\n      3.0\n      NaN\n    \n    \n      3\n      4.0\n      4.0\n      4.0\n    \n    \n      4\n      5.0\n      5.0\n      5.0\n    \n    \n      7\n      8.0\n      8.0\n      8.0\n    \n    \n      9\n      10.0\n      10.0\n      10.0\n    \n    \n      10\n      11.0\n      11.0\n      11.0\n    \n    \n      11\n      12.0\n      12.0\n      12.0"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#group-summarize",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#group-summarize",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Group & Summarize",
    "text": "Group & Summarize\n\nGroup by a factor level\n\ndf[[\"mpg\", \"disp\", \"cyl\"]].groupby(by = \"cyl\")\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f2094b7cb10>\n\n\n\n\nGroupby factor level and get the mean\n\ndf[[\"mpg\", \"disp\", \"cyl\"]].groupby(by = \"cyl\").mean()\n\n\n\n\n\n  \n    \n      \n      mpg\n      disp\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      4\n      26.663636\n      105.136364\n    \n    \n      6\n      19.742857\n      183.314286\n    \n    \n      8\n      15.100000\n      353.100000"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#melting-a-dataframe",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#melting-a-dataframe",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Melting a DataFrame",
    "text": "Melting a DataFrame\n\n# use the melt function specifying ids and value vars\ndf_long = pd.melt(df, id_vars = \"cyl\", value_vars = [\"mpg\", \"wt\"])\ndf_long\n\n\n\n\n\n  \n    \n      \n      cyl\n      variable\n      value\n    \n  \n  \n    \n      0\n      6\n      mpg\n      21.000\n    \n    \n      1\n      6\n      mpg\n      21.000\n    \n    \n      2\n      4\n      mpg\n      22.800\n    \n    \n      3\n      6\n      mpg\n      21.400\n    \n    \n      4\n      8\n      mpg\n      18.700\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      59\n      4\n      wt\n      1.513\n    \n    \n      60\n      8\n      wt\n      3.170\n    \n    \n      61\n      6\n      wt\n      2.770\n    \n    \n      62\n      8\n      wt\n      3.570\n    \n    \n      63\n      4\n      wt\n      2.780\n    \n  \n\n64 rows × 3 columns"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-1/index.html#misc",
    "href": "posts/2022-12-13-from-r-to-python-1/index.html#misc",
    "title": "Stats Python in a Hurry Part 1: Data Wrangling",
    "section": "Misc",
    "text": "Misc\n\nCounts of values (like table in R)\n\n# shows how many there are of each factor\ndf.cyl.value_counts()\n\n8    14\n4    11\n6     7\nName: cyl, dtype: int64\n\n\n\n\nFormatting long chains\n\n# a little ugly but it works\ndf[[\"mpg\", \"disp\", \"cyl\"]].\\\n    query(\"cyl > 4\").\\\n    groupby(by = \"cyl\").\\\n    mean()\n\n\n\n\n\n  \n    \n      \n      mpg\n      disp\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      6\n      19.742857\n      183.314286\n    \n    \n      8\n      15.100000\n      353.100000\n    \n  \n\n\n\n\n\n\nIterating over a df\nFor mutating the dataframe apply/map is recommended, but I’m showing how to do this for completeness.\n\nfor i, j in df.iterrows():\n    print(df[\"mpg\"][i])\n    # j would give you all the columns with just that row\n\n21.0\n21.0\n22.8\n21.4\n18.7\n18.1\n14.3\n24.4\n22.8\n19.2\n17.8\n16.4\n17.3\n15.2\n10.4\n10.4\n14.7\n32.4\n30.4\n33.9\n21.5\n15.5\n15.2\n13.3\n19.2\n27.3\n26.0\n30.4\n15.8\n19.7\n15.0\n21.4\n\n\nhttps://www.statsmodels.org/stable/examples/notebooks/generated/glm.html https://www.geeksforgeeks.org/linear-regression-in-python-using-statsmodels/ https://datagy.io/pandas-iterate-over-rows/ https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-2/index.html",
    "href": "posts/2022-12-13-from-r-to-python-2/index.html",
    "title": "Stats Python in a Hurry Part 2: Visualization",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache = True).data\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      Mazda RX4\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      Mazda RX4 Wag\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      Datsun 710\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      Hornet 4 Drive\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      Hornet Sportabout\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\nSeaborn seems to be the most efficient way to get decent looking exploratory plots in a hurry.\n\n\n\nsns.lineplot(df, x = \"mpg\", y = \"disp\")\n\n<AxesSubplot: xlabel='mpg', ylabel='disp'>\n\n\n\n\n\n\n\nUse the hue argument to break out factors into separate lines.\n\nsns.lineplot(df, x = \"mpg\", y = \"disp\", hue = \"am\")\n\n<AxesSubplot: xlabel='mpg', ylabel='disp'>\n\n\n\n\n\n\n\n\nMean/CI are automatic if you melt the df.\n\ndf_long = pd.melt(df, id_vars = \"cyl\", value_vars = \"mpg\")\n\ndf_long\n\n\n\n\n\n  \n    \n      \n      cyl\n      variable\n      value\n    \n  \n  \n    \n      0\n      6\n      mpg\n      21.0\n    \n    \n      1\n      6\n      mpg\n      21.0\n    \n    \n      2\n      4\n      mpg\n      22.8\n    \n    \n      3\n      6\n      mpg\n      21.4\n    \n    \n      4\n      8\n      mpg\n      18.7\n    \n    \n      5\n      6\n      mpg\n      18.1\n    \n    \n      6\n      8\n      mpg\n      14.3\n    \n    \n      7\n      4\n      mpg\n      24.4\n    \n    \n      8\n      4\n      mpg\n      22.8\n    \n    \n      9\n      6\n      mpg\n      19.2\n    \n    \n      10\n      6\n      mpg\n      17.8\n    \n    \n      11\n      8\n      mpg\n      16.4\n    \n    \n      12\n      8\n      mpg\n      17.3\n    \n    \n      13\n      8\n      mpg\n      15.2\n    \n    \n      14\n      8\n      mpg\n      10.4\n    \n    \n      15\n      8\n      mpg\n      10.4\n    \n    \n      16\n      8\n      mpg\n      14.7\n    \n    \n      17\n      4\n      mpg\n      32.4\n    \n    \n      18\n      4\n      mpg\n      30.4\n    \n    \n      19\n      4\n      mpg\n      33.9\n    \n    \n      20\n      4\n      mpg\n      21.5\n    \n    \n      21\n      8\n      mpg\n      15.5\n    \n    \n      22\n      8\n      mpg\n      15.2\n    \n    \n      23\n      8\n      mpg\n      13.3\n    \n    \n      24\n      8\n      mpg\n      19.2\n    \n    \n      25\n      4\n      mpg\n      27.3\n    \n    \n      26\n      4\n      mpg\n      26.0\n    \n    \n      27\n      4\n      mpg\n      30.4\n    \n    \n      28\n      8\n      mpg\n      15.8\n    \n    \n      29\n      6\n      mpg\n      19.7\n    \n    \n      30\n      8\n      mpg\n      15.0\n    \n    \n      31\n      4\n      mpg\n      21.4\n    \n  \n\n\n\n\n\nsns.lineplot(df_long, x = \"cyl\", y = \"value\")\n\n<AxesSubplot: xlabel='cyl', ylabel='value'>\n\n\n\n\n\nOr, use lmplot to git a linear model like you’d get with geom_smooth(method = lm).\n\nsns.lmplot(df, x = \"mpg\", y = \"disp\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f5d567270d0>\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(df, x = \"cyl\", y = \"mpg\")\n\n<AxesSubplot: xlabel='cyl', ylabel='mpg'>\n\n\n\n\n\n\n\n\n\nsns.histplot(df, x = \"wt\")\n\n<AxesSubplot: xlabel='wt', ylabel='Count'>\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df, x = \"disp\")\n\n<AxesSubplot: xlabel='disp', ylabel='Density'>\n\n\n\n\n\n\n\n\n\nsns.heatmap(df[[\"disp\", \"hp\"]])\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df.loc[:, [\"mpg\", \"wt\"]])\n\n<AxesSubplot: ylabel='Density'>\n\n\n\n\n\n\n\n\n\nsns.lineplot(df.loc[:, [\"mpg\", \"wt\"]])\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\n\n\n# create three empty spots\ngrid = sns.FacetGrid(data = df, col = \"cyl\", col_wrap=2)\n\n# puts a historgram on each of them\ngrid.map(sns.histplot, \"wt\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f5d55cf2590>\n\n\n\n\n\nThe initial display is automatic. If you want to show the same plot again, access the figure property of the object.\n\n# just typing it out gives the object metadata\ngrid\n\n<seaborn.axisgrid.FacetGrid at 0x7f5d55cf2590>\n\n\n\ngrid.figure\n\n\n\n\n\n\n\n\n\nThe plot we made of weight and mpg had mostly unusable x tick labels. Let’s revist it.\n\np_line = sns.lineplot(df.loc[:, [\"mpg\", \"wt\"]])\np_line.figure \n\n\n\n\n\n\n\nThe syntax is a little awkward. Essentially there is a set method, and you use a get method to retrieve the labels to pass into it, specifying a rotation.\n\n# set what you get from the get method v--here\np_line.set_xticklabels(p_line.get_xticklabels(), rotation = 45)\np_line.figure\n\n/tmp/ipykernel_9407/667538073.py:2: UserWarning: FixedFormatter should only be used together with FixedLocator\n  p_line.set_xticklabels(p_line.get_xticklabels(), rotation = 45)\n\n\n\n\n\nThey still conflict a little. We can make them a little smaller overall. The technique is the same, just setting a different property.\n\np_line.set_xticklabels(p_line.get_xticklabels(), size = 5)\np_line.figure\n\n/tmp/ipykernel_9407/3729791072.py:1: UserWarning: FixedFormatter should only be used together with FixedLocator\n  p_line.set_xticklabels(p_line.get_xticklabels(), size = 5)\n\n\n\n\n\n\n\n\n\np_line.set(title = \"0_o\")\n\np_line.figure\n\n\n\n\n\n\n\n\n\nSeaborn lets you preview color palettes by calling them as a function argument to sns.color_palette.\n\nsns.color_palette(\"dark\")\n\n\n\n\nThe plotting functions will then have arguments for color scheming:\n\np_box = sns.boxplot(df, x = \"cyl\", y = \"mpg\", palette = \"dark\")\n\n\n\n\n\n\n\n\nsns.color_palette(\"mako\", as_cmap = True)\n\nmako  underbad over \n\n\n\nsns.heatmap(df[[\"disp\", \"hp\"]], cmap = \"mako\")\n\n<AxesSubplot: >"
  },
  {
    "objectID": "posts/2022-12-13-from-r-to-python-3/index.html",
    "href": "posts/2022-12-13-from-r-to-python-3/index.html",
    "title": "Stats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport numpy as np\nfrom numpy.random import default_rng\n\nfrom scipy import stats\n\n\ndf = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache = True).data\n\n\n\n\n# get the data as a vector\nv = df.mpg\n\nnp.mean(v)\n# (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html)\nstats.mode(v, keepdims = False)\nnp.median(v)\nnp.var(v)\n\n35.188974609374995\n\n\n\n\n\nAfter trying a few things, it looks like numpy is king here.\n\n\nTo get started, create an RNG object that will make sure everything is initialized correctly.\n\nrng = default_rng()\n\nWe can now go through a list of common distributions, simulate from them, and visualize to make sure we’re on the right track.\n\n\n\n\nn = 1000\n\n# because calling the mean and sd the \"mean\" and \"sd\" would be too obvious\nv_norm = rng.normal(loc = 5, scale = 2.5, size = n)\nsns.kdeplot(v_norm)\n\n<AxesSubplot: ylabel='Density'>\n\n\n\n\n\n\n\n\n\n# this makes sense lol\nv_unif = rng.uniform(low = 0, high = 1, size = n)\nsns.kdeplot(v_unif)\n\n<AxesSubplot: ylabel='Density'>\n\n\n\n\n\n\n\n\n\n# lamda was too long I guess\nv_poisson = rng.poisson(lam = 1, size = n)\nsns.histplot(v_poisson)\n\n<AxesSubplot: ylabel='Count'>\n\n\n\n\n\n\n\n\n\n# bernoulli \n# https://stackoverflow.com/questions/47012474/bernoulli-random-number-generator\nv_bernoulli = rng.binomial(n = 1, p = 0.5, size = n)\nsns.histplot(v_bernoulli)\n\n<AxesSubplot: ylabel='Count'>\n\n\n\n\n\n\n\n\n\n# binomial\nv_binomial = rng.binomial(n = 4, p = 0.15, size = n)\nsns.histplot(v_binomial)\n\n<AxesSubplot: ylabel='Count'>\n\n\n\n\n\n\n\n\n\n# this makes sense\nv_negative_binomial = rng.negative_binomial(n = 1, p = 0.25, size = n)\nsns.histplot(v_negative_binomial)\n\n<AxesSubplot: ylabel='Count'>\n\n\n\n\n\n\n\n\n\n\n\nAnother routine thing that will come up a lot is linear regression. It’s not as obvious as R, but it’s pretty straight-forward.\n\n# you can use formula is you use smf \nlinear_model = smf.ols(formula = 'wt ~ disp + mpg', data = df).fit()\n\nWe can get the results using sume summary() method, and it will look pretty familiar to R users.\n\nlinear_model.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:           wt          R-squared:             0.836\n\n\n  Model:                   OLS         Adj. R-squared:        0.824\n\n\n  Method:             Least Squares    F-statistic:           73.65\n\n\n  Date:             Fri, 27 Jan 2023   Prob (F-statistic): 4.31e-12\n\n\n  Time:                 10:21:49       Log-Likelihood:      -15.323\n\n\n  No. Observations:          32        AIC:                   36.65\n\n\n  Df Residuals:              29        BIC:                   41.04\n\n\n  Df Model:                   2                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     3.5627     0.699     5.094  0.000     2.132     4.993\n\n\n  disp          0.0043     0.001     3.818  0.001     0.002     0.007\n\n\n  mpg          -0.0663     0.023    -2.878  0.007    -0.113    -0.019\n\n\n\n\n  Omnibus:        0.431   Durbin-Watson:         1.028\n\n\n  Prob(Omnibus):  0.806   Jarque-Bera (JB):      0.579\n\n\n  Skew:           0.187   Prob(JB):              0.749\n\n\n  Kurtosis:       2.458   Cond. No.           2.52e+03\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.52e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\nThe next logical step is logistic regression. No surprises here.\n\n# fit the model\nlogistic_model = smf.glm(\"am ~ wt + mpg\", data = df,\n  family = sm.families.Binomial()).fit()\n\nlogistic_model.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          am          No. Observations:        32 \n\n\n  Model:                  GLM         Df Residuals:            29 \n\n\n  Model Family:        Binomial       Df Model:                 2 \n\n\n  Link Function:         Logit        Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -8.5921\n\n\n  Date:            Fri, 27 Jan 2023   Deviance:             17.184\n\n\n  Time:                10:21:49       Pearson chi2:          32.7 \n\n\n  No. Iterations:          7          Pseudo R-squ. (CS):  0.5569 \n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    25.8866    12.194     2.123  0.034     1.988    49.785\n\n\n  wt           -6.4162     2.547    -2.519  0.012   -11.407    -1.425\n\n\n  mpg          -0.3242     0.239    -1.354  0.176    -0.794     0.145\n\n\n\n\n\n\n\nThe next basic anaylsis I wanted to recreate was ANOVA. This is handled nicely by statsmodels, looking more or less like the previous models.\n\n# fit the initial model\nanova_model = smf.ols(\"cyl ~ mpg + disp\", data = df).fit()\n\nanova = sm.stats.anova_lm(anova_model)\nanova\n\n\n\n\n\n  \n    \n      \n      df\n      sum_sq\n      mean_sq\n      F\n      PR(>F)\n    \n  \n  \n    \n      mpg\n      1.0\n      71.801048\n      71.801048\n      132.393794\n      2.496891e-12\n    \n    \n      disp\n      1.0\n      11.346399\n      11.346399\n      20.921600\n      8.274021e-05\n    \n    \n      Residual\n      29.0\n      15.727553\n      0.542329\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nAfter an ANOVA, some sort of post-hoc test is usually preformed. This isn’t as obvious as the ones above, requiring us to specify monovariate vectors instead of using a formla.\n\n# specify the groups without the formual\ntukey_results = sm.stats.multicomp.pairwise_tukeyhsd(endog = df[\"mpg\"],\n  groups = df[\"cyl\"])\n\nprint(tukey_results)\n\n Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\n     4      6  -6.9208 0.0003 -10.7693 -3.0722   True\n     4      8 -11.5636    0.0 -14.7708 -8.3565   True\n     6      8  -4.6429 0.0112  -8.3276 -0.9581   True\n-----------------------------------------------------"
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "",
    "text": "In this post we explain and re-implement two multiple testing corrections, Bonferroni and Benjamini-Hochberg, for the sake of understanding."
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#preliminaries",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#preliminaries",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe prepare the libraries we’ll need.\n\n# data wrangling, numerical computing, visualization\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# statistical models\nimport statsmodels\nimport statsmodels.api as sm\n\n# existing tools to check our implementation against\nfrom statsmodels.stats.multitest import fdrcorrection\nfrom statsmodels.stats.multitest import multipletests   \n\n# random number generation\nfrom numpy.random import default_rng"
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#simulating-a-motivating-example",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#simulating-a-motivating-example",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Simulating a motivating example",
    "text": "Simulating a motivating example\nThe motivation behind multiple testing correction is that if you test enough hypotheses, you will eventually find a result just by chance. The relevance of this concern increases with the number of tests - some experiments test dozens, hundreds of hypotheses.\nWe will develop a motivating example with which to work. Initially, we will simulate p values from populations with the same parameters so we know the null hypothesis of a t test is true: they’re not different.\nWe can set this up nicely in Python using a function utilizing numpy and a list comprehension:\n\n# seed the results for reproducibility \nrng = default_rng(seed = 1)\n\n# write a simulation function\ndef simulation(mu1, sd1, mu2, sd2, draws = 100, n = 1000):\n    # generate different populations \n    pop1 = rng.normal(loc = mu1, scale = sd1, size = draws)\n    pop2 = rng.normal(loc = mu2, scale = sd2, size = draws)\n    # this returns three things, we only need the middle one\n    tstat, pval, degf = sm.stats.ttest_ind(pop1, pop2)\n\n    return pval\n\nThis function generations two populations. We can make them the same by passing the same mean and standard deviation. Now we can test it:\n\n# set parameters for our populations.\nmu1 = 5\nsd1 = 2.5\n\nmu2 = 5\nsd2 = 2.5\n\n# get a single p value\nsimulation(mu1, sd1, mu2, sd2)\n\n0.9993893617043748\n\n\nRecall that we’ve “simulated under the null” - we’ve specified that the parameters are the same for both distributions. Thus, we assume any positives are false. We can now repeat this a bunch of times to get a distribution of p values. We’re using an \\(\\alpha\\) (alpha) of 0.5 (more on this later), so we expect around 5% to be false positives.\n\n# set a seed\nrng = default_rng(seed = 1)\n\n# run a thousand simulations\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1001)]\n\n# get the ones below alpha\nsig_vals = [p for p in pvals if p <= 0.05]\n\n# calculate the percent that are \"significant\"\nsig_percent = (len(sig_vals)/len(pvals))*100\n\n# percent significant hits\nround(sig_percent, 4)\n\n6.2937\n\n\nWe can see that we have a small but non-trivial amount of hits flagged as significant. If we think about the definition of a p value though, this makes sense.\n\np value: The probability of seeing evidence against the null that is this extreme or more given the null is true [@ 1] - given our modeling assumptions are met.\n\nIf we’re operating at \\(\\alpha = 0.05\\), we’ve accepting that we will be wrong about 5% of the time. In experiments where we’re testing a lot of hypotheses though, this adds up to a lot of mistakes.\nFor example, consider a differential expression experiment involving 20,000 genes:\n\n# 5 percent wrong in this case...\n0.05 * 20000\n\n1000.0\n\n\n…we’re talking about being wrong a thousand times. This is why multiple testing correction exists. We will consider two types with one example each."
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#controlling-family-wise-error-rate-fwer-with-the-bonferroni-method",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#controlling-family-wise-error-rate-fwer-with-the-bonferroni-method",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "Controlling Family-wise Error Rate (FWER) with the Bonferroni method",
    "text": "Controlling Family-wise Error Rate (FWER) with the Bonferroni method\nBonferroni is perhaps the most common method for controlling the Family-wise error rate, which is the probability of making at least one false finding [2]. In this method we simply establish a new threshold for significance by dividing \\(\\alpha\\) by the number of tests.\n\n# a function to apply the correction\ndef calc_bonferroni_threshold(vector, alpha = 0.5):\n    # divide alpha by the number of tests, ie, pvalues in the vector\n    return alpha/len(vector)\n\n# compute a new threshold\nbonferroni_threshold = calc_bonferroni_threshold(pvals)\n\n# new threshold\nbonferroni_threshold\n\n0.0004995004995004995\n\n\n\n# see which ones are significant now\nsig_adj_pvals = [p for p in pvals if p <= bonferroni_threshold]\n\n# calculate the percent\nsig_percent_adj = len(sig_adj_pvals)/len(pvals)\n\n# inspect \nround(sig_percent_adj, 4)\n\n0.0\n\n\nLet’s see if we also get 0 with Python’s version. This doesn’t return the threshold, it returns a list of corrected pvalues and a list of True/False values telling us to reject not. If our code agrees with this, we should see a vector with noTrues in it.\n\n# call the function\ncheck = multipletests(pvals, method=\"bonferroni\", alpha = 0.05)\n\n# the 0th index contains the booleans\ncheck_trues = [i for i in check[0] if i == True]\n\nlen(check_trues)/len(pvals)\n\n0.0\n\n\nWe’ve taken care of the false-positives. As you may have intuited, this is a pretty strict correction (dividing by 20k, in our other example); it does come at the cost of statistical power (the ability of a test to find something if it’s there). There is no free lunch, especially in statistical power. Sometimes a more permissive tradeoff can be made."
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#false-discovery-rate-fdr",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#false-discovery-rate-fdr",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\nWhile Bonferroni is tried and true, it’s something of a blunt instrument however, and there are some situations where another common method Benjamini-Hochberg [3], often abbreviated “BH”, is preferable. The Benjamini-Hochberg controls the False-discovery rate [3], which is the percent of the time we’re making a false positive. This is useful in situations where the strictness of Bonferroni might be limiting (more on comparing the two later). The method involves:\n\nRanking the p values from lowest to highest.\nComputing a critical value for each one.\nComparing the p value to the critical value.\nThe highest p value that is lower that the corresponding critical is the new threshold - it and all smaller than it are considered significant.\n\nThe formula for the critical value is:\n\\[\nc = (\\frac{r}{n})\\alpha\n\\]\nNote you will often see \\(\\alpha\\) denoted \\(q\\) - I’m just trying to keep things simple by calling it what it is (not exactly a time-honored tradition in statistics).\nThis is relatively straightforward to implement in Python (there is a built in function in R as well as a function in statsmodels for Python, but we will reimplement it for the sake of learning and transparency). We define the function below:\n\n# function to compute the new BH threshold\ndef calc_bh_threshold(vector, alpha = 0.05):\n\n    # the number of hypotheses\n    m = len(vector)\n    # the is just naming convention\n    q = alpha\n\n    # sort the pvalues\n    sorted_pvals = sorted(vector)\n\n    # collect a critical value for each p value\n    critical_vals = []\n    for i in range(0, len(sorted_pvals)):\n        rank = i+1 # the rank is the index +1 as Python is zero-indexed\n        # the formula for \n        critical_val = (rank/m)*q\n        critical_vals.append(critical_val)\n    \n    # organize our results\n    df_res = pd.DataFrame({\n        \"pvalue\": sorted_pvals,\n        \"critical_value\": critical_vals\n    })\n    # get the values where the pvalue is lower than the critical value\n    df_res_p_less_crit = df_res.query(\"pvalue <= critical_value\").\\\n        sort_values(by = \"pvalue\", ascending = False)\n    \n    # if none meet the criteria return 0 so no comparison to it will be significant\n    if len(df_res_p_less_crit) == 0:\n        return 0\n    else:\n        # the largest of these is the new threshold\n        return df_res_p_less_crit[\"pvalue\"].max()\n\nbh_threshold = calc_bh_threshold(pvals)\nbh_threshold\n\n0\n\n\nNow we can compare apply this method to correcting for multiple testing on our p values (some of which we know are false positives).\n\n# organize the p values\ndf_res = pd.DataFrame({\"pval\": pvals})\n\n# apply a test - are they lower than the new threshold?\ndf_res[\"bg_reject\"] = df_res.pval.apply(lambda x: x <= bh_threshold)\n\n# find where we reject the null (false positive)\nsig_adj_bh = df_res.query(\"bg_reject == True\")\n\n# get the new percent\nlen(sig_adj_bh)/len(pvals)\n\n0.0\n\n\nWe see we no longer get significant p values. Just to make sure we’re correct, we can simulate where the mean actually is different and see if our implementation agrees with the official one.\n\n# here, the means are different\nmu1 = 5\nsd1 = 1\n\nmu2 = 5.5\nsd2 = 1\n\n# create a another simulated vector of p values\npvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1000)]\n\n# organize it\ndf_check = pd.DataFrame({\"pval\": pvals})\n\n# put some ids to identify them by for better comparison\ndf_check[\"sample_id\"] = [\"sample\" + str(i+1) for i in range(0, len(pvals))]\n\n# get a new threshold\nbh_threshold_check = calc_bh_threshold(df_check[\"pval\"])\nbh_threshold_check\n\n0.04645320875671015\n\n\nWe’ve got our new threshold, now we can compare the p values to it.\n\n# compare the values in the pval function to the new criteria and see how many we get\ndf_check[\"bh_reject_custom\"] = df_check.pval.apply(lambda x: x <= bh_threshold_check)\n\n# a count of reject true/false\ndf_check.bh_reject_custom.value_counts()\n\nTrue     931\nFalse     69\nName: bh_reject_custom, dtype: int64\n\n\nNow we compare that to the statsmodels implementation:\n\n# apply the statsmodels function to the p values\ndf_check[\"bh_reject_default\"] = fdrcorrection(df_check[\"pval\"])[0]\n\n# a table of reject true/false\ndf_check.bh_reject_default.value_counts()\n\nTrue     931\nFalse     69\nName: bh_reject_default, dtype: int64\n\n\nLet’s asses the agreement by making a column based on their comparison:\n\n# do they agree?\ndf_check[\"agreement\"] = \\\n    df_check[\"bh_reject_custom\"] == df_check[\"bh_reject_default\"]\n\n# see where they disagree\n[i for i in df_check[\"agreement\"] if i == False]\n\n[]\n\n\nNo disagreement. Just to be absolutely safe, we check the IDs of the rejected p values.\n\n# subset where the custom rejects\ndf_check_custom_agree = df_check.query(\"bh_reject_custom == True\")\n\n# a subset where the default rejects\ndf_check_default_agree = df_check.query(\"bh_reject_default == True\")\n\n# compare the ids\nagreement = df_check_custom_agree.sample_id == \\\n  df_check_default_agree.sample_id\n\n# observe the concordance\nagreement.value_counts()\n\nTrue    931\nName: sample_id, dtype: int64"
  },
  {
    "objectID": "posts/2022-12-17-multiple-testing-correction/index.html#when-to-use-these-methods",
    "href": "posts/2022-12-17-multiple-testing-correction/index.html#when-to-use-these-methods",
    "title": "Multiple testing correction for family-wise error-rate and false discovery rate",
    "section": "When to use these methods?",
    "text": "When to use these methods?\nThis will depend on the particulars of the experiment in question and must be evaluated on a case-by-case basis. In rough terms however, it depends on if you need to be absolutely sure about a particular result vs if you’re willing to get a few false positives. For example, if you’re conducting a follow-up study to confirm a result before committing a large amount of time and money to a line of research, Bonferroni makes more sense - making even a single error is costly, and this is what the FWER tells us (the probability of even a single mistake). If you’re doing a preliminary experiment to nominate targets for further study, then a few false positives might be fine. You might nominate 10-15 targets to be followed up on in the lab, and if only a few pan out, that’s ok, they’re the new focus. This is common if differential expression experiments [1]."
  },
  {
    "objectID": "unpublished-posts/2017-09-26-idioms/index.html",
    "href": "unpublished-posts/2017-09-26-idioms/index.html",
    "title": "Idioms about idioms",
    "section": "",
    "text": "I am sitting in a coffee shop at school, theoretically so I can clear some e-todos and paperwork from my ledger, but I just realized something weird so I have to write a blog post.\nIt all started with me contemplating the term “pythonic”. For those who haven’t heard it before, it’s jargon from the Python programming community to describe something that fits the style and technique of Python well. One might say something “pythonic” is “idiomatic” Python.\nTaking a step back, an idiom in normal language according to The Notorious O.E.D is:\n1. A group of words established by usage as having a meaning not deducible from those of the individual words (e.g., rain cats and dogs, see the light).\n2. A characteristic mode of expression in music or art.\n‘they were both working in a neo-impressionist idiom’\nA classic idiom that is surprisingly well-conserved* all over the damn place is “When pigs fly”. True to definition 1A, this conveys no real information about what you’re talking about except that which a familiar listener has come to understand it does.\nAn example of the 2A sort would be all the references to having “99 Xs but a Y ain’t one” that appear in pop culture since Jay-Z released “99 Problems”, making a phrase Ice-T said in one song part of every day speech . If you want to get old school with it, think “I woke up this morning” (Da dum dum da) in a blues song (Did someone leave the radio on? It’s getting a little ‘NPR’ in here!).\nSometimes someone or something quirky is described as ‘idiosyncratic’, like Bob Dylan’s voice (or maybe Bob Dylan in general). You can follow this train of thinking all the way down to a specific person’s “idiolect” if you want to go as hard as this guy. In either case, we’re talking about traits that belong to a subset, even if that subset is one person.\nIt’s worth noting that something doesn’t have to be idiomatic to be correct, it’s just not what someone who is fully localized might say. On the street corner, this might be the difference between:\n“Pardon me. I require help to find the subway”\nand:\n“Excuse me, which way’s the train?”\nYou know what they both mean and neither are incorrect, but you could probably tell which one of these folks was on vacation. Anyway…\nBack to the coding example, something pythonic is python written the way a ‘fluent’ user would write it, and that embodies the traits Python is designed to facilitate. This often means it is concise, simple, and easy to read. It might even be an expression that is so common nobody knows who wrote it first, just like with human languages.\nExamples, you say? Take a look at the following code:\n# print out each letter of a string\nname = \"Guido\"\nnumber = 0\nwhile number > len(name):\n    print(name[number])\n    number = number + 1\n\nThis code achieves what it set out to do; there is nothing really ‘wrong’ with it (you’d still know they guy needs to get to the train, if you will). We all write stuff like this once in a while, and if we’re just trying to get a quick idea out for a prototype, it’s probably not worth slowing down to mess with. But it’s decidedly not pythonic.\nFor starters, Python would be more likely to sidestep the counter all together. The ‘While’ loop could easily be the more widely understood ‘For’, and the incrementer in line X could be written more concisely . Let’s give it a try.\nname = “Guido”\nfor letter in name:\n    print (letter)\n\nBoom. We’re down from 5 lines to 3. You could get down to 2 if you wanted to, (put the print() statement right after the : in the line above it) but this is the more typical.\nThe ‘for’ takes care of the iteration for us, and we take advantage of a default variable we can define as we see fit, ‘letter’. This would run with any string there, but why not take advantage of the human friendly syntax opportunity? It’s worth noting that ‘i’, is often used and is idiomatic in programming in general (the incrementer, in this case, is replaced by the for-loop but would be more idiomatically written “number + =1”).\nThat gets the point across, but the example is simple and easy to fix. Let’s look at something a little more true to life. Try this on for size:\n# filter out names that are 3 characters or less.\nnames = [\"Ada\", \"Mary\", \"Bev\", \"Margaret\", \"Rosalind\"]\nshort_names = []\nfor name in names:\n    if len(name) <= 3:\n        short_names.append(name)\n\nThis snippet sets out to do something slightly more complicated even though it’s also 5 lines long: there is a condition of the list containing unprocessed entries and a check of length. The reason I wrote it this way is because it’s less obvious how it could be made more pythonic, but it can be done, and using an especially python-y move: a list comprehension.\nObserve:\nnames = [\"Ada\", \"Mary\", \"Bev\", \"Margaret\", \"Rosalind\"]\nshort_names = [name for name in names if len(name) <= 3]\n\nWhoa, now! Is it all even there?\nThese can seem a little intimidating at first, but they are super handy**. If you just read through slowly, all of the same idea are expressed, just more smoothly. This is probably the ‘expression’ (idiom) that more experienced users would use, so it is worth noting.\nThe ‘=’ and the [] define the new list as a list, and the condition is basically typed in (somewhat awkward) English and that’s it.\nA skeptical reader might be wondering about the philosophy behind this. The Zen of Python states that it is better to be explicit that implicit, but this feels a bit like a shortcut, no? At least, that’s a question that came to my mind; maybe it is glaringly obviously to everyone else on the planet.\nHere’s the thing though: all of the logical components of the first code are carried over into the second. Python isn’t trying to ‘guess’ what we mean from context (nothing is implicit). The only thing that is different is how the command is structured, and that structure is more succinct. I like to read them in English, as well as one can, to illustrate the point.\nThe first one would look something like this:\n“Here is a list of names called names”\n“Create an empty list name short_names”\n“…for each name in names…”\n“…if the length is less than or equal to 3..”\n“…add that name to the short name list.”\nThe second:\n“Here is a list of names called names”\n“Create a list called short names that has each name from names if its length is three or less”\nThere are no important words in the first example that the second doesn’t have. Because all of the same ideas are there, it’s just shorter, I say it fits the philosophy.\nIdioms pop up all over the place in code, in all sorts of languages, and have done so since well before Python was invented. I’m willing to bet most programmers have seen this, or something like it, even if they never wrote a line of C code in their life.\nint main() { for (int i = 0; i < 10; i++) {     printf(“The number is %d”, i); } return 0;\nThat ‘for (int i = 0; i < 10; i++)’ expression, in one form or another, appears EVERYWHERE. C, C++, C#, Java, Perl, Groovy, JavaScript - it’s there. Hell, even Golang, a mere ten years old at the time of this writing and designed to balance old school power with new school syntax has a modified version of that old chestnut***.\nIn closing I share the random thought that started this rant: Saying something is pythonic is basically using an idiom to describe how idiomatic something is. Huh.\nI hope this has been illuminating and/or interesting, and thanks for reading!\nFootnotes:\n \n*The expressions ‘well conserved’ is used all the time in evolutionary biology to describe a protein that varies vary little from species to species. Idiom!\n \n**For example, if you needed a quick Python program to describe Jay-Z’s problems, you can say a lot in one line, much like the man himself:\ncurrent_probs = [prob for prob in old_probs if prob != “B*tch”]\n \n***this is an idiom."
  },
  {
    "objectID": "wip/wtf/testing.html",
    "href": "wip/wtf/testing.html",
    "title": "Thadryan J. Sweeney, MS",
    "section": "",
    "text": "var = 1\n\n\ndef inc(x):\n    return x+1\n\n\ninc(var)\n\n2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "[ What I Work On ]",
    "section": "",
    "text": "My work falls roughly into five (often overlapping) spheres.\n\nGeneral Research Programming\nI work on  apps  that complement  publications  to make their content easier to use and increase engagement.\n\n\nOriginal Software\nI work on creating new software for unmet needs. One example is our  Bayesian COVID Diagnostics app. This serves to complement our  manuscript  on lateral flow assays for SARS-CoV-2 antibodies as well as providing the user with an easy-to-use tool to avoid the pitfalls of black-and-white thinking about diagnostics and counter-intuitive conditional probability. Another is  ContrApption, which is a JavaScript-powered R package that allows to user to create an interactive widget for exploring the results of differential expression experiments or any dataset with similar structure.\n\n\nStatistics for biomarkers\nI work on statistical analysis for biomarkers of potential use in Alzheimer’s Dementia and Neurological symptoms of COVID-19. We’ve got two manuscripts submitted and a few more nearing completion in the sphere, so stay tuned and see my CV for the titles if you’re curious.\n\n\nStatistics and genomics for basic science\nFor an example of this, check out our  manuscript  on stress-dependent signatures of cellular and extracellular tRNA-derived small RNAs (tDRs). We’re interested in this from a basic science perspective as well as a potential avenue of biomarkers for diseases.\n\n\n“Hacktivism”\nI had the privilege of contributing to a  web tool  that allows users to interact with a dataset that ranks the states in the US based on how well they support the financial well-being of survivors of domestic abuse. I did this as a member of  RagTag  in support of  FREEFROM."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "[ Drops of Jupyter ]",
    "section": "",
    "text": "Tidbits on statistics, programming, science, and their collision with the world.\n\n \n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple testing correction for family-wise error-rate and false discovery rate\n\n\n\n\n\nRe-implementing for knowledge and profit(?).\n\n\n\n\n\n\nDec 17, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStats Python in a Hurry Part 3: Simulation, Basic Analysis, & Modeling\n\n\n\n\n\nTransferring the basics of my R modeling knowledge back to my first language.\n\n\n\n\n\n\nDec 13, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStats Python in a Hurry Part 2: Visualization\n\n\n\n\n\nTransferring my R data viz knowledge back to my first language.\n\n\n\n\n\n\nDec 12, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStats Python in a Hurry Part 1: Data Wrangling\n\n\n\n\n\nTransferring my R data-munging knowledge back to my first language.\n\n\n\n\n\n\nDec 11, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummarizing a review of contributors to the opioid epidemic\n\n\n\n\n\n\n\nOpioids\n\n\nResearch\n\n\n\n\nA concise summary of Cerdá, et al with some additional fundamentals.\n\n\n\n\n\n\nDec 10, 2022\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Survival Guide for Students Starting an MS in Bioinformatics\n\n\n\n\n\n\n\nAcademia\n\n\nAdvice\n\n\n\n\nSome lessons learned in grad school.\n\n\n\n\n\n\nSep 29, 2021\n\n\nThadyan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRygor: a Retraction Watch database companion\n\n\n\n\n\n\n\nTools\n\n\n\n\nA command line tool to check a list of citations against The Retraction Watch Database\n\n\n\n\n\n\nJan 19, 2021\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nIn Honor of MLK 2021: “I Have A Dream” Wordclouds Revisited\n\n\n\n\n\n\n\nSociopolitical\n\n\n\n\nSome more wordclouds to celebrate the day.\n\n\n\n\n\n\nJan 18, 2021\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn interactive Bayseian app for interpretation of SARS-CoV-2 antibody tests\n\n\n\n\n\n\n\nTools\n\n\nPublications\n\n\nStatistics\n\n\nJavaScript\n\n\n\n\nA webapp to complement our recent paper.\n\n\n\n\n\n\nJan 8, 2021\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFact Check: The Quote Heather Mac Donald presents when discussing Johnson et al comes from a passage of a pre-print that does not appear in the published work.\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nMore bad form.\n\n\n\n\n\n\nJul 11, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvidence Heather Mac Donald Presented A Pre-print Claim That Was Remove In Peer Review As Scientific\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nMore details on a previously observed biff from HMD.\n\n\n\n\n\n\nJul 10, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetter - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nApplying some more skepticism to a recent WSJ Op-ed, because it deserves it.\n\n\n\n\n\n\nJul 9, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo - “The Myth of Systemic Police Racism” is full of statistical and journalistic errors\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\n\n\nApplying some skepticism to a recent WSJ Op-ed\n\n\n\n\n\n\nJun 27, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Case study in Silent Data Corruption in an RNA-Seq Experiment\n\n\n\n\n\n\n\nR\n\n\nBioinformatics\n\n\nStatistics\n\n\nDifferential Expression\n\n\nTools\n\n\n\n\nHow a subtle bug and misleading error message can transform your RNA-Seq data.\n\n\n\n\n\n\nApr 27, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nHow to install Arch Linux on a VirtualBox VM\n\n\n\n\n\n\n\nLinux\n\n\n\n\nTesting Arch before you wreck your computer installing it.\n\n\n\n\n\n\nMar 30, 2020\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nIn Honor of MLK 2020: ‘Letter from Birmingham Jail’ Wordcloud Example in R\n\n\n\n\n\n\n\nSociopolitical\n\n\n\n\nMarking the occasion with word clouds.\n\n\n\n\n\n\nJan 20, 2020\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonality in gun deaths\n\n\n\n\n\n\n\nSociopolitical\n\n\nAnalysis\n\n\nR\n\n\nStatistics\n\n\n\n\nPracticing data analysis on some CDC gun data.\n\n\n\n\n\n\nAug 30, 2019\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClosures in Python\n\n\n\n\n\n\n\nPython\n\n\nFunctional Programming\n\n\n\n\nMakeshift type checking in Python and when I find it handy.\n\n\n\n\n\n\nMar 28, 2019\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA more rigorous ASH\n\n\n\n\n\n\n\nAntigens\n\n\nPython\n\n\n\n\nBuiling on the ASH tool from a previous post.\n\n\n\n\n\n\nJan 31, 2019\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerl for the impatient\n\n\n\n\n\n\n\nPerl\n\n\n\n\nA dangerously fast introduction to a dangerously concise language.\n\n\n\n\n\n\nOct 4, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython imports: Sample Project + Explanations\n\n\n\n\n\n\n\nPython\n\n\n\n\nAnswering a question on using imports in Python pograms\n\n\n\n\n\n\nSep 30, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObject-oriented Python: an overview\n\n\n\n\n\n\n\nPython\n\n\nObject-Oriented Programming\n\n\n\n\nAn overview of object-oriented programming in Python.\n\n\n\n\n\n\nAug 30, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA handshake with Scala\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome light-hearted oop\n\n\n\n\n\n\n\nPython\n\n\nObject-Oriented Programming\n\n\n\n\n…or: circumstantial evidence that I am still alive.\n\n\n\n\n\n\nMay 10, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind and replace in Python\n\n\n\n\n\n\n\nPython\n\n\nData Processing\n\n\n\n\nLike ctrl-f with for Python.\n\n\n\n\n\n\nMar 12, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nASH: Antigen Selection Heuristic\n\n\n\n\n\n\n\nAntigens\n\n\nPython\n\n\n\n\nA prototype to select antigenic peptides.\n\n\n\n\n\n\nFeb 20, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple date-string conversion\n\n\n\n\n\n\n\nPython\n\n\nData Processing\n\n\n\n\nAnswering a question an dates and strings.\n\n\n\n\n\n\nFeb 6, 2018\n\n\nThadryan\n\n\n\n\n\n\n  \n\n\n\n\nMatrin Luther King Day\n\n\n\n\n\n\n\nSociopolitical\n\n\nR\n\n\nNLP\n\n\n\n\nMarking the occasion with word clouds.\n\n\n\n\n\n\nJan 16, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrump v. Clinton: Twitter metadata classifier\n\n\n\n\n\nClassifiying tweets as Hilary or Trump.\n\n\n\n\n\n\nJan 9, 2018\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython for Perl users\n\n\n\n\n\n\n\nPython\n\n\nPerl\n\n\n\n\nAll the kids are using Python now.\n\n\n\n\n\n\nDec 21, 2017\n\n\nThadryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nzip and dict\n\n\n\n\n\n\n\nPerl\n\n\nPython\n\n\n\n\nThe peanutbetter and jelly of paired data.\n\n\n\n\n\n\nDec 20, 2017\n\n\nThadryan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "[ CV ]",
    "section": "",
    "text": "For computational and data science projects, see my portfolio."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thadryan J. Sweeney, MS",
    "section": "",
    "text": "I’m a Firefighter and Human Services Professional turned Data Scientist interested in statistics, interactive visualization, and biomarker discovery. I do statistically oriented biology research using R, Python, and JavaScript. I’m currently pursing a PhD in Quantitative Biomedical Sciences.\nBefore starting my doctorate, I was at Mass General Hospital where I worked in the  Alzheimer’s Clinical & Translational Research Unit. Before moving to ACTRU, I did statistics and web app development for studies of SARS-Cov-2 and various heart diseases at the  Cardiovascular Research Center.\nI also taught an introductory class in the Bioinformatics MS  program at Northeastern University. If you’re interested in seeing what I’m working on professionally, check out my  CV  or my  LinkedIn.  When I’m not doing science-y stuff, I play RPGs, write music, or make cartoons.\nOf the links below, Mastodon is my preferred platform."
  }
]