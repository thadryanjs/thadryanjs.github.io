---
author: Thadryan
title: "Stats Python in a Hurry Part 1: Data Wrangling"
date: 2022-12-11
description: Transferring my R data-munging knowledge back to my first language.
bibliography: Part1.bib
toc: true
---

> update: 12-17-22 - added sorting and table.

# Data Manipulation 

```{python}

import pandas as pd
import numpy as np
import statsmodels.api as sm
```

```{python}
# get a practice dataset
df = sm.datasets.get_rdataset("mtcars", "datasets", cache = True).data

df.head()
```

## Basics

How to get basic information about the df.

### Dimensions of a df

This is like `dim` in R.

```{python}
# rows by columns 
df.shape
```

### Number of rows

This is like `nrow` in R.

```{python}
# just the rows
len(df)
```

### Number of columns

This is like `ncol` in R.

```{python}
# just the columns
len(df.columns)
```

You could also just index the results of `df.shape`.

### Data types

```{python}
# a summary of types by column
df.info()
```

### Summary

```{python}
# summary of distributional information
df.describe()
```

## Selections

How to specify which column you want.

### Select a column

Note that I'm using `head()` to get the first few rows to keep the output small. It's not part of the selection of columns.

```{python}
# access a single column like an object property
df.mpg.head()
```

Or...

```{python}
# select a column (as a series)
df["mpg"].head()
```

```{python}
# select column (as a dataframe)
df[["mpg"]].head()
```

### Select several columns by string name

```{python}
# select a column by several names
df[["mpg", "wt"]].head()
```

### Select from one column through another with names

```{python}
# select a range by name of start and end "mph through hp"
df.loc[:, "mpg":"hp"].head()
```

### Select arbitrarily

How to get based on a condition of your choosing. We might want to get only columns that start with "d", for example.

```{python}
# get a list
starts_with_d = [i for i in list(df.columns) if i.startswith("d")]

# pass the list to pandas
df[starts_with_d].head()
```

## Filtering

Operations where we'd used `dplyr` in R. In Pandas, filtering refers to operations by index, so what I'm thinking of is more like "querying" in Pandas terms.

### Single condition

Pass logic in strings to the `query` method.

```{python}
df.query("cyl == 6")
```

### Multiple conditions

```{python}
df.query("cyl == 6 & hp > 105")
```

## Sorting

### Ascending

```{python}
df.mpg.sort_values().head(10)
```

### Descending

There is no 'descending' per se, just set `ascending` to False.

```{python}
df.mpg.sort_values(ascending = False).head(10)
```

## Applying functions

### Map a function with no arguments to a single column

```{python}
# some function to apply
def increment(x):
    return x+1

# could also be done as a lambda expression
df["ApplyResult"] = df.mpg.map(increment)

df[["mpg", "ApplyResult"]].head()
```

### Apply a function with arguments using kwargs 

```{python}
# some function yadda
def incrementBy(x, by):
    return x + by

# could also be done as a lambda expression
df["ApplyResult"] = df.mpg.apply(incrementBy, by = 5)

df[["mpg", "ApplyResult"]].head()
```

## Missing values

Expanding on an example from [here](https://datatofish.com/check-nan-pandas-dataframe/). Pandas uses `np.nan` instead of `NA`.

```{python}

df2 = pd.DataFrame({
    "x": [1,2,3,4,5,np.nan,7,8,np.nan,10,11,12,np.nan],
    "y": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,13]
})
df2
```

### Missing in the whole df

```{python}
# no NA the whole thing
df2.isnull()
```

### Missing in a single column

```{python}
# just the x column
df2.x.isnull()
```

### Replace NAs in a column

```{python}
# use the fillna function 
df2.x = df2.x.fillna("MISSING")
df2
```

### Replace the NAs in the full dataset

```{python}
# files the other even though I didn't specify a column
df2.fillna("MISSING!!!")
```

### Drop NAs

The [official docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) are great on this.

```{python}
# create another sample df
df3 = pd.DataFrame({
    "x": [1,np.nan,3,4,5,np.nan,np.nan,8,np.nan,10,11,12,np.nan],
    "y": [1,np.nan,3,4,5,6,np.nan,8,9,10,11,12,np.nan],
    "z": [1,2,np.nan,4,5,6,np.nan,8,9,10,11,12,np.nan]
})
df3
```

#### Drop all across the df

```{python}
# no NAs period 
df3.dropna()
```

#### Drop if they're all na in that row

```{python}
# drop if the whole row is NA
df3.dropna(how = "all")
```

#### Drop all in certain columns

This is so clutch! Much simpler than filtering across in R (though there might be a cleaner way for that)

```{python}
df3.dropna(subset = ["x", "y"])
```

## Group & Summarize

### Group by a factor level

```{python}
df[["mpg", "disp", "cyl"]].groupby(by = "cyl")
```

### Groupby factor level and get the mean

```{python}
df[["mpg", "disp", "cyl"]].groupby(by = "cyl").mean()
```

## Melting a DataFrame

```{python}
# use the melt function specifying ids and value vars
df_long = pd.melt(df, id_vars = "cyl", value_vars = ["mpg", "wt"])
df_long
```

## Misc

### Counts of values (like `table` in R)

```{python}
# shows how many there are of each factor
df.cyl.value_counts()
```


### Formatting long chains

```{python}
# a little ugly but it works
df[["mpg", "disp", "cyl"]].\
    query("cyl > 4").\
    groupby(by = "cyl").\
    mean()
```

### Iterating over a df

For mutating the dataframe apply/map is recommended, but I'm showing how to do this for completeness.

```{python}
for i, j in df.iterrows():
    print(df["mpg"][i])
    # j would give you all the columns with just that row
```

https://www.statsmodels.org/stable/examples/notebooks/generated/glm.html
https://www.geeksforgeeks.org/linear-regression-in-python-using-statsmodels/
https://datagy.io/pandas-iterate-over-rows/
https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html
