{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "author: Thadryan\n",
        "title: \"Multiple testing correction for family-wise error-rate and false discovery rate\"\n",
        "description: \"Re-implementing for knowledge and profit(?).\"\n",
        "date: 2022-12-17\n",
        "toc: true\n",
        "bibliography: MultipleTesting.bib\n",
        "csl: style.csl\n",
        "---"
      ],
      "id": "d83f7b56"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this post we explain and re-implement two multiple testing corrections, Bonferroni and Benjamini-Hochberg, for the sake of understanding.\n",
        "\n",
        "## Preliminaries\n",
        "\n",
        "We prepare the libraries we'll need.\n"
      ],
      "id": "3c67cd34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# data wrangling, numerical computing, visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# statistical models\n",
        "import statsmodels\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# existing tools to check our implementation against\n",
        "from statsmodels.stats.multitest import fdrcorrection\n",
        "from statsmodels.stats.multitest import multipletests\t\n",
        "\n",
        "# random number generation\n",
        "from numpy.random import default_rng"
      ],
      "id": "5c12c1c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating a motivating example\n",
        "\n",
        "The motivation behind multiple testing correction is that if you test enough hypotheses, you will eventually find a result just by chance. The relevance of this concern increases with the number of tests - some experiments test dozens, hundreds of hypotheses.\n",
        "\n",
        "We will develop a motivating example with which to work. Initially, we will simulate p values from populations with the same parameters so we know the null hypothesis of a t test is true: they're not different.\n",
        "\n",
        "We can set this up nicely in Python using a function utilizing numpy and a list comprehension:\n"
      ],
      "id": "c144642e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# seed the results for reproducibility \n",
        "rng = default_rng(seed = 1)\n",
        "\n",
        "# write a simulation function\n",
        "def simulation(mu1, sd1, mu2, sd2, draws = 100, n = 1000):\n",
        "    # generate different populations \n",
        "    pop1 = rng.normal(loc = mu1, scale = sd1, size = draws)\n",
        "    pop2 = rng.normal(loc = mu2, scale = sd2, size = draws)\n",
        "    # this returns three things, we only need the middle one\n",
        "    tstat, pval, degf = sm.stats.ttest_ind(pop1, pop2)\n",
        "\n",
        "    return pval"
      ],
      "id": "f60ea48d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function generations two populations. We can make them the same by passing the same mean and standard deviation. Now we can test it:\n"
      ],
      "id": "203b2264"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# set parameters for our populations.\n",
        "mu1 = 5\n",
        "sd1 = 2.5\n",
        "\n",
        "mu2 = 5\n",
        "sd2 = 2.5\n",
        "\n",
        "# get a single p value\n",
        "simulation(mu1, sd1, mu2, sd2)"
      ],
      "id": "92c4fef0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that we've \"simulated under the null\" - we've specified that the parameters are the same for both distributions. Thus, we assume any positives are false. We can now repeat this a bunch of times to get a distribution of p values. We're using an $\\alpha$ (alpha) of 0.5 (more on this later), so we expect around 5% to be false positives.\n"
      ],
      "id": "b8acfcc4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# set a seed\n",
        "rng = default_rng(seed = 1)\n",
        "\n",
        "# run a thousand simulations\n",
        "pvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1001)]\n",
        "\n",
        "# get the ones below alpha\n",
        "sig_vals = [p for p in pvals if p <= 0.05]\n",
        "\n",
        "# calculate the percent that are \"significant\"\n",
        "sig_percent = (len(sig_vals)/len(pvals))*100\n",
        "\n",
        "# percent significant hits\n",
        "round(sig_percent, 4)"
      ],
      "id": "bb2ecbc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we have a small but non-trivial amount of hits flagged as significant. If we think about the definition of a p value though, this makes sense. \n",
        "\n",
        "> p value: The probability of seeing evidence against the null that is this extreme or more given the null is true [@@HypothesisTestingHandbook] - given our modeling assumptions are met.\n",
        "\n",
        "If we're operating at $\\alpha = 0.05$, we've accepting that we will be wrong about 5% of the time. In experiments where we're testing a lot of hypotheses though, this adds up to a lot of mistakes.\n",
        "\n",
        "For example, consider a differential expression experiment involving 20,000 genes:\n"
      ],
      "id": "af72bf92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5 percent wrong in this case...\n",
        "0.05 * 20000"
      ],
      "id": "200763df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...we're talking about being wrong a thousand times. This is why multiple testing correction exists. We will consider two types with one example each.\n",
        "\n",
        "## Controlling Family-wise Error Rate (FWER) with the Bonferroni method\n",
        "\n",
        "Bonferroni is perhaps the most common method for controlling the Family-wise error rate, which is the probability of making at least one false finding [@MultipleComparisonsHandbook]. In this method we simply establish a new threshold for significance by dividing $\\alpha$ by the number of tests.\n"
      ],
      "id": "5280ee81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# a function to apply the correction\n",
        "def calc_bonferroni_threshold(vector, alpha = 0.5):\n",
        "    # divide alpha by the number of tests, ie, pvalues in the vector\n",
        "    return alpha/len(vector)\n",
        "\n",
        "# compute a new threshold\n",
        "bonferroni_threshold = calc_bonferroni_threshold(pvals)\n",
        "\n",
        "\n",
        "# new threshold\n",
        "bonferroni_threshold"
      ],
      "id": "0ad4b339",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# see which ones are significant now\n",
        "sig_adj_pvals = [p for p in pvals if p <= bonferroni_threshold]\n",
        "\n",
        "# calculate the percent\n",
        "sig_percent_adj = len(sig_adj_pvals)/len(pvals)\n",
        "\n",
        "# inspect \n",
        "round(sig_percent_adj, 4)"
      ],
      "id": "edf5a178",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see if we also get 0 with Python's version. This doesn't return the threshold, it returns a list of corrected pvalues and a list of True/False values telling us to reject not. If our code agrees with this, we should see a vector with no`True`s in it.\n"
      ],
      "id": "84288b96"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# call the function\n",
        "check = multipletests(pvals, method=\"bonferroni\", alpha = 0.05)\n",
        "\n",
        "# the 0th index contains the booleans\n",
        "check_trues = [i for i in check[0] if i == True]\n",
        "\n",
        "len(check_trues)/len(pvals)"
      ],
      "id": "5aab0efd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've taken care of the false-positives. As you may have intuited, this is a pretty strict correction (dividing by 20k, in our other example); it does come at the cost of statistical power (the ability of a test to find something if it's there). There is no free lunch, especially in statistical power. Sometimes a more permissive tradeoff can be made.\n",
        "\n",
        "## False Discovery Rate (FDR)\n",
        "\n",
        "While Bonferroni is tried and true, it's something of a blunt instrument however, and there are some situations where another common method Benjamini-Hochberg [@benjaminiControllingFalseDiscovery1995], often abbreviated \"BH\", is preferable. The Benjamini-Hochberg controls the False-discovery rate [@benjaminiControllingFalseDiscovery1995], which is the percent of the time we're making a false positive. This is useful in situations where the strictness of Bonferroni might be limiting (more on comparing the two later). The method involves:\n",
        "\n",
        "  - Ranking the p values from lowest to highest.\n",
        "  - Computing a critical value for each one.\n",
        "  - Comparing the p value to the critical value.\n",
        "  - The highest p value that is lower that the corresponding critical is the new threshold - it and all smaller than it are considered significant.\n",
        "\n",
        "The formula for the critical value is:\n",
        "\n",
        "$$\n",
        "c = (\\frac{r}{n})\\alpha\n",
        "$$\n",
        "\n",
        "Note you will often see $\\alpha$ denoted $q$ - I'm just trying to keep things simple by calling it what it is (not exactly a time-honored tradition in statistics).\n",
        "\n",
        "This is relatively straightforward to implement in Python (there is a built in function in R as well as a function in `statsmodels` for Python, but we will reimplement it for the sake of learning and transparency). We define the function below:\n"
      ],
      "id": "273ff08d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# function to compute the new BH threshold\n",
        "def calc_bh_threshold(vector, alpha = 0.05):\n",
        "\n",
        "    # the number of hypotheses\n",
        "    m = len(vector)\n",
        "    # the is just naming convention\n",
        "    q = alpha\n",
        "\n",
        "    # sort the pvalues\n",
        "    sorted_pvals = sorted(vector)\n",
        "\n",
        "    # collect a critical value for each p value\n",
        "    critical_vals = []\n",
        "    for i in range(0, len(sorted_pvals)):\n",
        "        rank = i+1 # the rank is the index +1 as Python is zero-indexed\n",
        "        # the formula for \n",
        "        critical_val = (rank/m)*q\n",
        "        critical_vals.append(critical_val)\n",
        "    \n",
        "    # organize our results\n",
        "    df_res = pd.DataFrame({\n",
        "        \"pvalue\": sorted_pvals,\n",
        "        \"critical_value\": critical_vals\n",
        "    })\n",
        "    # get the values where the pvalue is lower than the critical value\n",
        "    df_res_p_less_crit = df_res.query(\"pvalue <= critical_value\").\\\n",
        "        sort_values(by = \"pvalue\", ascending = False)\n",
        "    \n",
        "    # if none meet the criteria return 0 so no comparison to it will be significant\n",
        "    if len(df_res_p_less_crit) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        # the largest of these is the new threshold\n",
        "        return df_res_p_less_crit[\"pvalue\"].max()\n",
        "\n",
        "bh_threshold = calc_bh_threshold(pvals)\n",
        "bh_threshold"
      ],
      "id": "706d927d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compare apply this method to correcting for multiple testing on our p values (some of which we know are false positives).\n"
      ],
      "id": "001b6a4e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# organize the p values\n",
        "df_res = pd.DataFrame({\"pval\": pvals})\n",
        "\n",
        "# apply a test - are they lower than the new threshold?\n",
        "df_res[\"bg_reject\"] = df_res.pval.apply(lambda x: x <= bh_threshold)\n",
        "\n",
        "# find where we reject the null (false positive)\n",
        "sig_adj_bh = df_res.query(\"bg_reject == True\")\n",
        "\n",
        "# get the new percent\n",
        "len(sig_adj_bh)/len(pvals)"
      ],
      "id": "503c774b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see we no longer get significant p values. Just to make sure we're correct, we can simulate where the mean actually is different and see if our implementation agrees with the official one.\n"
      ],
      "id": "5f1e486f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# here, the means are different\n",
        "mu1 = 5\n",
        "sd1 = 1\n",
        "\n",
        "mu2 = 5.5\n",
        "sd2 = 1\n",
        "\n",
        "# create a another simulated vector of p values\n",
        "pvals = [simulation(mu1, sd1, mu2, sd2) for i in range(0,1000)]\n",
        "\n",
        "# organize it\n",
        "df_check = pd.DataFrame({\"pval\": pvals})\n",
        "\n",
        "# put some ids to identify them by for better comparison\n",
        "df_check[\"sample_id\"] = [\"sample\" + str(i+1) for i in range(0, len(pvals))]\n",
        "\n",
        "# get a new threshold\n",
        "bh_threshold_check = calc_bh_threshold(df_check[\"pval\"])\n",
        "bh_threshold_check"
      ],
      "id": "c5a977e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've got our new threshold, now we can compare the p values to it.\n"
      ],
      "id": "2ebcd8c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# compare the values in the pval function to the new criteria and see how many we get\n",
        "df_check[\"bh_reject_custom\"] = df_check.pval.apply(lambda x: x <= bh_threshold_check)\n",
        "\n",
        "# a count of reject true/false\n",
        "df_check.bh_reject_custom.value_counts()"
      ],
      "id": "2b756678",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we compare that to the statsmodels implementation:\n"
      ],
      "id": "d2336e4a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# apply the statsmodels function to the p values\n",
        "df_check[\"bh_reject_default\"] = fdrcorrection(df_check[\"pval\"])[0]\n",
        "\n",
        "# a table of reject true/false\n",
        "df_check.bh_reject_default.value_counts()"
      ],
      "id": "02e11cd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's asses the agreement by making a column based on their comparison:\n"
      ],
      "id": "06eaab61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# do they agree?\n",
        "df_check[\"agreement\"] = \\\n",
        "    df_check[\"bh_reject_custom\"] == df_check[\"bh_reject_default\"]\n",
        "\n",
        "# see where they disagree\n",
        "[i for i in df_check[\"agreement\"] if i == False]"
      ],
      "id": "9ef90646",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No disagreement. Just to be absolutely safe, we check the IDs of the rejected p values.\n"
      ],
      "id": "9e1e8447"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# subset where the custom rejects\n",
        "df_check_custom_agree = df_check.query(\"bh_reject_custom == True\")\n",
        "\n",
        "# a subset where the default rejects\n",
        "df_check_default_agree = df_check.query(\"bh_reject_default == True\")\n",
        "\n",
        "# compare the ids\n",
        "agreement = df_check_custom_agree.sample_id == \\\n",
        "  df_check_default_agree.sample_id\n",
        "\n",
        "# observe the concordance\n",
        "agreement.value_counts()"
      ],
      "id": "6076a061",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to use these methods?\n",
        "\n",
        "This will depend on the particulars of the experiment in question and must be evaluated on a case-by-case basis. In rough terms however, it depends on if you need to be absolutely sure about a particular result vs if you're willing to get a few false positives. For example, if you're conducting a follow-up study to confirm a result before committing a large amount of time and money to a line of research, Bonferroni makes more sense - making even a single error is costly, and this is what the FWER tells us (the probability of even a single mistake). If you're doing a preliminary experiment to nominate targets for further study, then a few false positives might be fine. You might nominate 10-15 targets to be followed up on in the lab, and if only a few pan out, that's ok, they're the new focus. This is common if differential expression experiments [@HypothesisTestingHandbook].\n",
        "\n",
        "## References "
      ],
      "id": "40a7d4ec"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}